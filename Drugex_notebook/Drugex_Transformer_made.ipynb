{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drug Explorer (DrugEx v3): Scaffold-Constrained Drug Design with Graph Transformer-based Reinforcement Learning\n",
    "https://github.com/XuhanLiu/DrugEx\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the large drug-like chemical space available to search for feasible drug-like molecules, rational drug design often starts from specific scaffolds to which side chains/substituents are added or modified. With the rapid growth of the application of deep learning in drug discovery, a variety of effective approaches have been developed for de novo drug design. In previous work, we proposed a method named DrugEx, which can be applied in polypharmacology based on multi-objective deep reinforcement learning. However, the previous version is trained under fixed objectives similar to other known methods and does not allow users to input any prior information (i.e. a desired scaffold). In order to improve the general applicability, we updated DrugEx to design drug molecules based on scaffolds which consist of multiple fragments provided by users. In this work, the Transformer model was employed to generate molecular structures. The Transformer is a multi-head self-attention deep learning model containing an encoder to receive scaffolds as input and a decoder to generate molecules as output. In order to deal with the graph representation of molecules we proposed a novel positional encoding for each atom and bond based on an adjacency matrix to extend the architecture of the Transformer. Each molecule was generated by growing and connecting procedures for the fragments in the given scaffold that were unified into one model. Moreover, we trained this generator under a reinforcement learning framework to increase the number of desired ligands. As a proof of concept, our proposed method was applied to design ligands for the adenosine A2A receptor (A2AAR) and compared with SMILES-based methods. The results demonstrated the effectiveness of our method in that 100% of the generated molecules are valid and most of them had a high predicted affinity value towards A2AAR with given scaffolds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "# Python >= 3.7\n",
    "# Numpy (version >= 1.19)\n",
    "# Scikit-Learn (version >= 0.23)\n",
    "# Pandas (version >= 1.2.2)\n",
    "# PyTorch (version == 1.7)\n",
    "# Matplotlib (version >= 2.0)\n",
    "# RDKit (version >= 2020.03)\n",
    "\n",
    "# You have\n",
    "# !python -V\n",
    "# 3.7.7\n",
    "# import numpy as np\n",
    "# print(np.__version__)\n",
    "# 1.16.5\n",
    "# import sklearn as sklearn\n",
    "# print(sklearn.__version__)\n",
    "# 0.24.1\n",
    "# import pandas as pd\n",
    "# print(pd.__version__)\n",
    "# 0.24.2\n",
    "# import torch\n",
    "# print(torch.__version__)\n",
    "# notaavailabl at jupter60\n",
    "# import Matplotlib as Matplotlib\n",
    "# print(Matplotlib.__version__)\n",
    "# import rdkit as rdkit\n",
    "# print(rdkit.__version__)\n",
    "# 2018.09.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python -m pip install --user numpy --upgrade\n",
    "#!python -m pip install --user tensorflow --upgrade\n",
    "#!python -m pip install --user numpy pycocotools==2.0.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <ins>Let's start</ins> \n",
    "\n",
    "We'll start with required imports. These includes the [Keras](https://keras.io/) and [Tensorflow](https://www.tensorflow.org/) libraries for the neural network models, [Pandas](https://pandas.pydata.org/) and [Numpy](https://numpy.org/) to process data, as well as other relevant Python libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018.09.1\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "# general imports\n",
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "#import tensorflow.compat.v1 as tf\n",
    "#tf.disable_v2_behavior() \n",
    "import keras\n",
    "from keras import initializers\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from keras import optimizers\n",
    "from keras import regularizers\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "#from matplotlib import pyplot as plt\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "import plotly.express as px\n",
    "import numpy as np\n",
    "import csv\n",
    "import copy\n",
    "import random\n",
    "import rdkit as rdkit\n",
    "print(rdkit.__version__)\n",
    "from rdkit import Chem\n",
    "#from rdkit.Chem import AllChem as Chem\n",
    "from rdkit.Chem import AllChem\n",
    "from rdkit.Chem import Draw\n",
    "from rdkit.Chem import Crippen\n",
    "from rdkit.Chem import Descriptors, Descriptors3D \n",
    "from rdkit.ML.Descriptors import MoleculeDescriptors\n",
    "from rdkit.Chem import Lipinski, rdDepictor, rdMolDescriptors\n",
    "from rdkit.Chem import MolSurf\n",
    "from rdkit.Chem import PandasTools\n",
    "from rdkit.Chem.Draw import IPythonConsole\n",
    "from rdkit.Chem import rdFingerprintGenerator\n",
    "from rdkit import DataStructs\n",
    "from rdkit.Chem import MACCSkeys\n",
    "from rdkit.Chem.Fingerprints import FingerprintMols\n",
    "from descriptastorus.descriptors.DescriptorGenerator import MakeGenerator\n",
    "#https://github.com/bp-kelley/descriptastorus\n",
    "from mordred import Calculator, descriptors\n",
    "import time\n",
    "rdDepictor.SetPreferCoordGen(True)\n",
    "\n",
    "\n",
    "#from util import partTypeNum\n",
    "#import util\n",
    "# from tqdm import tqdm\n",
    "# from sklearn.decomposition import PCA\n",
    "# from sklearn.manifold import TSNE\n",
    "# from sklearn.cluster import KMeans\n",
    "# from sklearn.preprocessing import StandardScaler as Scaler\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#import pymatgen as pymat\n",
    "#import mendeleev as mendel\n",
    "from subprocess import call\n",
    "import gzip\n",
    "\n",
    "from scipy.stats import norm\n",
    "from IPython.display import HTML\n",
    "\n",
    "# keras imports\n",
    "from keras.layers import (Input, Dense, Conv1D, MaxPool1D, Dropout, GRU, LSTM, TimeDistributed, Add, Flatten, RepeatVector, Lambda, Concatenate)\n",
    "from keras.models import Model, load_model\n",
    "from keras.metrics import binary_crossentropy\n",
    "from keras import initializers, regularizers\n",
    "from keras.callbacks import EarlyStopping\n",
    "import keras.backend as K\n",
    "\n",
    "# Visualization\n",
    "from keras_sequential_ascii import keras2ascii\n",
    "\n",
    "\n",
    "# from utils import label_map_util\n",
    "# from utils import visualization_utils as vis_util\n",
    "\n",
    "#from object_detection.utils import label_map_util\n",
    "#from object_detection.utils import visualization_utils as vis_util\n",
    "\n",
    "# utils functions\n",
    "#from python_utils import *\n",
    "from utils import *\n",
    "\n",
    "# Hacky MacOS fix for Tensorflow runtimes... (You won't need this unless you are on MacOS)\n",
    "# This fixes a display bug with progress bars that can pop up on MacOS sometimes.\n",
    "#import sys\n",
    "#import os\n",
    "#sys.path.insert(0, '../src/')\n",
    "\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "\n",
    "# Remove warnings from output\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#!python -m pip install --user numpy --upgrade\n",
    "#!python -m pip install --user tensorflow --upgrade\n",
    "#!python -m pip install --user numpy pycocotools==2.0.0\n",
    "\n",
    "#!python -V\n",
    "#import tensorflow as tf\n",
    "#print(tf.__version__)\n",
    "#import numpy as np\n",
    "#print(np.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For designing the novel drug molecules with SMILES representation, you should do the following steps sequentially by running scripts:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 dataset.py:\n",
    "\n",
    "Preparing your dataset for pre-training and fine-tuning the RNN model as initial states of exploitation network and exploration network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from rdkit import Chem\n",
    "from rdkit import rdBase\n",
    "from rdkit.Chem import Recap, BRICS\n",
    "from rdkit.Chem.MolStandardize import rdMolStandardize\n",
    "from tqdm import tqdm\n",
    "from utils import VocSmiles as Voc\n",
    "import utils\n",
    "import re\n",
    "import numpy as np\n",
    "from itertools import combinations\n",
    "import gzip\n",
    "import getopt, sys\n",
    "rdBase.DisableLog('rdApp.info')\n",
    "rdBase.DisableLog('rdApp.warning')\n",
    "\n",
    "\n",
    "def corpus(input, output, suffix='sdf'):\n",
    "    if suffix =='sdf':\n",
    "        inf = gzip.open(input)\n",
    "        mols = Chem.ForwardSDMolSupplier(inf)\n",
    "        # mols = [mol for mol in suppl]\n",
    "    else:\n",
    "        df = pd.read_table(input).Smiles.dropna()\n",
    "        mols = [Chem.MolFromSmiles(s) for s in df]\n",
    "    voc = Voc('data/voc_smiles.txt')\n",
    "    charger = rdMolStandardize.Uncharger()\n",
    "    chooser = rdMolStandardize.LargestFragmentChooser()\n",
    "    disconnector = rdMolStandardize.MetalDisconnector()\n",
    "    normalizer = rdMolStandardize.Normalizer()\n",
    "    words = set()\n",
    "    canons = []\n",
    "    tokens = []\n",
    "    smiles = set()\n",
    "    for mol in tqdm(mols):\n",
    "        try:\n",
    "            mol = disconnector.Disconnect(mol)\n",
    "            mol = normalizer.normalize(mol)\n",
    "            mol = chooser.choose(mol)\n",
    "            mol = charger.uncharge(mol)\n",
    "            mol = disconnector.Disconnect(mol)\n",
    "            mol = normalizer.normalize(mol)\n",
    "            smileR = Chem.MolToSmiles(mol, 0)\n",
    "            smiles.add(Chem.CanonSmiles(smileR))\n",
    "        except:\n",
    "            print('Parsing Error:') #, Chem.MolToSmiles(mol))\n",
    "\n",
    "    for smile in tqdm(smiles):\n",
    "        token = voc.split(smile) + ['EOS']\n",
    "        if {'C', 'c'}.isdisjoint(token):\n",
    "            print('Warning:', smile)\n",
    "            continue\n",
    "        if not {'[Na]', '[Zn]'}.isdisjoint(token):\n",
    "            print('Redudent', smile)\n",
    "            continue\n",
    "        if 10 < len(token) <= 100:\n",
    "            words.update(token)\n",
    "            canons.append(smile)\n",
    "            tokens.append(' '.join(token))\n",
    "    log = open(output + '_voc.txt', 'w')\n",
    "    log.write('\\n'.join(sorted(words)))\n",
    "    log.close()\n",
    "\n",
    "    log = pd.DataFrame()\n",
    "    log['Smiles'] = canons\n",
    "    log['Token'] = tokens\n",
    "    log.drop_duplicates(subset='Smiles')\n",
    "    log.to_csv(output + '_corpus.txt', sep='\\t', index=False)\n",
    "\n",
    "\n",
    "def graph_corpus(input, output, suffix='sdf'):\n",
    "    metals = {'Na', 'Zn', 'Li', 'K', 'Ca', 'Mg', 'Ag', 'Cs', 'Ra', 'Rb', 'Al', 'Sr', 'Ba', 'Bi'}\n",
    "    voc = utils.VocGraph('data/voc_atom.txt')\n",
    "    inf = gzip.open(input)\n",
    "    if suffix == 'sdf':\n",
    "        mols = Chem.ForwardSDMolSupplier(inf)\n",
    "        total = 2e6\n",
    "    else:\n",
    "        mols = pd.read_table(input).drop_duplicates(subset=['Smiles']).dropna(subset=['Smiles'])\n",
    "        total = len(mols)\n",
    "        mols = mols.iterrows()\n",
    "    vals = {}\n",
    "    exps = {}\n",
    "    codes, ids = [], []\n",
    "    chooser = rdMolStandardize.LargestFragmentChooser()\n",
    "    disconnector = rdMolStandardize.MetalDisconnector()\n",
    "    normalizer = rdMolStandardize.Normalizer()\n",
    "    for i, mol in enumerate(tqdm(mols, total=total)):\n",
    "        if mol is None: continue\n",
    "        if suffix != 'sdf':\n",
    "            idx = mol[1]['Molecule ChEMBL ID']\n",
    "\n",
    "            mol = Chem.MolFromSmiles(mol[1].Smiles)\n",
    "        else:\n",
    "            idx = mol.GetPropsAsDict()\n",
    "            idx = idx['chembl_id']\n",
    "        try:\n",
    "            mol = disconnector.Disconnect(mol)\n",
    "            mol = normalizer.normalize(mol)\n",
    "            mol = chooser.choose(mol)\n",
    "            mol = disconnector.Disconnect(mol)\n",
    "            mol = normalizer.normalize(mol)\n",
    "        except:\n",
    "            print(idx)\n",
    "        symb = [a.GetSymbol() for a in mol.GetAtoms()]\n",
    "        # Nr. of the atoms\n",
    "        bonds = mol.GetBonds()\n",
    "        if len(bonds) < 4 or len(bonds) >= 63: continue\n",
    "        if {'C'}.isdisjoint(symb): continue\n",
    "        if not metals.isdisjoint(symb): continue\n",
    "\n",
    "        smile = Chem.MolToSmiles(mol)\n",
    "        try:\n",
    "            s0 = smile.replace('[O]', 'O').replace('[C]', 'C') \\\n",
    "                 .replace('[N]', 'N').replace('[B]', 'B') \\\n",
    "                 .replace('[2H]', '[H]').replace('[3H]', '[H]')\n",
    "            s0 = Chem.CanonSmiles(s0, 0)\n",
    "            code = voc.encode([smile])\n",
    "            s1 = voc.decode(code)[0]\n",
    "            assert s0 == s1\n",
    "            codes.append(code[0].reshape(-1).tolist())\n",
    "            ids.append(idx)\n",
    "        except Exception as ex:\n",
    "            print(ex)\n",
    "            print('Parse Error:', idx)\n",
    "    df = pd.DataFrame(codes, index=ids, columns=['C%d' % i for i in range(64*4)])\n",
    "    df.to_csv(output, sep='\\t', index=True)\n",
    "    print(vals)\n",
    "    print(exps)\n",
    "\n",
    "\n",
    "def pair_frags(fname, out, method='Recap', is_mf=True):\n",
    "    smiles = pd.read_table(fname).Smiles.dropna()\n",
    "    pairs = []\n",
    "    for i, smile in enumerate(tqdm(smiles)):\n",
    "        smile = utils.clean_mol(smile)\n",
    "        mol = Chem.MolFromSmiles(smile)\n",
    "        if method == 'recap':\n",
    "            frags = np.array(sorted(Recap.RecapDecompose(mol).GetLeaves().keys()))\n",
    "        else:\n",
    "            frags = BRICS.BRICSDecompose(mol)\n",
    "            frags = np.array(sorted({re.sub(r'\\[\\d+\\*\\]', '*', f) for f in frags}))\n",
    "        if len(frags) == 1: continue\n",
    "        du, hy = Chem.MolFromSmiles('*'), Chem.MolFromSmiles('[H]')\n",
    "        subs = np.array([Chem.MolFromSmiles(f) for f in frags])\n",
    "        subs = np.array([Chem.RemoveHs(Chem.ReplaceSubstructs(f, du, hy, replaceAll=True)[0]) for f in subs])\n",
    "        subs = np.array([m for m in subs if m.GetNumAtoms() > 1])\n",
    "        match = np.array([[m.HasSubstructMatch(f) for f in subs] for m in subs])\n",
    "        frags = subs[match.sum(axis=0) == 1]\n",
    "        frags = sorted(frags, key=lambda x:-x.GetNumAtoms())[:voc.n_frags]\n",
    "        frags = [Chem.MolToSmiles(Chem.RemoveHs(f)) for f in frags]\n",
    "\n",
    "        max_comb = len(frags) if is_mf else 1\n",
    "        for ix in range(1, max_comb+1):\n",
    "            combs = combinations(frags, ix)\n",
    "            for comb in combs:\n",
    "                input = '.'.join(comb)\n",
    "                if len(input) > len(smile): continue\n",
    "                if mol.HasSubstructMatch(Chem.MolFromSmarts(input)):\n",
    "                    pairs.append([input, smile])\n",
    "    df = pd.DataFrame(pairs, columns=['Frags', 'Smiles'])\n",
    "    df.to_csv(out, sep='\\t',  index=False)\n",
    "\n",
    "\n",
    "def pair_graph_encode(fname, voc, out):\n",
    "    df = pd.read_table(fname)\n",
    "    col = ['C%d' % d for d in range(voc.max_len*5)]\n",
    "    codes = []\n",
    "    for i, row in tqdm(df.iterrows(), total=len(df)):\n",
    "        frag, smile = row.Frags, row.Smiles\n",
    "        # smile = voc_smi.decode(row.Output.split(' '))\n",
    "        # frag = voc_smi.decode(row.Input.split(' '))\n",
    "        mol = Chem.MolFromSmiles(smile)\n",
    "        total = mol.GetNumBonds()\n",
    "        if total >= 75 or smile == frag:\n",
    "            continue\n",
    "        try:\n",
    "            # s = utils.clean_mol(smile)\n",
    "            # f = utils.clean_mol(frag, is_deep=False)\n",
    "            output = voc.encode([smile], [frag])\n",
    "            f, s = voc.decode(output)\n",
    "\n",
    "            assert smile == s[0]\n",
    "            # assert f == frag[0]\n",
    "            code = output[0].reshape(-1).tolist()\n",
    "            codes.append(code)\n",
    "        except:\n",
    "            print(i, frag, smile)\n",
    "    codes = pd.DataFrame(codes, columns=col)\n",
    "    codes.to_csv(out, sep='\\t', index=False)\n",
    "\n",
    "\n",
    "def pair_smiles_encode(fname, voc, out):\n",
    "    df = pd.read_table(fname)\n",
    "    col = ['Input', 'Output']\n",
    "    codes = []\n",
    "    for i, row in tqdm(df.iterrows(), total=len(df)):\n",
    "        frag, smile = row.Frags, row.Smiles\n",
    "        mol = voc.split(smile)\n",
    "        if len(mol) > 100: continue\n",
    "        sub = voc.split(frag)\n",
    "        codes.append([' '.join(sub), ' '.join(mol)])\n",
    "    codes = pd.DataFrame(codes, columns=col)\n",
    "    codes.to_csv(out, sep='\\t', index=False)\n",
    "\n",
    "\n",
    "def pos_neg_split():\n",
    "    pair = ['Target ChEMBL ID', 'Smiles', 'pChEMBL Value', 'Comment',\n",
    "            'Standard Type', 'Standard Relation']\n",
    "    obj = pd.read_table('data/LIGAND.tsv').dropna(subset=pair[1:2])\n",
    "    df = obj[obj[pair[0]] == 'CHEMBL251']\n",
    "    df = df[pair].set_index(pair[1])\n",
    "    numery = df[pair[2]].groupby(pair[1]).mean().dropna()\n",
    "\n",
    "    comments = df[(df.Comment.str.contains('Not Active') == True)]\n",
    "    inhibits = df[(df['Standard Type'] == 'Inhibition') & df['Standard Relation'].isin(['<', '<='])]\n",
    "    relations = df[df['Standard Type'].isin(['EC50', 'IC50', 'Kd', 'Ki']) & df['Standard Relation'].isin(['>', '>='])]\n",
    "    binary = pd.concat([comments, inhibits, relations], axis=0)\n",
    "    binary = binary[~binary.index.isin(numery.index)]\n",
    "    binary[pair[2]] = 3.99\n",
    "    binary = binary[pair[2]].groupby(binary.index).first()\n",
    "    df = numery.append(binary)\n",
    "    pos = {utils.clean_mol(s) for s in df[df >=6.5].index}\n",
    "    neg = {utils.clean_mol(s) for s in df[df < 6.5].index}.difference(pos)\n",
    "    oth = obj[~obj.Smiles.isin(df.index)].Smiles\n",
    "    oth = {utils.clean_mol(s) for s in oth}.difference(pos).difference(neg)\n",
    "    for data in ['pos', 'neg', 'oth']:\n",
    "        file = open('data/ligand_%s.tsv' % data, 'w')\n",
    "        file.write('Smiles\\n')\n",
    "        file.write('\\n'.join(eval(data)))\n",
    "        file.close()\n",
    "\n",
    "\n",
    "def train_test_split(fname, out):\n",
    "    df = pd.read_table(fname)\n",
    "    frags = set(df.Frags)\n",
    "    test_in = df.Frags.drop_duplicates().sample(len(frags) // 10)\n",
    "    test = df[df.Frags.isin(test_in)]\n",
    "    train = df[~df.Frags.isin(test_in)]\n",
    "    test.to_csv(out + '_test.txt', sep='\\t', index=False)\n",
    "    train.to_csv(out + '_train.txt', sep='\\t', index=False)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    opts, args = getopt.getopt(sys.argv[1:], \"d:m:f:\")\n",
    "    OPT = dict(opts)\n",
    "    method = OPT.get('-m', 'brics')\n",
    "    dataset = OPT.get('-d', 'chembl')\n",
    "    is_mf = bool(OPT.get('-f', 1))\n",
    "    BATCH_SIZE = 256\n",
    "\n",
    "    corpus('data/LIGAND_RAW.tsv', 'data/ligand', suffix='tsv')\n",
    "    corpus('data/chembl_27.sdf.gz', 'data/chembl')\n",
    "\n",
    "    voc = utils.VocGraph('data/voc_graph.txt', n_frags=4)\n",
    "    voc_smi = utils.VocSmiles('data/voc_smiles.txt')\n",
    "    out = 'data/%s_%s_%s' % (dataset, 'mf' if is_mf else 'sf', method)\n",
    "    pair_frags('data/chembl_corpus.txt', out + '.txt', method=method, is_mf=is_mf)\n",
    "    pair_frags('data/ligand_corpus.txt', out + '.txt', method=method, is_mf=is_mf)\n",
    "    train_test_split('data/chembl_mf_brics.txt', 'data/chembl_mf_brics')\n",
    "    train_test_split('data/ligand_mf_brics.txt', 'data/ligand_mf_brics')\n",
    "    for ds in ['train']:\n",
    "        pair_graph_encode(out + '_%s.txt' % ds, voc, out + '_%s_code.txt' % ds)\n",
    "        pair_smiles_encode(out + '_%s.txt' % ds, voc_smi, out + '_%s_smi.txt' % ds)\n",
    "    pos_neg_split()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 environ.py:\n",
    "\n",
    "Training your predictor as the environment for providing the final reward for the action from the agent. The performance can also be evaluated through n-fold cross validation and independent test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
    "from sklearn.preprocessing import MinMaxScaler as Scaler\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC, SVR\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import models\n",
    "import os\n",
    "import utils\n",
    "import joblib\n",
    "from copy import deepcopy\n",
    "from rdkit import Chem\n",
    "\n",
    "\n",
    "def SVM(X, y, X_ind, y_ind, reg=False):\n",
    "    \"\"\" Cross validation and Independent test for SVM classifion/regression model.\n",
    "        Arguments:\n",
    "            X (np.ndarray): m x n feature matrix for cross validation, where m is the number of samples\n",
    "                and n is the number of features.\n",
    "            y (np.ndarray): m-d label array for cross validation, where m is the number of samples and\n",
    "                equals to row of X.\n",
    "            X_ind (np.ndarray): m x n Feature matrix for independent set, where m is the number of samples\n",
    "                and n is the number of features.\n",
    "            y_ind (np.ndarray): m-d label array for independent set, where m is the number of samples and\n",
    "                equals to row of X_ind, and l is the number of types.\n",
    "            reg (bool): it True, the training is for regression, otherwise for classification.\n",
    "         Returns:\n",
    "            cvs (np.ndarray): m x l result matrix for cross validation, where m is the number of samples and\n",
    "                equals to row of X, and l is the number of types and equals to row of X.\n",
    "            inds (np.ndarray): m x l result matrix for independent test, where m is the number of samples and\n",
    "                equals to row of X, and l is the number of types and equals to row of X.\n",
    "    \"\"\"\n",
    "    if reg:\n",
    "        folds = KFold(5).split(X)\n",
    "        alg = SVR()\n",
    "    else:\n",
    "        folds = StratifiedKFold(5).split(X, y)\n",
    "        alg = SVC(probability=True)\n",
    "    cvs = np.zeros(y.shape)\n",
    "    inds = np.zeros(y_ind.shape)\n",
    "    gs = GridSearchCV(deepcopy(alg), {'C': 2.0 ** np.array([-15, 15]), 'gamma': 2.0 ** np.array([-15, 15])}, n_jobs=10)\n",
    "    gs.fit(X, y)\n",
    "    params = gs.best_params_\n",
    "    print(params)\n",
    "    for i, (trained, valided) in enumerate(folds):\n",
    "        model = deepcopy(alg)\n",
    "        model.C = params['C']\n",
    "        model.gamma = params['gamma']\n",
    "        if not reg:\n",
    "            model.probability=True\n",
    "        model.fit(X[trained], y[trained], sample_weight=[1 if v >= 4 else 0.1 for v in y[trained]])\n",
    "        if reg:\n",
    "            cvs[valided] = model.predict(X[valided])\n",
    "            inds += model.predict(X_ind)\n",
    "        else:\n",
    "            cvs[valided] = model.predict_proba(X[valided])[:, 1]\n",
    "            inds += model.predict_proba(X_ind)[:, 1]\n",
    "    return cvs, inds / 5\n",
    "\n",
    "\n",
    "def RF(X, y, X_ind, y_ind, reg=False):\n",
    "    \"\"\" Cross validation and Independent test for RF classifion/regression model.\n",
    "        Arguments:\n",
    "            X (np.ndarray): m x n feature matrix for cross validation, where m is the number of samples\n",
    "                and n is the number of features.\n",
    "            y (np.ndarray): m-d label array for cross validation, where m is the number of samples and\n",
    "                equals to row of X.\n",
    "            X_ind (np.ndarray): m x n Feature matrix for independent set, where m is the number of samples\n",
    "                and n is the number of features.\n",
    "            y_ind (np.ndarray): m-d label array for independent set, where m is the number of samples and\n",
    "                equals to row of X_ind, and l is the number of types.\n",
    "            reg (bool): it True, the training is for regression, otherwise for classification.\n",
    "         Returns:\n",
    "            cvs (np.ndarray): m x l result matrix for cross validation, where m is the number of samples and\n",
    "                equals to row of X, and l is the number of types and equals to row of X.\n",
    "            inds (np.ndarray): m x l result matrix for independent test, where m is the number of samples and\n",
    "                equals to row of X, and l is the number of types and equals to row of X.\n",
    "    \"\"\"\n",
    "    if reg:\n",
    "        folds = KFold(5).split(X)\n",
    "        alg = RandomForestRegressor\n",
    "    else:\n",
    "        folds = StratifiedKFold(5).split(X, y)\n",
    "        alg = RandomForestClassifier\n",
    "    cvs = np.zeros(y.shape)\n",
    "    inds = np.zeros(y_ind.shape)\n",
    "    for i, (trained, valided) in enumerate(folds):\n",
    "        model = alg(n_estimators=1000, n_jobs=10)\n",
    "        model.fit(X[trained], y[trained], sample_weight=[1 if v >= 4 else 0.1 for v in y[trained]])\n",
    "        if reg:\n",
    "            cvs[valided] = model.predict(X[valided])\n",
    "            inds += model.predict(X_ind)\n",
    "        else:\n",
    "            cvs[valided] = model.predict_proba(X[valided])[:, 1]\n",
    "            inds += model.predict_proba(X_ind)[:, 1]\n",
    "    return cvs, inds / 5\n",
    "\n",
    "\n",
    "def KNN(X, y, X_ind, y_ind, reg=False):\n",
    "    \"\"\" Cross validation and Independent test for KNN classifion/regression model.\n",
    "        Arguments:\n",
    "            X (np.ndarray): m x n feature matrix for cross validation, where m is the number of samples\n",
    "                and n is the number of features.\n",
    "            y (np.ndarray): m-d label array for cross validation, where m is the number of samples and\n",
    "                equals to row of X.\n",
    "            X_ind (np.ndarray): m x n Feature matrix for independent set, where m is the number of samples\n",
    "                and n is the number of features.\n",
    "            y_ind (np.ndarray): m-d label array for independent set, where m is the number of samples and\n",
    "                equals to row of X_ind, and l is the number of types.\n",
    "            reg (bool): it True, the training is for regression, otherwise for classification.\n",
    "         Returns:\n",
    "            cvs (np.ndarray): m x l result matrix for cross validation, where m is the number of samples and\n",
    "                equals to row of X, and l is the number of types and equals to row of X.\n",
    "            inds (np.ndarray): m x l result matrix for independent test, where m is the number of samples and\n",
    "                equals to row of X, and l is the number of types and equals to row of X.\n",
    "    \"\"\"\n",
    "    if reg:\n",
    "        folds = KFold(5).split(X)\n",
    "        alg = KNeighborsRegressor\n",
    "    else:\n",
    "        folds = StratifiedKFold(5).split(X, y)\n",
    "        alg = KNeighborsClassifier\n",
    "    cvs = np.zeros(y.shape)\n",
    "    inds = np.zeros(y_ind.shape)\n",
    "    for i, (trained, valided) in enumerate(folds):\n",
    "        model = alg(n_jobs=10)\n",
    "        model.fit(X[trained], y[trained])\n",
    "        if reg:\n",
    "            cvs[valided] = model.predict(X[valided])\n",
    "            inds += model.predict(X_ind)\n",
    "        else:\n",
    "            cvs[valided] = model.predict_proba(X[valided])[:, 1]\n",
    "            inds += model.predict_proba(X_ind)[:, 1]\n",
    "    return cvs, inds / 5\n",
    "\n",
    "\n",
    "def NB(X, y, X_ind, y_ind):\n",
    "    \"\"\" Cross validation and Independent test for Naive Bayes classifion model.\n",
    "        Arguments:\n",
    "            X (np.ndarray): m x n feature matrix for cross validation, where m is the number of samples\n",
    "                and n is the number of features.\n",
    "            y (np.ndarray): m-d label array for cross validation, where m is the number of samples and\n",
    "                equals to row of X.\n",
    "            X_ind (np.ndarray): m x n Feature matrix for independent set, where m is the number of samples\n",
    "                and n is the number of features.\n",
    "            y_ind (np.ndarray): m-d label array for independent set, where m is the number of samples and\n",
    "                equals to row of X_ind, and l is the number of types.\n",
    "         Returns:\n",
    "            cvs (np.ndarray): m x l result matrix for cross validation, where m is the number of samples and\n",
    "                equals to row of X, and l is the number of types and equals to row of X.\n",
    "            inds (np.ndarray): m x l result matrix for independent test, where m is the number of samples and\n",
    "                equals to row of X, and l is the number of types and equals to row of X.\n",
    "    \"\"\"\n",
    "    folds = KFold(5).split(X)\n",
    "    cvs = np.zeros(y.shape)\n",
    "    inds = np.zeros(y_ind.shape)\n",
    "    for i, (trained, valided) in enumerate(folds):\n",
    "        model = GaussianNB()\n",
    "        model.fit(X[trained], y[trained], sample_weight=[1 if v >= 4 else 0.1 for v in y[trained]])\n",
    "        cvs[valided] = model.predict_proba(X[valided])[:, 1]\n",
    "        inds += model.predict_proba(X_ind)[:, 1]\n",
    "    return cvs, inds / 5\n",
    "\n",
    "\n",
    "def PLS(X, y, X_ind, y_ind):\n",
    "    \"\"\" Cross validation and Independent test for PLS regression model.\n",
    "        Arguments:\n",
    "            X (np.ndarray): m x n feature matrix for cross validation, where m is the number of samples\n",
    "                and n is the number of features.\n",
    "            y (np.ndarray): m-d label array for cross validation, where m is the number of samples and\n",
    "                equals to row of X.\n",
    "            X_ind (np.ndarray): m x n Feature matrix for independent set, where m is the number of samples\n",
    "                and n is the number of features.\n",
    "            y_ind (np.ndarray): m-d label array for independent set, where m is the number of samples and\n",
    "                equals to row of X_ind, and l is the number of types.\n",
    "            reg (bool): it True, the training is for regression, otherwise for classification.\n",
    "         Returns:\n",
    "            cvs (np.ndarray): m x l result matrix for cross validation, where m is the number of samples and\n",
    "                equals to row of X, and l is the number of types and equals to row of X.\n",
    "            inds (np.ndarray): m x l result matrix for independent test, where m is the number of samples and\n",
    "                equals to row of X, and l is the number of types and equals to row of X.\n",
    "    \"\"\"\n",
    "    folds = KFold(5).split(X)\n",
    "    cvs = np.zeros(y.shape)\n",
    "    inds = np.zeros(y_ind.shape)\n",
    "    for i, (trained, valided) in enumerate(folds):\n",
    "        model = PLSRegression()\n",
    "        model.fit(X[trained], y[trained])\n",
    "        cvs[valided] = model.predict(X[valided])[:, 0]\n",
    "        inds += model.predict(X_ind)[:, 0]\n",
    "    return cvs, inds / 5\n",
    "\n",
    "\n",
    "def DNN(X, y, X_ind, y_ind, out, reg=False):\n",
    "    \"\"\" Cross validation and Independent test for DNN classifion/regression model.\n",
    "        Arguments:\n",
    "            X (np.ndarray): m x n feature matrix for cross validation, where m is the number of samples\n",
    "                and n is the number of features.\n",
    "            y (np.ndarray): m x l label matrix for cross validation, where m is the number of samples and\n",
    "                equals to row of X, and l is the number of types.\n",
    "            X_ind (np.ndarray): m x n Feature matrix for independent set, where m is the number of samples\n",
    "                and n is the number of features.\n",
    "            y_ind (np.ndarray): m-d label arrays for independent set, where m is the number of samples and\n",
    "                equals to row of X_ind, and l is the number of types.\n",
    "            reg (bool): it True, the training is for regression, otherwise for classification.\n",
    "         Returns:\n",
    "            cvs (np.ndarray): m x l result matrix for cross validation, where m is the number of samples and\n",
    "                equals to row of X, and l is the number of types and equals to row of X.\n",
    "            inds (np.ndarray): m x l result matrix for independent test, where m is the number of samples and\n",
    "                equals to row of X, and l is the number of types and equals to row of X.\n",
    "    \"\"\"\n",
    "    if y.shape[1] > 1 or reg:\n",
    "        folds = KFold(5).split(X)\n",
    "    else:\n",
    "        folds = StratifiedKFold(5).split(X, y[:, 0])\n",
    "    NET = models.STFullyConnected if y.shape[1] == 1 else models.MTFullyConnected\n",
    "    indep_set = TensorDataset(torch.Tensor(X_ind), torch.Tensor(y_ind))\n",
    "    indep_loader = DataLoader(indep_set, batch_size=BATCH_SIZE)\n",
    "    cvs = np.zeros(y.shape)\n",
    "    inds = np.zeros(y_ind.shape)\n",
    "    for i, (trained, valided) in enumerate(folds):\n",
    "        train_set = TensorDataset(torch.Tensor(X[trained]), torch.Tensor(y[trained]))\n",
    "        train_loader = DataLoader(train_set, batch_size=BATCH_SIZE)\n",
    "        valid_set = TensorDataset(torch.Tensor(X[valided]), torch.Tensor(y[valided]))\n",
    "        valid_loader = DataLoader(valid_set, batch_size=BATCH_SIZE)\n",
    "        net = NET(X.shape[1], y.shape[1], is_reg=reg)\n",
    "        net.fit(train_loader, valid_loader, out='%s_%d' % (out, i), epochs=N_EPOCH, lr=LR)\n",
    "        cvs[valided] = net.predict(valid_loader)\n",
    "        inds += net.predict(indep_loader)\n",
    "    return cvs, inds / 5\n",
    "\n",
    "\n",
    "def Train_RF(X, y, out, reg=False):\n",
    "    if reg:\n",
    "        model = RandomForestRegressor(n_estimators=1000, n_jobs=10)\n",
    "    else:\n",
    "        model = RandomForestClassifier(n_estimators=1000, n_jobs=10)\n",
    "    model.fit(X, y, sample_weight=[1 if v >= 4 else 0.1 for v in y])\n",
    "    joblib.dump(model, out, compress=3)\n",
    "\n",
    "\n",
    "def mt_task(fname, out, reg=False, is_extra=True, time_split=False):\n",
    "    df = pd.read_table(fname)[pair].dropna(subset=pair[1:2])\n",
    "    df = df[df.Target_ChEMBL_ID.isin(trgs)]\n",
    "    year = df.groupby(pair[1])[pair[-1:]].min().dropna()\n",
    "    year = year[year.Document_Year > 2015].index\n",
    "    df = df[pair].set_index(pair[0:2])\n",
    "    numery = df[pair[2]].groupby(pair[0:2]).mean().dropna()\n",
    "\n",
    "    comments = df[(df.Comment.str.contains('Not Active') == True)]\n",
    "    inhibits = df[(df.Standard_Type == 'Inhibition') & df.Standard_Relation.isin(['<', '<='])]\n",
    "    relations = df[df.Standard_Type.isin(['EC50', 'IC50', 'Kd', 'Ki']) & df.Standard_Relation.isin(['>', '>='])]\n",
    "    binary = pd.concat([comments, inhibits, relations], axis=0)\n",
    "    binary = binary[~binary.index.isin(numery.index)]\n",
    "    binary[pair[2]] = 3.99\n",
    "    binary = binary[pair[2]].groupby(pair[0:2]).first()\n",
    "    df = numery.append(binary) if is_extra else numery\n",
    "    if not reg:\n",
    "        df[pair[2]] = (df[pair[2]] > th).astype(float)\n",
    "    df = df.unstack(pair[0])\n",
    "    test_ix = set(df.index).intersection(year)\n",
    "\n",
    "    df_test = df.loc[test_ix] if time_split else df.sample(len(test_ix))\n",
    "    df_data = df.drop(df_test.index)\n",
    "    df_data = df_data.sample(len(df_data))\n",
    "    for alg in ['RF', 'MT_DNN', 'SVM', 'PLS', 'KNN', 'DNN']:\n",
    "        if alg == 'MT_DNN':\n",
    "            test_x = utils.Predictor.calc_fp([Chem.MolFromSmiles(mol) for mol in df_test.index])\n",
    "            data_x = utils.Predictor.calc_fp([Chem.MolFromSmiles(mol) for mol in df_data.index])\n",
    "            scaler = Scaler(); scaler.fit(data_x)\n",
    "            test_x = scaler.transform(test_x)\n",
    "            data_x = scaler.transform(data_x)\n",
    "\n",
    "            data = df_data.stack().to_frame(name='Label')\n",
    "            test = df_test.stack().to_frame(name='Label')\n",
    "            data_p, test_p = DNN(data_x, df_data.values, test_x, df_test.values, out=out, reg=reg)\n",
    "            data['Score'] = pd.DataFrame(data_p, index=df_data.index, columns=df_data.columns).stack()\n",
    "            test['Score'] = pd.DataFrame(test_p, index=df_test.index, columns=df_test.columns).stack()\n",
    "            data.to_csv(out + alg + '_LIGAND.cv.tsv', sep='\\t')\n",
    "            test.to_csv(out + alg + '_LIGAND.ind.tsv', sep='\\t')\n",
    "        else:\n",
    "            for trg in trgs:\n",
    "                test_y = df_test[trg].dropna()\n",
    "                data_y = df_data[trg].dropna()\n",
    "                test_x = utils.Predictor.calc_fp([Chem.MolFromSmiles(mol) for mol in test_y.index])\n",
    "                data_x = utils.Predictor.calc_fp([Chem.MolFromSmiles(mol) for mol in data_y.index])\n",
    "                if alg != 'RF':\n",
    "                    scaler = Scaler(); scaler.fit(data_x)\n",
    "                    test_x = scaler.transform(test_x)\n",
    "                    data_x = scaler.transform(data_x)\n",
    "                else:\n",
    "                    X = np.concatenate([data_x, test_x], axis=0)\n",
    "                    y = np.concatenate([data_y.values, test_y.values], axis=0)\n",
    "                    Train_RF(X, y, out=out + '%s_%s.pkg' % (alg, trg), reg=reg)\n",
    "                data, test = data_y.to_frame(name='Label'), test_y.to_frame(name='Label')\n",
    "                a, b = cross_validation(data_x, data.values, test_x, test.values,\n",
    "                                        alg, out + '%s_%s' % (alg, trg), reg=reg)\n",
    "                data['Score'], test['Score'] = a, b\n",
    "                data.to_csv(out + '%s_%s.cv.tsv' % (alg, trg), sep='\\t')\n",
    "                test.to_csv(out + '%s_%s.ind.tsv' % (alg, trg), sep='\\t')\n",
    "\n",
    "\n",
    "def single_task(feat, alg='RF', reg=False, is_extra=True):\n",
    "    df = pd.read_table('data/LIGAND_RAW.tsv').dropna(subset=pair[1:2])\n",
    "    df = df[df[pair[0]] == feat]\n",
    "    df = df[pair].set_index(pair[1])\n",
    "    year = df[pair[-1:]].groupby(pair[1]).min().dropna()\n",
    "    test = year[year[pair[-1]] > 2015].index\n",
    "    numery = df[pair[2]].groupby(pair[1]).mean().dropna()\n",
    "\n",
    "    comments = df[(df.Comment.str.contains('Not Active') == True)]\n",
    "    inhibits = df[(df.Standard_Type == 'Inhibition') & df.Standard_Relation.isin(['<', '<='])]\n",
    "    relations = df[df.Standard_Type.isin(['EC50', 'IC50', 'Kd', 'Ki']) & df.Standard_Relation.isin(['>', '>='])]\n",
    "    binary = pd.concat([comments, inhibits, relations], axis=0)\n",
    "    binary = binary[~binary.index.isin(numery.index)]\n",
    "    binary[pair[2]] = 3.99\n",
    "    binary = binary[pair[2]].groupby(binary.index).first()\n",
    "    df = numery.append(binary) if is_extra else numery\n",
    "    if not reg:\n",
    "        df = (df > th).astype(float)\n",
    "    df = df.sample(len(df))\n",
    "    print(feat, len(numery[numery >= th]), len(numery[numery < th]), len(binary))\n",
    "\n",
    "    test_ix = set(df.index).intersection(test)\n",
    "    test = df.loc[test_ix].dropna()\n",
    "    data = df.drop(test.index)\n",
    "\n",
    "    test_x = utils.Predictor.calc_fp([Chem.MolFromSmiles(mol) for mol in test.index])\n",
    "    data_x = utils.Predictor.calc_fp([Chem.MolFromSmiles(mol) for mol in data.index])\n",
    "    out = 'output/single/%s_%s_%s' % (alg, 'REG' if reg else 'CLS', feat)\n",
    "    if alg != 'RF':\n",
    "        scaler = Scaler(); scaler.fit(data_x)\n",
    "        test_x = scaler.transform(test_x)\n",
    "        data_x = scaler.transform(data_x)\n",
    "    else:\n",
    "        X = np.concatenate([data_x, test_x], axis=0)\n",
    "        y = np.concatenate([data.values, test.values], axis=0)\n",
    "        Train_RF(X, y[:, 0], out=out + '.pkg', reg=reg)\n",
    "    data, test = data.to_frame(name='Label'), test.to_frame(name='Label')\n",
    "    data['Score'], test['Score'] = cross_validation(data_x, data.values, test_x, test.values, alg, out, reg=reg)\n",
    "    data.to_csv(out + '.cv.tsv', sep='\\t')\n",
    "    test.to_csv(out + '.ind.tsv', sep='\\t')\n",
    "\n",
    "\n",
    "def cross_validation(X, y, X_ind, y_ind, alg='DNN', out=None, reg=False):\n",
    "    if alg == 'RF':\n",
    "        cv, ind = RF(X, y[:, 0], X_ind, y_ind[:, 0], reg=reg)\n",
    "    elif alg == 'SVM':\n",
    "        cv, ind = SVM(X, y[:, 0], X_ind, y_ind[:, 0], reg=reg)\n",
    "    elif alg == 'KNN':\n",
    "        cv, ind = KNN(X, y[:, 0], X_ind, y_ind[:, 0], reg=reg)\n",
    "    elif alg == 'NB':\n",
    "        cv, ind = NB(X, y[:, 0], X_ind, y_ind[:, 0])\n",
    "    elif alg == 'PLS':\n",
    "        cv, ind = PLS(X, y[:, 0], X_ind, y_ind[:, 0])\n",
    "    elif alg == 'DNN':\n",
    "        cv, ind = DNN(X, y, X_ind, y_ind, out=out, reg=reg)\n",
    "    return cv, ind\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    pair = ['Target_ChEMBL_ID', 'Smiles', 'pChEMBL_Value', 'Comment',\n",
    "            'Standard_Type', 'Standard_Relation', 'Document_Year']\n",
    "    BATCH_SIZE = int(2 ** 11)\n",
    "    N_EPOCH = 1000\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "    th= 6.5\n",
    "    trgs = ['CHEMBL226', 'CHEMBL251', 'CHEMBL240']\n",
    "\n",
    "    for reg in [False, True]:\n",
    "        LR = 1e-4 if reg else 1e-5\n",
    "        for chembl in trgs:\n",
    "            single_task(chembl, 'DNN', reg=reg)\n",
    "            single_task(chembl, 'RF', reg=reg)\n",
    "            single_task(chembl, 'SVM', reg=reg)\n",
    "            if reg:\n",
    "                single_task(chembl, 'PLS', reg=reg)\n",
    "            else:\n",
    "                single_task(chembl, 'NB', reg=reg)\n",
    "            single_task(chembl, 'KNN', reg=reg)\n",
    "\n",
    "    mt_task('data/LIGAND_RAW.tsv', 'output/random_split/', reg=reg, time_split=False)\n",
    "    mt_task('data/LIGAND_RAW.tsv', 'output/time_split/', reg=reg, time_split=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 train_graph.py:\n",
    "\n",
    "Pre-training an training the graph transformer model with graph representation under supvervision and reinforcement learning frameworks, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "import torch\n",
    "from rdkit import rdBase\n",
    "from models.explorer import GraphExplorer\n",
    "import utils\n",
    "import pandas as pd\n",
    "from models import GraphModel\n",
    "from torch.utils.data import DataLoader\n",
    "import getopt\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "from shutil import copy2\n",
    "\n",
    "np.random.seed(2)\n",
    "torch.manual_seed(2)\n",
    "rdBase.DisableLog('rdApp.error')\n",
    "torch.set_num_threads(1)\n",
    "\n",
    "\n",
    "def pretrain():\n",
    "    out = 'output/%s_graph_%d' % (dataset, BATCH_SIZE)\n",
    "    agent.fit(valid_loader, valid_loader, epochs=1000, out=out)\n",
    "\n",
    "\n",
    "def train_ex():\n",
    "    agent.load_state_dict(torch.load(params['pr_path'] + '.pkg', map_location=utils.dev))\n",
    "\n",
    "    prior = GraphModel(voc)\n",
    "    prior.load_state_dict(torch.load(params['ft_path'] + '.pkg', map_location=utils.dev))\n",
    "\n",
    "    evolver = GraphExplorer(agent, mutate=prior)\n",
    "\n",
    "    evolver.batch_size = BATCH_SIZE\n",
    "    evolver.epsilon = float(OPT.get('-e', '1e-2'))\n",
    "    evolver.sigma = float(OPT.get('-b', '0.00'))\n",
    "    evolver.scheme = OPT.get('-s', 'WS')\n",
    "    evolver.repeat = 1\n",
    "\n",
    "    keys = ['A2A', 'QED']\n",
    "    A2A = utils.Predictor('output/env/RF_%s_CHEMBL251.pkg' % z, type=z)\n",
    "    QED = utils.Property('QED')\n",
    "\n",
    "    # Chose the desirability function\n",
    "    objs = [A2A, QED]\n",
    "\n",
    "    if evolver.scheme == 'WS':\n",
    "        mod1 = utils.ClippedScore(lower_x=3, upper_x=10)\n",
    "        mod2 = utils.ClippedScore(lower_x=0, upper_x=1.0)\n",
    "        ths = [0.5, 0]\n",
    "    else:\n",
    "        mod1 = utils.ClippedScore(lower_x=3, upper_x=6.5)\n",
    "        mod2 = utils.ClippedScore(lower_x=0, upper_x=1.0)\n",
    "        ths = [0.99, 0]\n",
    "    mods = [mod1, mod2]\n",
    "    evolver.env = utils.Env(objs=objs, mods=mods, keys=keys, ths=ths)\n",
    "\n",
    "    # import evolve as agent\n",
    "    evolver.out = root + '/%s_%s_%.0e' % (alg, evolver.scheme, evolver.epsilon)\n",
    "    evolver.fit(train_loader, test_loader=valid_loader)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    params = {'pr_path': 'output/ligand_mf_brics_graph_256', 'ft_path': 'output/ligand_mf_brics_graph_256'}\n",
    "    opts, args = getopt.getopt(sys.argv[1:], \"a:e:b:d:g:s:\")\n",
    "    OPT = dict(opts)\n",
    "    z = OPT.get('-z', 'REG')\n",
    "    alg = OPT.get('-a', 'graph')\n",
    "    devs = OPT.get('-g', \"0\")\n",
    "    utils.devices = eval(devs) if ',' in devs else [eval(devs)]\n",
    "    torch.cuda.set_device(utils.devices[0])\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = devs\n",
    "\n",
    "    BATCH_SIZE = int(OPT.get('-b', '128'))\n",
    "    dataset = OPT.get('-d', 'ligand_mf_brics')\n",
    "\n",
    "    voc = utils.VocGraph('data/voc_atom.txt', max_len=80, n_frags=4)\n",
    "    data = pd.read_table('data/%s_train_code.txt' % dataset)\n",
    "    data = torch.from_numpy(data.values).long().view(len(data), voc.max_len, -1)\n",
    "    train_loader = DataLoader(data, batch_size=BATCH_SIZE * 4, drop_last=True, shuffle=True)\n",
    "\n",
    "    test = pd.read_table('data/%s_test_code.txt' % dataset)\n",
    "    # test = test.sample(int(1e4))\n",
    "    test = torch.from_numpy(test.values).long().view(len(test), voc.max_len, -1)\n",
    "    valid_loader = DataLoader(test, batch_size=BATCH_SIZE * 10, drop_last=True, shuffle=True)\n",
    "\n",
    "    agent = GraphModel(voc).to(utils.dev)\n",
    "    root = 'output/%s_%s' % (alg, time.strftime('%y%m%d_%H%M%S', time.localtime()))\n",
    "\n",
    "    os.mkdir(root)\n",
    "    copy2(alg + '_ex.py', root)\n",
    "    copy2(alg + '.py', root)\n",
    "\n",
    "    pretrain()\n",
    "    train_ex()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.train_smiles.py:\n",
    "\n",
    "Pre-training an training the SMILES-based deep learning models with SMILES representation under supvervision and reinforcement learning frameworks, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "import os\n",
    "import pandas as pd\n",
    "from shutil import copy2\n",
    "import utils\n",
    "import getopt\n",
    "import sys\n",
    "import time\n",
    "import torch\n",
    "from models import GPT2Model\n",
    "from models import generator\n",
    "from models.explorer import SmilesExplorer\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "\n",
    "def pretrain(method='gpt'):\n",
    "    if method == 'ved':\n",
    "        agent = generator.EncDec(voc, voc).to(utils.dev)\n",
    "    elif method == 'attn':\n",
    "        agent = generator.Seq2Seq(voc, voc).to(utils.dev)\n",
    "    else:\n",
    "        agent = GPT2Model(voc, n_layer=12).to(utils.dev)\n",
    "\n",
    "    out = 'output/%s_%s_%d' % (dataset, method, BATCH_SIZE)\n",
    "    agent.fit(data_loader, test_loader, epochs=1000, out=out)\n",
    "\n",
    "\n",
    "def rl_train():\n",
    "    opts, args = getopt.getopt(sys.argv[1:], \"a:e:b:g:c:s:z:\")\n",
    "    OPT = dict(opts)\n",
    "    case = OPT['-c'] if '-c' in OPT else 'OBJ1'\n",
    "    z = OPT['-z'] if '-z' in OPT else 'REG'\n",
    "    alg = OPT['-a'] if '-a' in OPT else 'smile'\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = OPT['-g'] if '-g' in OPT else \"0,1,2,3\"\n",
    "\n",
    "    voc = utils.VocSmiles(init_from_file=\"data/chembl_voc.txt\", max_len=100)\n",
    "    agent = GPT2Model(voc, n_layer=12)\n",
    "    agent.load_state_dict(torch.load(params['pr_path'] + '.pkg', map_location=utils.dev))\n",
    "\n",
    "    prior = GPT2Model(voc, n_layer=12)\n",
    "    prior.load_state_dict(torch.load(params['ft_path'] + '.pkg', map_location=utils.dev))\n",
    "\n",
    "    evolver = SmilesExplorer(agent, mutate=prior)\n",
    "\n",
    "    evolver.batch_size = BATCH_SIZE\n",
    "    evolver.epsilon = float(OPT.get('-e', '1e-2'))\n",
    "    evolver.sigma = float(OPT.get('-b', '0.00'))\n",
    "    evolver.scheme = OPT.get('-s', 'WS')\n",
    "    evolver.repeat = 1\n",
    "\n",
    "    keys = ['A2A', 'QED']\n",
    "    A2A = utils.Predictor('output/env/RF_%s_CHEMBL251.pkg' % z, type=z)\n",
    "    QED = utils.Property('QED')\n",
    "\n",
    "    # Chose the desirability function\n",
    "    objs = [A2A, QED]\n",
    "\n",
    "    if evolver.scheme == 'WS':\n",
    "        mod1 = utils.ClippedScore(lower_x=3, upper_x=10)\n",
    "        mod2 = utils.ClippedScore(lower_x=0, upper_x=1)\n",
    "        ths = [0.5, 0]\n",
    "    else:\n",
    "        mod1 = utils.ClippedScore(lower_x=3, upper_x=6.5)\n",
    "        mod2 = utils.ClippedScore(lower_x=0, upper_x=0.5)\n",
    "        ths = [0.99, 0]\n",
    "    mods = [mod1, mod2] if case == 'OBJ3' else [mod1, mod2]\n",
    "    evolver.env = utils.Env(objs=objs, mods=mods, keys=keys, ths=ths)\n",
    "\n",
    "    root = 'output/%s_%s' % (alg, time.strftime('%y%m%d_%H%M%S', time.localtime()))\n",
    "\n",
    "    os.mkdir(root)\n",
    "    copy2(alg + '_ex.py', root)\n",
    "    copy2(alg + '.py', root)\n",
    "\n",
    "    # import evolve as agent\n",
    "    evolver.out = root + '/%s_%s_%s_%s_%.0e' % (alg, evolver.scheme, z, case, evolver.epsilon)\n",
    "    evolver.fit(data_loader, test_loader=test_loader)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    params = {'pr_path': 'output/ligand_mf_brics_gpt_256', 'ft_path': 'output/ligand_mf_brics_gpt_256'}\n",
    "    opts, args = getopt.getopt(sys.argv[1:], \"m:g:b:d:\")\n",
    "    OPT = dict(opts)\n",
    "    torch.cuda.set_device(0)\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = OPT.get('-g', \"0,1,2,3\")\n",
    "    method = OPT.get('-m', 'gpt')\n",
    "    step = OPT['-s']\n",
    "    BATCH_SIZE = int(OPT.get('-b', '256'))\n",
    "    dataset = OPT.get('-d', 'ligand_mf_brics')\n",
    "\n",
    "    data = pd.read_table('data/%s_train_smi.txt' % dataset)\n",
    "    test = pd.read_table('data/%s_test_smi.txt' % dataset)\n",
    "    test = test.Input.drop_duplicates().sample(BATCH_SIZE * 10).values\n",
    "    if method in ['gpt']:\n",
    "        voc = utils.Voc('data/voc_smiles.txt', src_len=100, trg_len=100)\n",
    "    else:\n",
    "        voc = utils.VocSmiles('data/voc_smiles.txt', max_len=100)\n",
    "    data_in = voc.encode([seq.split(' ') for seq in data.Input.values])\n",
    "    data_out = voc.encode([seq.split(' ') for seq in data.Output.values])\n",
    "    data_set = TensorDataset(data_in, data_out)\n",
    "    data_loader = DataLoader(data_set, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "    test_set = voc.encode([seq.split(' ') for seq in test])\n",
    "    test_set = utils.TgtData(test_set, ix=[voc.decode(seq, is_tk=False) for seq in test_set])\n",
    "    test_loader = DataLoader(test_set, batch_size=BATCH_SIZE, collate_fn=test_set.collate_fn)\n",
    "\n",
    "    pretrain(method=method)\n",
    "    rl_train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. designer.py:\n",
    "\n",
    "Finally, generating molecules with well-trained deep learning model with either graph or SMILES representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "import torch\n",
    "from rdkit import rdBase\n",
    "from models import generator\n",
    "import utils\n",
    "import pandas as pd\n",
    "from models import GPT2Model, GraphModel\n",
    "from torch.utils.data import DataLoader\n",
    "import getopt\n",
    "import sys\n",
    "import os\n",
    "\n",
    "\n",
    "rdBase.DisableLog('rdApp.error')\n",
    "torch.set_num_threads(1)\n",
    "BATCH_SIZE = 1024\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    opts, args = getopt.getopt(sys.argv[1:], \"m:d:g:p:\")\n",
    "    OPT = dict(opts)\n",
    "    # torch.cuda.set_device(0)\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = OPT['-g'] if '-g' in OPT else \"0, 1, 2, 3\"\n",
    "    method = OPT['-m'] if '-m' in OPT else 'atom'\n",
    "    dataset = OPT['-d'] if '-d' in OPT else 'ligand_mf_brics'\n",
    "    path = OPT['-p'] if '-p' in OPT else dataset\n",
    "    utils.devices = [0]\n",
    "\n",
    "    if method in ['gpt']:\n",
    "        voc = utils.Voc('data/chembl_voc.txt', src_len=100, trg_len=100)\n",
    "    else:\n",
    "        voc = utils.VocSmiles('data/chembl_voc.txt', max_len=100)\n",
    "    if method == 'ved':\n",
    "        agent = generator.EncDec(voc, voc).to(utils.dev)\n",
    "    elif method == 'attn':\n",
    "        agent = generator.Seq2Seq(voc, voc).to(utils.dev)\n",
    "    elif method == 'gpt':\n",
    "        agent = GPT2Model(voc, n_layer=12).to(utils.dev)\n",
    "    else:\n",
    "        voc = utils.VocGraph('data/voc_atom.txt')\n",
    "        agent = GraphModel(voc_trg=voc)\n",
    "\n",
    "    for agent_path in ['benchmark/graph_PR_REG_OBJ1_0e+00.pkg', 'benchmark/graph_PR_REG_OBJ1_1e-01.pkg',\n",
    "                       'benchmark/graph_PR_REG_OBJ1_1e-02.pkg', 'benchmark/graph_PR_REG_OBJ1_1e-03.pkg',\n",
    "                       'benchmark/graph_PR_REG_OBJ1_1e-04.pkg', 'benchmark/graph_PR_REG_OBJ1_1e-05.pkg']:\n",
    "        # agent_path = 'output/%s_%s_256.pkg' % (path, method)\n",
    "        print(agent_path)\n",
    "        agent.load_state_dict(torch.load(agent_path))\n",
    "\n",
    "        z = 'REG'\n",
    "        keys = ['A2A']\n",
    "        A2A = utils.Predictor('output/env/RF_%s_CHEMBL251.pkg' % z, type=z)\n",
    "        QED = utils.Property('QED')\n",
    "\n",
    "        # Chose the desirability function\n",
    "        objs = [A2A, QED]\n",
    "\n",
    "        ths = [6.5, 0.0]\n",
    "\n",
    "        env =  utils.Env(objs=objs, mods=None, keys=keys, ths=ths)\n",
    "        if method in ['atom']:\n",
    "            data = pd.read_table('data/ligand_mf_brics_test.txt')\n",
    "            # data = data.sample(BATCH_SIZE * 10)\n",
    "            data = torch.from_numpy(data.values).long().view(len(data), voc.max_len, -1)\n",
    "            loader = DataLoader(data, batch_size=BATCH_SIZE)\n",
    "\n",
    "            out = '%s.txt' % agent_path\n",
    "        else:\n",
    "            data = pd.read_table('data/%s_test.txt' % dataset).Input.drop_duplicates()\n",
    "            # data = data.sample(BATCH_SIZE * 10)\n",
    "            data = voc.encode([seq.split(' ')[:-1] for seq in data.values])\n",
    "            loader = DataLoader(data, batch_size=BATCH_SIZE)\n",
    "\n",
    "            out = agent_path + '.txt'\n",
    "        frags, smiles, scores = agent.evaluate(loader, repeat=10, method=env)\n",
    "        scores['Frags'] = frags\n",
    "        scores['Smiles'] = smiles\n",
    "        scores.to_csv(out, index=False, sep='\\t')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. plot.py:\n",
    "\n",
    "It provides a variety of the methods to measure the performance of every step during the training process of DrugEx, and form the figure for results visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit import Chem\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from rdkit.Chem import Draw\n",
    "from utils.metric import logP_mw, dimension\n",
    "import seaborn as sns\n",
    "from matplotlib_venn import venn3\n",
    "from scipy import stats\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "def figure3(out='Figure_3.tif'):\n",
    "    fig = plt.figure(figsize=(8, 8))\n",
    "    dataset = ['LIGAND+', 'LIGAND-', 'LIGAND0', 'ChEMBL']\n",
    "    ax1 = fig.add_subplot(221)\n",
    "    num = pd.DataFrame(columns=['Num', 'Set'])\n",
    "    for ds in dataset:\n",
    "        sub = pd.read_table('figures/%s_num.txt' % ds, dtype=float)\n",
    "        sub['Set'] = ds\n",
    "        num = num.append(sub)\n",
    "    num = num.dropna()\n",
    "    sns.set(style=\"white\", palette=\"pastel\", color_codes=True)\n",
    "    sns.violinplot(x='Set', y='Num', data=num, order=dataset, linewidth=1.5, bw=0.8)\n",
    "    plt.text(0.02, 0.95, chr(ord('A')), fontweight=\"bold\", transform=ax1.transAxes)\n",
    "    ax1.set(ylim=[0.0, 15.0], xlabel='Dataset', ylabel='Number of Fragments per Molecule')\n",
    "\n",
    "    frags = []\n",
    "    ax2 = fig.add_subplot(222)\n",
    "    for ds in dataset:\n",
    "        sub = pd.read_table('figures/%s_frag.txt' % ds)\n",
    "        frag = set(sub['Frags'])\n",
    "        frags.append(frag)\n",
    "        sns.kdeplot(sub['MW'], shade=True, linewidth=1.5, label=ds)\n",
    "        plt.text(0.02, 0.95, chr(ord('B')), fontweight=\"bold\", transform=ax2.transAxes)\n",
    "    ax2.set(xlabel='Molecular Weight', ylabel='Value')\n",
    "\n",
    "    ax3 = fig.add_subplot(223)\n",
    "    for ds in dataset:\n",
    "        sub = np.loadtxt('figures/%s_div.txt' % ds)\n",
    "        np.fill_diagonal(sub, np.NaN)\n",
    "        sub = sub[sub == sub]\n",
    "        sns.kdeplot(sub, shade=True, linewidth=1.5, label=ds)\n",
    "    plt.text(0.02, 0.95, chr(ord('C')), fontweight=\"bold\", transform=ax3.transAxes)\n",
    "    ax3.set(xlim=[0.0, 0.5], xlabel='Tanimoto Similarity', ylabel='Value')\n",
    "\n",
    "    ax4 = fig.add_subplot(224)\n",
    "    venn3(frags[:-1], set_labels=dataset)\n",
    "    plt.text(0.02, 0.95, chr(ord('D')), fontweight=\"bold\", transform=ax4.transAxes)\n",
    "    fig.subplots_adjust(wspace=0.5, hspace=0.5)\n",
    "    # plt.tight_layout()\n",
    "    if out is None:\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.savefig(out, dpi=600, bbox_inches = \"tight\", pil_kwargs={\"compression\": \"tiff_lzw\"})\n",
    "\n",
    "\n",
    "def figure4():\n",
    "    fnames = ['data/chembl_mf_brics_test.txt', 'benchmark/chembl_mix.txt']\n",
    "    labels, keys = [], []\n",
    "    fig = plt.figure(figsize=(12, 8))\n",
    "    lab = ['ChEMBL Set', 'Pre-trained Model']\n",
    "\n",
    "    ax1 = fig.add_subplot(231)\n",
    "    df = logP_mw(fnames)\n",
    "    group0, group1 = df[df.LABEL == 0], df[df.LABEL == 1]\n",
    "    plt.text(0.05, 0.9, chr(ord('A')), fontweight=\"bold\", transform=ax1.transAxes)\n",
    "    ax1.scatter(group0.MWT, group0.LOGP, s=1, marker='o', label=lab[0], c='', edgecolor=colors[0])\n",
    "    ax1.scatter(group1.MWT, group1.LOGP, s=10, marker='o', label=lab[1], c='', edgecolor=colors[1])\n",
    "    ax1.set(ylabel='LogP', xlabel='Molecular Weight', xlim=[0, 1000], ylim=[-5, 10])\n",
    "    handle, label = ax1.get_legend_handles_labels()\n",
    "    labels.extend(handle)\n",
    "    keys.extend(label)\n",
    "\n",
    "    ax2 = fig.add_subplot(232)\n",
    "    df, ratio = dimension(fnames, fp='physchem')\n",
    "    group0, group1 = df[df.LABEL == 0], df[df.LABEL == 1]\n",
    "    plt.text(0.05, 0.9, chr(ord('C')), fontweight=\"bold\", transform=ax2.transAxes)\n",
    "    ax2.scatter(group0.X, group0.Y, s=1, marker='o', label=lab[0], c='', edgecolor=colors[0])\n",
    "    ax2.scatter(group1.X, group1.Y, s=10, marker='o', label=lab[1], c='', edgecolor=colors[1])\n",
    "    ax2.set(ylabel='Principal Component 2 (%.2f%%)' % (ratio[1] * 100),\n",
    "            xlabel='Principal Component 1 (%.2f%%)' % (ratio[0] * 100))\n",
    "\n",
    "    ax3 = fig.add_subplot(233)\n",
    "    # df, ratio = dimension(fnames, alg='TSNE')\n",
    "    df = pd.read_table('t-SNE_pr.txt')\n",
    "    group0, group1 = df[df.LABEL == 0], df[df.LABEL == 1]\n",
    "    plt.text(0.05, 0.9, chr(ord('E')), fontweight=\"bold\", transform=ax3.transAxes)\n",
    "    ax3.scatter(group0.X, group0.Y, s=1, marker='o', label=lab[0], c='', edgecolor=colors[0])\n",
    "    ax3.scatter(group1.X, group1.Y, s=10, marker='o', label=lab[1], c='', edgecolor=colors[1])\n",
    "    ax3.set(ylabel='Component 2', xlabel='Component 1')\n",
    "\n",
    "    fnames = ['data/ligand_mf_brics_test.txt', 'benchmark/ligand_mix.txt']\n",
    "    lab = ['LIGAND Set', 'Fine-tuned Model']\n",
    "    ax4 = fig.add_subplot(234)\n",
    "    df = logP_mw(fnames)\n",
    "    group0, group1 = df[df.LABEL == 0], df[df.LABEL == 1]\n",
    "    plt.text(0.05, 0.9, chr(ord('B')), fontweight=\"bold\", transform=ax4.transAxes)\n",
    "    ax4.scatter(group0.MWT, group0.LOGP, s=10, marker='o', label=lab[0], c='', edgecolor=colors[2])\n",
    "    ax4.scatter(group1.MWT, group1.LOGP, s=1, marker='o', label=lab[1], c='', edgecolor=colors[3])\n",
    "    ax4.set(ylabel='LogP', xlabel='Molecular Weight', xlim=[0, 1000], ylim=[-5, 10])\n",
    "    handle, label = ax4.get_legend_handles_labels()\n",
    "    labels.extend(handle)\n",
    "    keys.extend(label)\n",
    "\n",
    "    ax5 = fig.add_subplot(235)\n",
    "    df, ratio = dimension(fnames, fp='physchem')\n",
    "    group0, group1 = df[df.LABEL == 0], df[df.LABEL == 1]\n",
    "    plt.text(0.05, 0.9, chr(ord('D')), fontweight=\"bold\", transform=ax5.transAxes)\n",
    "    ax5.scatter(group0.X, group0.Y, s=10, marker='o', label=lab[0], c='', edgecolor=colors[2])\n",
    "    ax5.scatter(group1.X, group1.Y, s=1, marker='o', label=lab[1], c='', edgecolor=colors[3])\n",
    "    ax5.set(ylabel='Principal Component 2 (%.2f%%)' % (ratio[1] * 100),\n",
    "            xlabel='Principal Component 1 (%.2f%%)' % (ratio[0] * 100))\n",
    "\n",
    "    ax6 = fig.add_subplot(236)\n",
    "    # df, ratio = dimension(fnames, alg='TSNE')\n",
    "    df = pd.read_table('t-SNE_ft.txt')\n",
    "    group0, group1 = df[df.LABEL == 0], df[df.LABEL == 1]\n",
    "    plt.text(0.05, 0.9, chr(ord('F')), fontweight=\"bold\", transform=ax6.transAxes)\n",
    "    ax6.scatter(group0.X, group0.Y, s=10, marker='o', label=lab[0], c='', edgecolor=colors[2])\n",
    "    ax6.scatter(group1.X, group1.Y, s=1, marker='o', label=lab[1], c='', edgecolor=colors[3])\n",
    "    ax6.set(ylabel='Component 2', xlabel='Component 1')\n",
    "\n",
    "    fig.legend(labels, keys, loc=\"lower center\", ncol=len(keys), bbox_to_anchor=(0.45, 0.00))\n",
    "    fig.subplots_adjust(wspace=0.5, hspace=0.5)\n",
    "    # plt.tight_layout()\n",
    "    plt.savefig('Figure_4.tif', dpi=600, bbox_inches = \"tight\", pil_kwargs={\"compression\": \"tiff_lzw\"})\n",
    "\n",
    "\n",
    "def figure5():\n",
    "    fig = plt.figure(figsize=(8, 8))\n",
    "    objs = ['QED', 'SA']\n",
    "    ix = 0\n",
    "    keys, labels = [], []\n",
    "    methods = ['ved', 'attn', 'gpt', 'graph']\n",
    "    for i, d in enumerate(['chembl', 'ligand']):\n",
    "        labs = ['ChEMBL Set' if d == 'chembl' else 'LIGAND Set',\n",
    "                  'LSTM-BASE', 'LSTM+ATTN', 'Sequence Transformer', 'Graph Transformer']\n",
    "        dfs = {}\n",
    "        fnames = ['benchmark/%s_set_qed_sa.txt' % d] + ['benchmark/%s_%s_qed_sa.txt' % (d, m) for m in methods]\n",
    "        for j, fname in enumerate(fnames):\n",
    "            dfs[labs[j]] = pd.read_table(fname)\n",
    "        for k, obj in enumerate(objs):\n",
    "            ix += 1\n",
    "            ax = plt.subplot(220 + ix)\n",
    "            plt.text(0.02, 0.9, chr(ord('A') + ix - 1), fontweight=\"bold\", transform=ax.transAxes)\n",
    "            for l, (key, df) in enumerate(dfs.items()):\n",
    "                if obj in ['SA']:\n",
    "                    xx = np.linspace(0, 10, 1000)\n",
    "                else:\n",
    "                    xx = np.linspace(0, 1, 1000)\n",
    "                data = df[obj].values\n",
    "                density = stats.gaussian_kde(data)(xx)\n",
    "                if key in ['ChEMBL Set']:\n",
    "                    color = colors[0]\n",
    "                elif key in ['LIGAND Set']:\n",
    "                    color = colors[1]\n",
    "                else:\n",
    "                    color = colors[l+1]\n",
    "                label = plt.plot(xx, density, c=color)[0]\n",
    "                if (i == 0 and k == 0) or (key in ['LIGAND Set'] and k == 0):\n",
    "                    keys.append(key)\n",
    "                    labels.append(label)\n",
    "            # ax.title(obj + ' Score')\n",
    "    fig.legend(labels, keys, loc=\"upper center\", ncol=3, bbox_to_anchor=(0.45, 0.08))\n",
    "    fig.subplots_adjust(wspace=0.35, hspace=0.35)\n",
    "    fig.savefig('figure_5.tif', dpi=600, bbox_inches=\"tight\", pil_kwargs={\"compression\": \"tiff_lzw\"})\n",
    "\n",
    "\n",
    "def figure6():\n",
    "    fig = plt.figure(figsize=(12, 8))\n",
    "    er = ['0e+00', '1e-01', '2e-01', '3e-01', '4e-01', '5e-01']\n",
    "    ers = {'0e+00': '0.0', '1e-01': '0.1', '2e-01': '0.2', '3e-01': '0.3', '4e-01': '0.4', '5e-01': '0.5'}\n",
    "    # df = dimension(['benchmark/ligand_rl_%s.txt' % e for e in ers], alg='TSNE')\n",
    "    # df.to_csv('t-SNE.txt', index=False, sep='\\t')\n",
    "    df = pd.read_table('t-SNE.txt')\n",
    "    for i, e in enumerate(er):\n",
    "        group0 = df[df.LABEL == 0]\n",
    "        # group0 = group0[group0['QED'] > 0.4]\n",
    "        group1 = df[df.LABEL == i + 1]\n",
    "        ax = fig.add_subplot(231 + i)\n",
    "        plt.text(0.02, 0.9, chr(ord('A') + i), fontweight=\"bold\", transform=ax.transAxes)\n",
    "        ax.scatter(group1.X, group1.Y, s=10, marker='.', label='ε = %s' % ers[e], c='', edgecolor=colors[2])\n",
    "        ax.scatter(group0.X, group0.Y, s=10, marker='o', label='LIGAND set', c='', edgecolor=colors[1])\n",
    "        ax.set(ylabel='Component 2', xlabel='Component 1')\n",
    "        ax.legend(loc='upper right')\n",
    "    plt.savefig('Figure_6.tif', dpi=600, bbox_inches = \"tight\", pil_kwargs={\"compression\": \"tiff_lzw\"})\n",
    "\n",
    "\n",
    "def figure7():\n",
    "    df = pd.read_table('benchmark/ligand_rl_2e-01.txt')\n",
    "    df = df[df.DESIRE == 1]\n",
    "    subs = ['c1cocc1.n1cncnc1.n1c[nH]nc1',\n",
    "            'c1cocc1.O=c1[nH]c(=O)c2nc[nH]c2[nH]1', 'c1cocc1.Nc1ncc2[nH]nnc2n1',\n",
    "            'c1cocc1.n1cncnc1', 'c1cocc1.n1c[nH]nc1','n1cncnc1.n1c[nH]nc1']\n",
    "\n",
    "    subset = {sub: [] for sub in subs}\n",
    "    submol = {sub: Chem.MolFromSmiles(sub) for sub in subs}\n",
    "    for smile in df.Smiles:\n",
    "        if smile != smile: continue\n",
    "        mol = Chem.MolFromSmiles(smile)\n",
    "        for sub in subs:\n",
    "            s = submol[sub]\n",
    "            match = mol.HasSubstructMatch(s)\n",
    "            if match:\n",
    "                subset[sub].append(smile)\n",
    "                break\n",
    "    for i, sub in enumerate(subs[::-1]):\n",
    "        if len(subset[sub]) > 120:\n",
    "            mol = list(subset[sub])[:60]\n",
    "        else:\n",
    "            mol = list(subset[sub])\n",
    "        mols = [Chem.MolFromSmiles(m) for m in mol]\n",
    "        img = Draw.MolsToGridImage(mols, molsPerRow=6, subImgSize=(400, 300))\n",
    "        img.save('figures/figure_6_%d.tif' % i)\n",
    "        print(mol)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    colors = ['#ff7f0e', '#1f77b4', '#d62728', '#2ca02c', '#9467bd', 'cyan']  # orange, blue, green, red, purple\n",
    "    figure3()\n",
    "    figure4()\n",
    "    figure5()\n",
    "    figure6()\n",
    "    figure7()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition, this toolkit also provides some other scripts for definition of special data structures, model architectures and coefficient measurements, etc.\n",
    "\n",
    "#### 1 models/*.py:\n",
    "\n",
    "It contains all of the deep learning models that possibly used in this project, including single/multiple fully-connected regression/classification models, RNN generative model and highway CNN classification model.\n",
    "\n",
    "#### 2 utils/vocab.py:\n",
    "\n",
    "It defines some special data structures, such as vocabulary of SMILES tokens and elements in the graph, molecule dataset, environment and some methods for SMILES and graph checking. The statistical methods that extracting properties from generated molecules.\n",
    "\n",
    "#### 3 utils/metric.py:\n",
    "\n",
    "The statistical methods that extracting properties from generated molecules.\n",
    "\n",
    "#### 4 utils/fingerprints.py:\n",
    "\n",
    "There are a variety of chemical fingerprints calculations, such as ECFP, MACCS etc.\n",
    "\n",
    "#### 5 utils/modifier.py\n",
    "\n",
    "It provides a variety of desirability function to normalize the scoring furntions. For more details, please check GuacaMol benchemark.\n",
    "\n",
    "#### 6 utils/objective.py\n",
    "\n",
    "It provides the construction of different scoring functions, including similary score, chemical properties, QSAR modelling, etc. Moreoever, it can also integrate multiple objective into an environment to calculate reward for the agent.\n",
    "\n",
    "#### 7 utils/nsgaii.py\n",
    "\n",
    "The implementation of non-dominate sorting and crowding distance algorithm (NSGAII). Importantly, we employ PyTorch to accelerate its performance and also modify the calculation of crowding distance with Tanimoto-distance.\n",
    "\n",
    "#### 8 utils/sacorer.py\n",
    "\n",
    "The implementation of SA score to measure the synthezability score of each molecule. More details about SA score can be found \n",
    "\n",
    "https://jcheminf.biomedcentral.com/articles/10.1186/1758-2946-1-8\n",
    "\n",
    "Liu X, IJzerman AP, van Westen GJP. Computational Approaches for De Novo Drug Design: Past, Present, and Future. Methods Mol Biol. 2021;2190:139-65.\n",
    "https://link.springer.com/protocol/10.1007%2F978-1-0716-0826-5_6\n",
    "\n",
    "Liu X, Ye K, van Vlijmen HWT, IJzerman AP, van Westen GJP. DrugEx v3: Scaffold-Constrained Drug Design with Graph Transformer-based Reinforcement Learning. Preprint\n",
    "https://chemrxiv.org/engage/chemrxiv/article-details/61aa8b58bc299c0b30887f80\n",
    "\n",
    "Liu X, Ye K, van Vlijmen HWT, Emmerich MTM, IJzerman AP, van Westen GJP. DrugEx v2: De Novo Design of Drug Molecule by Pareto-based Multi-Objective Reinforcement Learning in Polypharmacology. Journal of cheminformatics 2021:13(1):85.\n",
    "https://doi.org/10.1186/s13321-021-00561-9\n",
    "\n",
    "Liu X, Ye K, van Vlijmen HWT, IJzerman AP, van Westen GJP. An exploration strategy improves the diversity of de novo ligands using deep reinforcement learning: a case for the adenosine A2A receptor. Journal of cheminformatics. 2019;11(1):35.\n",
    "https://jcheminf.biomedcentral.com/articles/10.1186/s13321-019-0355-6\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <ins>Load, view, and preprocess dataset</ins> \n",
    "\n",
    "\n",
    "We will use the [ESOL dataset](http://moleculenet.ai/datasets-1) to train our models. The ESOL dataset contains the solubility of various small organic molecules. We will begin by loading the dataset as a dataframe and then inspecting some basic metadata. We'll also preprocess the dataset and create train/test splits for the Convolutional Neural Network (CNN) and Variational AutoEncoder (VAE) models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in dataset: ['smiles', 'MaxEStateIndex', 'MinEStateIndex', 'MaxAbsEStateIndex', 'MinAbsEStateIndex', 'qed', 'MolWt', 'HeavyAtomMolWt', 'ExactMolWt', 'NumValenceElectrons', 'NumRadicalElectrons', 'MaxPartialCharge', 'MinPartialCharge', 'MaxAbsPartialCharge', 'MinAbsPartialCharge', 'FpDensityMorgan1', 'FpDensityMorgan2', 'FpDensityMorgan3', 'BalabanJ', 'BertzCT', 'Chi0', 'Chi0n', 'Chi0v', 'Chi1', 'Chi1n', 'Chi1v', 'Chi2n', 'Chi2v', 'Chi3n', 'Chi3v', 'Chi4n', 'Chi4v', 'HallKierAlpha', 'Ipc', 'Kappa1', 'Kappa2', 'Kappa3', 'LabuteASA', 'PEOE_VSA1', 'PEOE_VSA10', 'PEOE_VSA11', 'PEOE_VSA12', 'PEOE_VSA13', 'PEOE_VSA14', 'PEOE_VSA2', 'PEOE_VSA3', 'PEOE_VSA4', 'PEOE_VSA5', 'PEOE_VSA6', 'PEOE_VSA7', 'PEOE_VSA8', 'PEOE_VSA9', 'SMR_VSA1', 'SMR_VSA10', 'SMR_VSA2', 'SMR_VSA3', 'SMR_VSA4', 'SMR_VSA5', 'SMR_VSA6', 'SMR_VSA7', 'SMR_VSA8', 'SMR_VSA9', 'SlogP_VSA1', 'SlogP_VSA10', 'SlogP_VSA11', 'SlogP_VSA12', 'SlogP_VSA2', 'SlogP_VSA3', 'SlogP_VSA4', 'SlogP_VSA5', 'SlogP_VSA6', 'SlogP_VSA7', 'SlogP_VSA8', 'SlogP_VSA9', 'TPSA', 'EState_VSA1', 'EState_VSA10', 'EState_VSA11', 'EState_VSA2', 'EState_VSA3', 'EState_VSA4', 'EState_VSA5', 'EState_VSA6', 'EState_VSA7', 'EState_VSA8', 'EState_VSA9', 'VSA_EState1', 'VSA_EState10', 'VSA_EState2', 'VSA_EState3', 'VSA_EState4', 'VSA_EState5', 'VSA_EState6', 'VSA_EState7', 'VSA_EState8', 'VSA_EState9', 'FractionCSP3', 'HeavyAtomCount', 'NHOHCount', 'NOCount', 'NumAliphaticCarbocycles', 'NumAliphaticHeterocycles', 'NumAliphaticRings', 'NumAromaticCarbocycles', 'NumAromaticHeterocycles', 'NumAromaticRings', 'NumHAcceptors', 'NumHDonors', 'NumHeteroatoms', 'NumRotatableBonds', 'NumSaturatedCarbocycles', 'NumSaturatedHeterocycles', 'NumSaturatedRings', 'RingCount', 'MolLogP', 'MolMR', 'fr_Al_COO', 'fr_Al_OH', 'fr_Al_OH_noTert', 'fr_ArN', 'fr_Ar_COO', 'fr_Ar_N', 'fr_Ar_NH', 'fr_Ar_OH', 'fr_COO', 'fr_COO2', 'fr_C_O', 'fr_C_O_noCOO', 'fr_C_S', 'fr_HOCCN', 'fr_Imine', 'fr_NH0', 'fr_NH1', 'fr_NH2', 'fr_N_O', 'fr_Ndealkylation1', 'fr_Ndealkylation2', 'fr_Nhpyrrole', 'fr_SH', 'fr_aldehyde', 'fr_alkyl_carbamate', 'fr_alkyl_halide', 'fr_allylic_oxid', 'fr_amide', 'fr_amidine', 'fr_aniline', 'fr_aryl_methyl', 'fr_azide', 'fr_azo', 'fr_barbitur', 'fr_benzene', 'fr_benzodiazepine', 'fr_bicyclic', 'fr_diazo', 'fr_dihydropyridine', 'fr_epoxide', 'fr_ester', 'fr_ether', 'fr_furan', 'fr_guanido', 'fr_halogen', 'fr_hdrzine', 'fr_hdrzone', 'fr_imidazole', 'fr_imide', 'fr_isocyan', 'fr_isothiocyan', 'fr_ketone', 'fr_ketone_Topliss', 'fr_lactam', 'fr_lactone', 'fr_methoxy', 'fr_morpholine', 'fr_nitrile', 'fr_nitro', 'fr_nitro_arom', 'fr_nitro_arom_nonortho', 'fr_nitroso', 'fr_oxazole', 'fr_oxime', 'fr_para_hydroxylation', 'fr_phenol', 'fr_phenol_noOrthoHbond', 'fr_phos_acid', 'fr_phos_ester', 'fr_piperdine', 'fr_piperzine', 'fr_priamide', 'fr_prisulfonamd', 'fr_pyridine', 'fr_quatN', 'fr_sulfide', 'fr_sulfonamd', 'fr_sulfone', 'fr_term_acetylene', 'fr_tetrazole', 'fr_thiazole', 'fr_thiocyan', 'fr_thiophene', 'fr_unbrch_alkane', 'fr_urea']\n",
      "\n",
      "Length of dataset: 1000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>smiles</th>\n",
       "      <th>MaxEStateIndex</th>\n",
       "      <th>MinEStateIndex</th>\n",
       "      <th>MaxAbsEStateIndex</th>\n",
       "      <th>MinAbsEStateIndex</th>\n",
       "      <th>qed</th>\n",
       "      <th>MolWt</th>\n",
       "      <th>HeavyAtomMolWt</th>\n",
       "      <th>ExactMolWt</th>\n",
       "      <th>NumValenceElectrons</th>\n",
       "      <th>...</th>\n",
       "      <th>fr_sulfide</th>\n",
       "      <th>fr_sulfonamd</th>\n",
       "      <th>fr_sulfone</th>\n",
       "      <th>fr_term_acetylene</th>\n",
       "      <th>fr_tetrazole</th>\n",
       "      <th>fr_thiazole</th>\n",
       "      <th>fr_thiocyan</th>\n",
       "      <th>fr_thiophene</th>\n",
       "      <th>fr_unbrch_alkane</th>\n",
       "      <th>fr_urea</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>993</th>\n",
       "      <td>N#CC(Oc1ccccc1)C(c1ccccc1)c1ccc(-c2ccccc2)cc1</td>\n",
       "      <td>9.937998</td>\n",
       "      <td>-0.636690</td>\n",
       "      <td>9.937998</td>\n",
       "      <td>0.188676</td>\n",
       "      <td>0.387502</td>\n",
       "      <td>375.471</td>\n",
       "      <td>354.303</td>\n",
       "      <td>375.162314</td>\n",
       "      <td>140</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>859</th>\n",
       "      <td>CCc1ccc(C(C)C(C)OCOC)cc1</td>\n",
       "      <td>5.541674</td>\n",
       "      <td>0.173543</td>\n",
       "      <td>5.541674</td>\n",
       "      <td>0.173543</td>\n",
       "      <td>0.687059</td>\n",
       "      <td>222.328</td>\n",
       "      <td>200.152</td>\n",
       "      <td>222.161980</td>\n",
       "      <td>90</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>COCOC(c1ccccc1)C(c1ccccc1)c1ccc([N+](=O)[O-])cc1</td>\n",
       "      <td>11.013109</td>\n",
       "      <td>-0.391463</td>\n",
       "      <td>11.013109</td>\n",
       "      <td>0.069104</td>\n",
       "      <td>0.317703</td>\n",
       "      <td>363.413</td>\n",
       "      <td>342.245</td>\n",
       "      <td>363.147058</td>\n",
       "      <td>138</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>553</th>\n",
       "      <td>COCOC(OC)C(C)(C)c1ccc(C(=O)OC)cc1</td>\n",
       "      <td>11.417669</td>\n",
       "      <td>-0.452383</td>\n",
       "      <td>11.417669</td>\n",
       "      <td>0.154941</td>\n",
       "      <td>0.567332</td>\n",
       "      <td>282.336</td>\n",
       "      <td>260.160</td>\n",
       "      <td>282.146724</td>\n",
       "      <td>112</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>672</th>\n",
       "      <td>COCOCC(C#N)(c1ccccc1)c1ccc(C(=O)OC)cc1</td>\n",
       "      <td>11.596376</td>\n",
       "      <td>-0.991628</td>\n",
       "      <td>11.596376</td>\n",
       "      <td>0.093758</td>\n",
       "      <td>0.444664</td>\n",
       "      <td>325.364</td>\n",
       "      <td>306.212</td>\n",
       "      <td>325.131408</td>\n",
       "      <td>124</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>971</th>\n",
       "      <td>COC(Br)Cc1ccccc1</td>\n",
       "      <td>5.070751</td>\n",
       "      <td>0.129537</td>\n",
       "      <td>5.070751</td>\n",
       "      <td>0.129537</td>\n",
       "      <td>0.703875</td>\n",
       "      <td>215.090</td>\n",
       "      <td>204.002</td>\n",
       "      <td>213.999327</td>\n",
       "      <td>60</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>CCc1ccc(C(C#N)C(C#N)OCOC)cc1</td>\n",
       "      <td>9.184892</td>\n",
       "      <td>-0.817198</td>\n",
       "      <td>9.184892</td>\n",
       "      <td>0.003423</td>\n",
       "      <td>0.720398</td>\n",
       "      <td>244.294</td>\n",
       "      <td>228.166</td>\n",
       "      <td>244.121178</td>\n",
       "      <td>94</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231</th>\n",
       "      <td>COCC(C)(C)c1ccc(C#N)cc1</td>\n",
       "      <td>8.659967</td>\n",
       "      <td>0.002714</td>\n",
       "      <td>8.659967</td>\n",
       "      <td>0.002714</td>\n",
       "      <td>0.731433</td>\n",
       "      <td>189.258</td>\n",
       "      <td>174.138</td>\n",
       "      <td>189.115364</td>\n",
       "      <td>74</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>306</th>\n",
       "      <td>c1ccc(OC(c2ccccc2)C(c2ccccc2)c2ccccc2)cc1</td>\n",
       "      <td>6.571502</td>\n",
       "      <td>-0.126481</td>\n",
       "      <td>6.571502</td>\n",
       "      <td>0.092598</td>\n",
       "      <td>0.378653</td>\n",
       "      <td>350.461</td>\n",
       "      <td>328.285</td>\n",
       "      <td>350.167065</td>\n",
       "      <td>132</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>706</th>\n",
       "      <td>CCc1ccc(C(C)(C)C(OC)c2ccccc2)cc1</td>\n",
       "      <td>5.812137</td>\n",
       "      <td>-0.067089</td>\n",
       "      <td>5.812137</td>\n",
       "      <td>0.052015</td>\n",
       "      <td>0.748005</td>\n",
       "      <td>268.400</td>\n",
       "      <td>244.208</td>\n",
       "      <td>268.182715</td>\n",
       "      <td>106</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>COCOC(C)C(C)(C#N)c1ccc(C#N)cc1</td>\n",
       "      <td>9.399143</td>\n",
       "      <td>-0.771314</td>\n",
       "      <td>9.399143</td>\n",
       "      <td>0.147536</td>\n",
       "      <td>0.745551</td>\n",
       "      <td>244.294</td>\n",
       "      <td>228.166</td>\n",
       "      <td>244.121178</td>\n",
       "      <td>94</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>558</th>\n",
       "      <td>COC(Oc1ccccc1)C(c1ccccc1)c1ccc([N+](=O)[O-])cc1</td>\n",
       "      <td>10.939469</td>\n",
       "      <td>-0.582170</td>\n",
       "      <td>10.939469</td>\n",
       "      <td>0.054172</td>\n",
       "      <td>0.349835</td>\n",
       "      <td>349.386</td>\n",
       "      <td>330.234</td>\n",
       "      <td>349.131408</td>\n",
       "      <td>132</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>784</th>\n",
       "      <td>CCc1ccc(C(C)(C#N)C(Br)OC)cc1</td>\n",
       "      <td>9.306742</td>\n",
       "      <td>-0.665653</td>\n",
       "      <td>9.306742</td>\n",
       "      <td>0.307892</td>\n",
       "      <td>0.792873</td>\n",
       "      <td>282.181</td>\n",
       "      <td>266.053</td>\n",
       "      <td>281.041526</td>\n",
       "      <td>86</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <td>COCOC(Br)C(C)(C)c1ccc(C(=O)OC)cc1</td>\n",
       "      <td>11.376928</td>\n",
       "      <td>-0.337610</td>\n",
       "      <td>11.376928</td>\n",
       "      <td>0.190809</td>\n",
       "      <td>0.456476</td>\n",
       "      <td>331.206</td>\n",
       "      <td>312.054</td>\n",
       "      <td>330.046671</td>\n",
       "      <td>106</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>578</th>\n",
       "      <td>COC(=O)c1ccc(C(C#N)(C#N)C(C)OC)cc1</td>\n",
       "      <td>11.326606</td>\n",
       "      <td>-1.399405</td>\n",
       "      <td>11.326606</td>\n",
       "      <td>0.362129</td>\n",
       "      <td>0.768689</td>\n",
       "      <td>258.277</td>\n",
       "      <td>244.165</td>\n",
       "      <td>258.100442</td>\n",
       "      <td>98</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>COCOCC(C#N)(c1ccccc1)c1ccc(C#N)cc1</td>\n",
       "      <td>9.873415</td>\n",
       "      <td>-0.936730</td>\n",
       "      <td>9.873415</td>\n",
       "      <td>0.117602</td>\n",
       "      <td>0.606136</td>\n",
       "      <td>292.338</td>\n",
       "      <td>276.210</td>\n",
       "      <td>292.121178</td>\n",
       "      <td>110</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>906</th>\n",
       "      <td>CC(C)(c1ccccc1)C(Br)Oc1ccccc1</td>\n",
       "      <td>5.950196</td>\n",
       "      <td>-0.100098</td>\n",
       "      <td>5.950196</td>\n",
       "      <td>0.074259</td>\n",
       "      <td>0.739738</td>\n",
       "      <td>305.215</td>\n",
       "      <td>288.079</td>\n",
       "      <td>304.046277</td>\n",
       "      <td>94</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>COCC(C#N)(c1ccccc1)c1ccc(C(=O)OC)cc1</td>\n",
       "      <td>11.527728</td>\n",
       "      <td>-0.912044</td>\n",
       "      <td>11.527728</td>\n",
       "      <td>0.227266</td>\n",
       "      <td>0.795745</td>\n",
       "      <td>295.338</td>\n",
       "      <td>278.202</td>\n",
       "      <td>295.120843</td>\n",
       "      <td>112</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>COCOC(C#N)C(C)(C#N)c1ccc(C#N)cc1</td>\n",
       "      <td>9.379143</td>\n",
       "      <td>-1.123744</td>\n",
       "      <td>9.379143</td>\n",
       "      <td>0.063714</td>\n",
       "      <td>0.748764</td>\n",
       "      <td>255.277</td>\n",
       "      <td>242.173</td>\n",
       "      <td>255.100777</td>\n",
       "      <td>96</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>CC(c1ccc([N+](=O)[O-])cc1)C(C#N)Oc1ccccc1</td>\n",
       "      <td>10.641827</td>\n",
       "      <td>-0.658467</td>\n",
       "      <td>10.641827</td>\n",
       "      <td>0.030959</td>\n",
       "      <td>0.618953</td>\n",
       "      <td>282.299</td>\n",
       "      <td>268.187</td>\n",
       "      <td>282.100442</td>\n",
       "      <td>106</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20 rows × 201 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               smiles  MaxEStateIndex  MinEStateIndex  MaxAbsEStateIndex  MinAbsEStateIndex       qed    MolWt  HeavyAtomMolWt  ExactMolWt  NumValenceElectrons  ...  fr_sulfide  fr_sulfonamd  fr_sulfone  fr_term_acetylene  fr_tetrazole  fr_thiazole  fr_thiocyan  fr_thiophene  fr_unbrch_alkane  fr_urea\n",
       "993     N#CC(Oc1ccccc1)C(c1ccccc1)c1ccc(-c2ccccc2)cc1        9.937998       -0.636690           9.937998           0.188676  0.387502  375.471         354.303  375.162314                  140  ...           0             0           0                  0             0            0            0             0                 0        0\n",
       "859                          CCc1ccc(C(C)C(C)OCOC)cc1        5.541674        0.173543           5.541674           0.173543  0.687059  222.328         200.152  222.161980                   90  ...           0             0           0                  0             0            0            0             0                 0        0\n",
       "298  COCOC(c1ccccc1)C(c1ccccc1)c1ccc([N+](=O)[O-])cc1       11.013109       -0.391463          11.013109           0.069104  0.317703  363.413         342.245  363.147058                  138  ...           0             0           0                  0             0            0            0             0                 0        0\n",
       "553                 COCOC(OC)C(C)(C)c1ccc(C(=O)OC)cc1       11.417669       -0.452383          11.417669           0.154941  0.567332  282.336         260.160  282.146724                  112  ...           0             0           0                  0             0            0            0             0                 0        0\n",
       "672            COCOCC(C#N)(c1ccccc1)c1ccc(C(=O)OC)cc1       11.596376       -0.991628          11.596376           0.093758  0.444664  325.364         306.212  325.131408                  124  ...           0             0           0                  0             0            0            0             0                 1        0\n",
       "971                                  COC(Br)Cc1ccccc1        5.070751        0.129537           5.070751           0.129537  0.703875  215.090         204.002  213.999327                   60  ...           0             0           0                  0             0            0            0             0                 0        0\n",
       "27                       CCc1ccc(C(C#N)C(C#N)OCOC)cc1        9.184892       -0.817198           9.184892           0.003423  0.720398  244.294         228.166  244.121178                   94  ...           0             0           0                  0             0            0            0             0                 0        0\n",
       "231                           COCC(C)(C)c1ccc(C#N)cc1        8.659967        0.002714           8.659967           0.002714  0.731433  189.258         174.138  189.115364                   74  ...           0             0           0                  0             0            0            0             0                 0        0\n",
       "306         c1ccc(OC(c2ccccc2)C(c2ccccc2)c2ccccc2)cc1        6.571502       -0.126481           6.571502           0.092598  0.378653  350.461         328.285  350.167065                  132  ...           0             0           0                  0             0            0            0             0                 0        0\n",
       "706                  CCc1ccc(C(C)(C)C(OC)c2ccccc2)cc1        5.812137       -0.067089           5.812137           0.052015  0.748005  268.400         244.208  268.182715                  106  ...           0             0           0                  0             0            0            0             0                 0        0\n",
       "496                    COCOC(C)C(C)(C#N)c1ccc(C#N)cc1        9.399143       -0.771314           9.399143           0.147536  0.745551  244.294         228.166  244.121178                   94  ...           0             0           0                  0             0            0            0             0                 0        0\n",
       "558   COC(Oc1ccccc1)C(c1ccccc1)c1ccc([N+](=O)[O-])cc1       10.939469       -0.582170          10.939469           0.054172  0.349835  349.386         330.234  349.131408                  132  ...           0             0           0                  0             0            0            0             0                 0        0\n",
       "784                      CCc1ccc(C(C)(C#N)C(Br)OC)cc1        9.306742       -0.665653           9.306742           0.307892  0.792873  282.181         266.053  281.041526                   86  ...           0             0           0                  0             0            0            0             0                 0        0\n",
       "239                 COCOC(Br)C(C)(C)c1ccc(C(=O)OC)cc1       11.376928       -0.337610          11.376928           0.190809  0.456476  331.206         312.054  330.046671                  106  ...           0             0           0                  0             0            0            0             0                 0        0\n",
       "578                COC(=O)c1ccc(C(C#N)(C#N)C(C)OC)cc1       11.326606       -1.399405          11.326606           0.362129  0.768689  258.277         244.165  258.100442                   98  ...           0             0           0                  0             0            0            0             0                 0        0\n",
       "55                 COCOCC(C#N)(c1ccccc1)c1ccc(C#N)cc1        9.873415       -0.936730           9.873415           0.117602  0.606136  292.338         276.210  292.121178                  110  ...           0             0           0                  0             0            0            0             0                 1        0\n",
       "906                     CC(C)(c1ccccc1)C(Br)Oc1ccccc1        5.950196       -0.100098           5.950196           0.074259  0.739738  305.215         288.079  304.046277                   94  ...           0             0           0                  0             0            0            0             0                 0        0\n",
       "175              COCC(C#N)(c1ccccc1)c1ccc(C(=O)OC)cc1       11.527728       -0.912044          11.527728           0.227266  0.795745  295.338         278.202  295.120843                  112  ...           0             0           0                  0             0            0            0             0                 0        0\n",
       "14                   COCOC(C#N)C(C)(C#N)c1ccc(C#N)cc1        9.379143       -1.123744           9.379143           0.063714  0.748764  255.277         242.173  255.100777                   96  ...           0             0           0                  0             0            0            0             0                 0        0\n",
       "77          CC(c1ccc([N+](=O)[O-])cc1)C(C#N)Oc1ccccc1       10.641827       -0.658467          10.641827           0.030959  0.618953  282.299         268.187  282.100442                  106  ...           0             0           0                  0             0            0            0             0                 0        0\n",
       "\n",
       "[20 rows x 201 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!ls /home/nanohub/bbishnoi/data/results/vae/qm9.csv\n",
    "#dataset = pd.read_csv(\"/home/nanohub/bbishnoi/data/results/vae/qm9.csv\")\n",
    "# read dataset as a dataframe\n",
    "#dataset = pd.read_csv(\"../data/ESOL_delaney-processed.csv\")\n",
    "\n",
    "from random import shuffle\n",
    "dataset = pd.read_csv(\"./SMILES_RDKit_2D.csv\")\n",
    "#dataset = pd.read_csv(\"./SMILES_feature.csv\")\n",
    "#dataset = pd.read_csv(\"gdrive/MyDrive/Colab Notebooks/data/qm9.csv\")\n",
    "\n",
    "# This function randomly arranges the elements so we can have representation for all groups both in the training and testing set\n",
    "#shuffle(dataset) \n",
    "\n",
    "# print column names in dataset\n",
    "print(f\"Columns in dataset: {list(dataset.columns)}\")\n",
    "\n",
    "# print number of rows in dataset\n",
    "print(f\"\\nLength of dataset: {len(dataset)}\")\n",
    "\n",
    "# shuffle rows of the dataset (we could do this later as well when doing train/test splits)\n",
    "dataset = dataset.sample(frac=1, random_state=0)\n",
    "\n",
    "# show first 5 rows of dataframe\n",
    "dataset.head(20)\n",
    "#dataset.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To calculate all the rdkit descriptors, you can use the following code:\n",
    "descriptor_names = list(rdMolDescriptors.Properties.GetAvailableProperties())\n",
    "get_descriptors = rdMolDescriptors.Properties(descriptor_names)\n",
    "\n",
    "print(descriptor_names)\n",
    "# print(DescriptorSummaries())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate descriptors using smile strings\n",
    "def smi_to_descriptors(smile):\n",
    "    mol = Chem.MolFromSmiles(smile)\n",
    "    descriptors = []\n",
    "    if mol:\n",
    "        descriptors = np.array(get_descriptors.ComputeProperties(mol))\n",
    "    return descriptors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if the the smiles are in pandas dataframe\n",
    "dataset['descriptors'] = dataset.SMILES.apply(smi_to_descriptors)\n",
    "#dataset= dataset.SMILES.apply(smi_to_descriptors)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dataset = dataset\n",
    "full_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit import Chem    # make sure to import it if you haven't done so\n",
    "from rdkit.Chem import Descriptors    # make sure to import it if you haven't done so\n",
    "descriptors_list = [x[0] for x in Descriptors._descList]\n",
    "print(descriptors_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calc = MoleculeDescriptors.MolecularDescriptorCalculator([x[0] for x in Descriptors._descList])\n",
    "type(calc)\n",
    "mol = Chem.MolFromSmiles('CC1=CC(=C(C=C1NC(=O)C2=C(C(=CC(=C2)I)I)O)Cl)C(C#N)C3=CC=C(C=C3)Cl')\n",
    "ds = calc.CalcDescriptors(mol)\n",
    "print(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pandas==0.21\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np;\n",
    "import seaborn as sns; \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "#qm9 = pd.read_csv(\"./SMILES_RDKit_2D.csv\")\n",
    "qm9 = pd.read_csv(\"./x_df_SMILES_RDKit_2D.csv\")\n",
    "\n",
    "#couple_columns = qm9[['gap','zpve', 'mu']].head(10)\n",
    "#print(couple_columns.shape)\n",
    "plt.figure(figsize=(30,30))\n",
    "\n",
    "# calculate the correlation matrix\n",
    "corr = qm9.corr()\n",
    "# plot the heatmap\n",
    "sns.heatmap(corr, \n",
    "        xticklabels=corr.columns,\n",
    "        yticklabels=corr.columns, cmap=\"YlGnBu\")#YlGnBu viridis_r Spectral_r\n",
    "ax=plt.savefig('./corr_x_df_SMILES_RDKit_2D_YlGnBu.png', dpi=900, facecolor='w', edgecolor='w', format=None, transparent=False, bbox_inches=None, pad_inches=None, metadata=None)\n",
    "\n",
    "#sns.heatmap(corr, cmap=\"Blues\", annot=True)\n",
    "\n",
    "#Heat Map using Seaborn\n",
    "#import numpy as np;\n",
    "#import seaborn as sns; \n",
    "\n",
    "# To translate into Excel Terms for those familiar with Excel\n",
    "# string 1 is row labels 'helix1 phase'\n",
    "# string 2 is column labels 'helix 2 phase'\n",
    "# string 3 is values 'Energy'\n",
    "# Official pivot documentation\n",
    "# http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.pivot.html\n",
    "\n",
    "#homo_lumo_mix.pivot('zpve', 'mu','gap').head()\n",
    "#homo_lumo_mix.pivot('zpve', 'mu')['gap'].head()\n",
    "\n",
    "#!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from https://proxy.nanohub.org/weber/2004336/GBdSjVSdDDS3NYpl/4/notebooks/LLZO_MachineLearning.ipynb\n",
    "# This code is to drop columns with std = 0. \n",
    "#x_df = pd.DataFrame(X)\n",
    "#All columns that have a standard deviation of zero are dropped, as they don't contribute new information to the models.\n",
    "x_df = pd.read_csv(\"./features.csv\")\n",
    "x_df = x_df.loc[:, x_df.std() != 0]\n",
    "print(x_df.shape) # This shape is (#Entries, #Descriptors per entry)\n",
    "x_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_df.to_csv('./x_df_features.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#!pip install pandas==0.21\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np;\n",
    "import seaborn as sns; \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "#qm9 = pd.read_csv(\"./SMILES_RDKit_2D.csv\")\n",
    "df1 = pd.read_csv(\"./x_df_features.csv\")\n",
    "df2 = pd.read_csv(\"./x_df_SMILES_RDKit_2D.csv\")\n",
    "\n",
    "qm9= pd.concat([df1, df2], axis=1, keys=['df1', 'df2']).corr().loc['df2', 'df1']\n",
    "\n",
    "#couple_columns = qm9[['gap','zpve', 'mu']].head(10)\n",
    "#print(couple_columns.shape)\n",
    "plt.figure(figsize=(30,30))\n",
    "\n",
    "# calculate the correlation matrix\n",
    "corr = qm9.corr()\n",
    "# plot the heatmap\n",
    "sns.heatmap(corr, \n",
    "        xticklabels=corr.columns,\n",
    "        yticklabels=corr.columns, cmap=\"YlGnBu\")#YlGnBu viridis_r Spectral_r\n",
    "ax=plt.savefig('./corr_x_df_SMILES_RDKit_2D_YlGnBu.png', dpi=900, facecolor='w', edgecolor='w', format=None, transparent=False, bbox_inches=None, pad_inches=None, metadata=None)\n",
    "\n",
    "#sns.heatmap(corr, cmap=\"Blues\", annot=True)\n",
    "\n",
    "#Heat Map using Seaborn\n",
    "#import numpy as np;\n",
    "#import seaborn as sns; \n",
    "\n",
    "# To translate into Excel Terms for those familiar with Excel\n",
    "# string 1 is row labels 'helix1 phase'\n",
    "# string 2 is column labels 'helix 2 phase'\n",
    "# string 3 is values 'Energy'\n",
    "# Official pivot documentation\n",
    "# http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.pivot.html\n",
    "\n",
    "#homo_lumo_mix.pivot('zpve', 'mu','gap').head()\n",
    "#homo_lumo_mix.pivot('zpve', 'mu')['gap'].head()\n",
    "\n",
    "#!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "##  Read Data  ##\n",
    "\n",
    "#!wget https://raw.githubusercontent.com/AIScienceTutorial/Material_Science/master/Formation_Energies/Data.csv\n",
    "#!wget https://raw.githubusercontent.com/AIScienceTutorial/Material_Science/master/Formation_Energies/Data_norm.csv\n",
    "\n",
    "#ifile  = open('Data.csv', \"rt\")\n",
    "ifile  = open('x_df_features.csv', \"rt\")\n",
    "# df1 = pd.read_csv(\"./x_df_features.csv\")\n",
    "# df2 = pd.read_csv(\"./x_df_SMILES_RDKit_2D.csv\")\n",
    "reader = csv.reader(ifile)\n",
    "csvdata=[]\n",
    "for row in reader:\n",
    "        csvdata.append(row)   \n",
    "ifile.close()\n",
    "numrow=len(csvdata)\n",
    "numcol=len(csvdata[0]) \n",
    "csvdata = np.array(csvdata).reshape(numrow,numcol)\n",
    "dopant = csvdata[:,0]\n",
    "CdX = csvdata[:,1]\n",
    "doping_site = csvdata[:,2]\n",
    "\n",
    "prop  = csvdata[:,5]  ## Cd-rich Delta_H\n",
    "#prop  = csvdata[:,4]  ## Mod. Delta_H\n",
    "#prop  = csvdata[:,5]  ## X-rich Delta_H\n",
    "\n",
    "#X = csvdata[:,6:20]       ##  Elemental Properties\n",
    "#X = csvdata[:,20:25]       ##  Unit Cell Defect Properties\n",
    "X = csvdata[:,6:]       ##  Elemental + Unit Cell Defect Properties\n",
    "\n",
    "n = prop.size\n",
    "\n",
    "\n",
    "\n",
    "    # Read CdX alloy data: CdTe_0.5Se_0.5 and CdSe_0.5S_0.5\n",
    "\n",
    "#!wget https://raw.githubusercontent.com/AIScienceTutorial/Material_Science/master/Formation_Energies/Outside.csv\n",
    "#!wget https://raw.githubusercontent.com/AIScienceTutorial/Material_Science/master/Formation_Energies/Outside_norm.csv\n",
    "\n",
    "#ifile2  = open('Outside.csv', \"rt\")\n",
    "ifile2  = open('x_df_SMILES_RDKit_2D.csv', \"rt\")\n",
    "reader2 = csv.reader(ifile2)\n",
    "csvdata2=[]\n",
    "for row2 in reader2:\n",
    "        csvdata2.append(row2)\n",
    "ifile2.close()\n",
    "numrow2=len(csvdata2)\n",
    "numcol2=len(csvdata2[0])\n",
    "csvdata2 = np.array(csvdata2).reshape(numrow2,numcol2)\n",
    "dopant_out = csvdata2[:,0]\n",
    "CdX_out = csvdata2[:,1]\n",
    "doping_site_out = csvdata2[:,2]\n",
    "prop_out  = csvdata2[:,3]\n",
    "#prop_out  = csvdata2[:,4]\n",
    "#prop_out  = csvdata2[:,5]\n",
    "#X_out = csvdata2[:,6:20]\n",
    "#X_out = csvdata2[:,20:25]\n",
    "X_out = csvdata2[:,6:]\n",
    "\n",
    "n_out = prop_out.size\n",
    "\n",
    "\n",
    "    # Read Entire Dataset\n",
    "\n",
    "#!wget https://raw.githubusercontent.com/AIScienceTutorial/Material_Science/master/Formation_Energies/X.csv\n",
    "#!wget https://raw.githubusercontent.com/AIScienceTutorial/Material_Science/master/Formation_Energies/X_norm.csv\n",
    "\n",
    "#ifile3  = open('X.csv', \"rt\")\n",
    "ifile3  = open('x_df_SMILES_RDKit_2D.csv', \"rt\")\n",
    "reader3 = csv.reader(ifile3)\n",
    "csvdata3=[]\n",
    "for row3 in reader3:\n",
    "        csvdata3.append(row3)\n",
    "ifile3.close()\n",
    "numrow3=len(csvdata3)\n",
    "numcol3=len(csvdata3[0])\n",
    "csvdata3 = np.array(csvdata3).reshape(numrow3,numcol3)\n",
    "dopant_all = csvdata3[:,0]\n",
    "CdX_all = csvdata3[:,1]\n",
    "doping_site_all = csvdata3[:,2]\n",
    "X_all = csvdata3[:,3:17]\n",
    "#X_all = csvdata3[:,17:22]\n",
    "#X_all = csvdata3[:,3:]\n",
    "\n",
    "n_all = dopant_all.size\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##   Visualize Data   ##\n",
    "##   Visualize data: plot desired descriptor dimension vs property.\n",
    "\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.subplots_adjust(left=0.16, bottom=0.16, right=0.95, top=0.90)\n",
    "plt.rc('font', family='Arial narrow')\n",
    "\n",
    "plt.ylabel('Property', fontname='Arial Narrow', size=32)\n",
    "plt.xlabel('Descriptor', fontname='Arial Narrow', size=32)\n",
    "plt.rc('xtick', labelsize=32)\n",
    "plt.rc('ytick', labelsize=32)\n",
    "\n",
    "yy = [0.0]*n\n",
    "xx = [0.0]*n\n",
    "\n",
    "for i in range(0,n):\n",
    "    yy[i] = np.float(prop[i])\n",
    "    xx[i] = np.float(X[i,12])\n",
    "\n",
    "plt.scatter(xx[:], yy[:], c='k', marker='*', s=200, edgecolors='dimgrey', alpha=1.0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/zinph/Cheminformatics/blob/master/compute_descriptors/RDKit_2D.py\n",
    "# RDKit 2D Fingerprint\n",
    "import pandas as pd\n",
    "from molvs import standardize_smiles\n",
    "#from RDKit_2D import *\n",
    "\n",
    "class RDKit_2D:\n",
    "    def __init__(self, smiles):\n",
    "        self.mols = [Chem.MolFromSmiles(i) for i in smiles]\n",
    "        self.smiles = smiles\n",
    "        \n",
    "    def compute_2Drdkit(self, name):\n",
    "        rdkit_2d_desc = []\n",
    "        calc = MoleculeDescriptors.MolecularDescriptorCalculator([x[0] for x in Descriptors._descList])\n",
    "        header = calc.GetDescriptorNames()\n",
    "        for i in range(len(self.mols)):\n",
    "            ds = calc.CalcDescriptors(self.mols[i])\n",
    "            rdkit_2d_desc.append(ds)\n",
    "        df = pd.DataFrame(rdkit_2d_desc,columns=header)\n",
    "        df.insert(loc=0, column='smiles', value=self.smiles)\n",
    "        df.to_csv(name[:-4]+'_RDKit_2D.csv', index=False)\n",
    "\n",
    "def main():\n",
    "    filename = './qm9.csv'  # path to your csv file\n",
    "    #filename = './SMILES.csv'\n",
    "    df = pd.read_csv(filename)               # read the csv file as pandas data frame\n",
    "    smiles = [standardize_smiles(i) for i in df['SMILES'].values]  \n",
    "\n",
    "    ## Compute RDKit_2D Fingerprints and export a csv file.\n",
    "    RDKit_descriptor = RDKit_2D(smiles)        # create your RDKit_2D object and provide smiles\n",
    "    RDKit_descriptor.compute_2Drdkit(filename) # compute RDKit_2D and provide the name of your desired output file. you can use the same name as the input file because the RDKit_2D class will ensure to add \"_RDKit_2D.csv\" as part of the output file.\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/zinph/Cheminformatics/blob/master/compute_descriptors/ECFP6.py\n",
    "# ECFP6 Fingerprint\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from rdkit.Chem import AllChem\n",
    "from rdkit import Chem, DataStructs\n",
    "\n",
    "class ECFP6:\n",
    "    def __init__(self, smiles):\n",
    "        self.mols = [Chem.MolFromSmiles(i) for i in smiles]\n",
    "        self.smiles = smiles\n",
    "\n",
    "    def mol2fp(self, mol, radius = 3):\n",
    "        fp = AllChem.GetMorganFingerprintAsBitVect(mol, radius = radius)\n",
    "        array = np.zeros((1,))\n",
    "        DataStructs.ConvertToNumpyArray(fp, array)\n",
    "        return array\n",
    "\n",
    "    def compute_ECFP6(self, name):\n",
    "        bit_headers = ['bit' + str(i) for i in range(2048)]\n",
    "        arr = np.empty((0,2048), int).astype(int)\n",
    "        for i in self.mols:\n",
    "            fp = self.mol2fp(i)\n",
    "            arr = np.vstack((arr, fp))\n",
    "        df_ecfp6 = pd.DataFrame(np.asarray(arr).astype(int),columns=bit_headers)\n",
    "        df_ecfp6.insert(loc=0, column='smiles', value=self.smiles)\n",
    "        df_ecfp6.to_csv(name[:-4]+'_ECFP6.csv', index=False)\n",
    "\n",
    "def main():\n",
    "    #filename = './qm9.csv'  # path to your csv file\n",
    "    filename = './SMILES.csv'\n",
    "    df = pd.read_csv(filename)               # read the csv file as pandas data frame\n",
    "    smiles = [standardize_smiles(i) for i in df['SMILES'].values]  \n",
    "\n",
    "    ## Compute RDKit_2D Fingerprints and export a csv file.\n",
    "    ECFP6_descriptor = ECFP6(smiles)        # create your RDKit_2D object and provide smiles\n",
    "    ECFP6_descriptor.compute_ECFP6(filename) # compute RDKit_2D and provide the name of your desired output file. you can use the same name as the input file because the RDKit_2D class will ensure to add \"_RDKit_2D.csv\" as part of the output file.\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/zinph/Cheminformatics/blob/master/compute_descriptors/MACCS.py\n",
    "# MACCS Fingerprint\n",
    "\n",
    "import pandas as pd\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import MACCSkeys\n",
    "\n",
    "class MACCS:\n",
    "    def __init__(self, smiles):\n",
    "        self.mols = [Chem.MolFromSmiles(i) for i in smiles]\n",
    "        self.smiles = smiles\n",
    "\n",
    "    def compute_MACCS(self, name):\n",
    "        MACCS_list = []\n",
    "        header = ['bit' + str(i) for i in range(167)]\n",
    "        for i in range(len(self.mols)):\n",
    "            ds = list(MACCSkeys.GenMACCSKeys(self.mols[i]).ToBitString())\n",
    "            MACCS_list.append(ds)\n",
    "        df = pd.DataFrame(MACCS_list,columns=header)\n",
    "        df.insert(loc=0, column='smiles', value=self.smiles)\n",
    "        df.to_csv(name[:-4]+'_MACCS.csv', index=False)\n",
    "\n",
    "def main():\n",
    "    #filename = './qm9.csv'  # path to your csv file\n",
    "    filename = './SMILES.csv'\n",
    "    df = pd.read_csv(filename)               # read the csv file as pandas data frame\n",
    "    smiles = [standardize_smiles(i) for i in df['SMILES'].values]  \n",
    "\n",
    "    ## Compute RDKit_2D Fingerprints and export a csv file.\n",
    "    MACCS_descriptor = MACCS(smiles)        # create your RDKit_2D object and provide smiles\n",
    "    MACCS_descriptor.compute_MACCS(filename) # compute RDKit_2D and provide the name of your desired output file. you can use the same name as the input file because the RDKit_2D class will ensure to add \"_RDKit_2D.csv\" as part of the output file.\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://drzinph.com/mordred_mrc_descriptors-in-python-part-5/\n",
    "#https://github.com/zinph/Cheminformatics/blob/master/compute_descriptors/Macrocycle_Descriptors.py\n",
    "# Macrocycle_Descriptors\n",
    "\n",
    "import itertools\n",
    "import pandas as pd\n",
    "from rdkit import Chem\n",
    "from mordred import Calculator, descriptors\n",
    "from mordred.RingCount import RingCount\n",
    "\n",
    "class Macrocycle_Descriptors:\n",
    "\n",
    "    def __init__(self, smiles):\n",
    "        self.mols = [Chem.MolFromSmiles(i) for i in smiles]\n",
    "        self.smiles = smiles\n",
    "        self.mordred = None\n",
    "\n",
    "\n",
    "    def compute_ringsize(self, mol):\n",
    "        '''\n",
    "        check for macrolides of RS 3 to 99, return a  list of ring counts.\n",
    "        [RS3,RS4,.....,RS99]\n",
    "        [0,0,0,...,1,...,0]\n",
    "        '''\n",
    "        RS_3_99 = [i+3 for i in range(97)]\n",
    "        RS_count = []\n",
    "        for j in RS_3_99:\n",
    "            RS = RingCount(order=j)(mol)\n",
    "            RS_count.append(RS)\n",
    "        return RS_count\n",
    "\n",
    "    def macrolide_ring_info(self):\n",
    "        headers = ['n'+str(i+13)+'Ring' for i in range(87)]+['SmallestRS','LargestRS']\n",
    "        # up to nR12 is already with mordred, start with nR13 to nR99\n",
    "        ring_sizes = []\n",
    "        for i in range(len(self.mols)):\n",
    "            RS = self.compute_ringsize(self.mols[i])  # nR3 to nR99\n",
    "            RS_12_99 = RS[9:]    # start with nR12 up to nR99\n",
    "            ring_indices = [i for i,x in enumerate(RS_12_99) if x!=0]  # get index if item isn't equal to 0\n",
    "            # if there is a particular ring present, the frequency won't be zero. Find those indexes. \n",
    "\t\t\tif ring_indices:\n",
    "                # Add 12 (starting ring count) to get up to the actual ring size\n",
    "                smallest_RS = ring_indices[0]+12     # Retrieve the first index (for the smallest core RS - note the list is in ascending order)\n",
    "                largest_RS = ring_indices[-1]+12\t # Retrieve the last index (for the largest core RS)\n",
    "                RS_12_99.append(smallest_RS)  # Smallest RS\n",
    "                RS_12_99.append(largest_RS)  # Largest RS\n",
    "            else:\n",
    "                RS_12_99.extend(['',''])\n",
    "            ring_sizes.append(RS_12_99[1:]) # up to nR12 is already with mordred, start with nR13 to nR99\n",
    "        df = pd.DataFrame(ring_sizes, columns=headers)\n",
    "        return df\n",
    "\n",
    "    def sugar_count(self):\n",
    "        sugar_patterns = [\n",
    "        '[OX2;$([r5]1@C@C@C(O)@C1),$([r6]1@C@C@C(O)@C(O)@C1)]',\n",
    "        '[OX2;$([r5]1@C(!@[OX2,NX3,SX2,FX1,ClX1,BrX1,IX1])@C@C@C1),$([r6]1@C(!@[OX2,NX3,SX2,FX1,ClX1,BrX1,IX1])@C@C@C@C1)]',\n",
    "        '[OX2;$([r5]1@C(!@[OX2,NX3,SX2,FX1,ClX1,BrX1,IX1])@C@C(O)@C1),$([r6]1@C(!@[OX2,NX3,SX2,FX1,ClX1,BrX1,IX1])@C@C(O)@C(O)@C1)]',\n",
    "        '[OX2;$([r5]1@C(!@[OX2H1])@C@C@C1),$([r6]1@C(!@[OX2H1])@C@C@C@C1)]',\n",
    "        '[OX2;$([r5]1@[C@@](!@[OX2,NX3,SX2,FX1,ClX1,BrX1,IX1])@C@C@C1),$([r6]1@[C@@](!@[OX2,NX3,SX2,FX1,ClX1,BrX1,IX1])@C@C@C@C1)]',\n",
    "        '[OX2;$([r5]1@[C@](!@[OX2,NX3,SX2,FX1,ClX1,BrX1,IX1])@C@C@C1),$([r6]1@[C@](!@[OX2,NX3,SX2,FX1,ClX1,BrX1,IX1])@C@C@C@C1)]',\n",
    "        ]\n",
    "        sugar_mols = [Chem.MolFromSmarts(i) for i in sugar_patterns]\n",
    "        sugar_counts = []\n",
    "        for i in self.mols:\n",
    "            matches_total = []\n",
    "            for s_mol in sugar_mols:\n",
    "                raw_matches = i.GetSubstructMatches(s_mol)\n",
    "                matches = list(sum(raw_matches, ()))\n",
    "                if matches not in matches_total and len(matches) !=0:\n",
    "                    matches_total.append(matches)\n",
    "            sugar_indices = set((list(itertools.chain(*matches_total))))\n",
    "            count = len(sugar_indices)\n",
    "            sugar_counts.append(count)\n",
    "        df = pd.DataFrame(sugar_counts, columns=['nSugars'])\n",
    "        return df\n",
    "\n",
    "    def core_ester_count(self):\n",
    "        '''\n",
    "        Returns pandas frame containing the count of esters in core rings of >=12 membered macrocycles.\n",
    "        '''\n",
    "        ester_smarts = '[CX3](=[OX1])O@[r;!r3;!r4;!r5;!r6;!r7;!r8;!r9;!r10;!r11]'\n",
    "        core_ester = []\n",
    "        ester_mol = Chem.MolFromSmarts(ester_smarts)\n",
    "        for i in self.mols:\n",
    "            ester_count = len(i.GetSubstructMatches(ester_mol))\n",
    "            core_ester.append(ester_count)\n",
    "        df = pd.DataFrame(core_ester, columns=['core_ester'])\n",
    "        return df\n",
    "\n",
    "    def mordred_compute(self, name):\n",
    "        calc = Calculator(descriptors, ignore_3D=True)\n",
    "        df = calc.pandas(self.mols)\n",
    "        self.mordred = df\n",
    "        df.insert(loc=0, column='smiles', value=self.smiles)\n",
    "        df.to_csv(name[:-4]+'_mordred.csv', index=False)\n",
    "\n",
    "    def compute_mordred_macrocycle(self, name):\n",
    "        if not isinstance(self.mordred, pd.DataFrame):\n",
    "            self.mordred = self.mordred_compute(name)\n",
    "        ring_df = self.macrolide_ring_info()\n",
    "        sugar_df = self.sugar_count()\n",
    "        ester_df = self.core_ester_count()\n",
    "#        self.mrc = pd.concat([ring_df,sugar_df, ester_df], axis=1)\n",
    "        mordred_mrc = pd.concat([self.mordred, ring_df,sugar_df, ester_df], axis=1)\n",
    "        mordred_mrc.to_csv(name[:-4]+'_mordred_mrc.csv', index=False)\n",
    "\n",
    "def main():\n",
    "    #filename = './qm9.csv'  # path to your csv file\n",
    "    filename = './SMILES.csv'\n",
    "    df = pd.read_csv(filename)               # read the csv file as pandas data frame\n",
    "    smiles = [standardize_smiles(i) for i in df['SMILES'].values]  \n",
    "\n",
    "    ## Compute RDKit_2D Fingerprints and export a csv file.\n",
    "    Macrocycle_descriptor = Macrocycle_Descriptors(smiles)        # create your RDKit_2D object and provide smiles\n",
    "    Macrocycle_descriptor.compute_mordred_macrocycle(filename) # compute RDKit_2D and provide the name of your desired output file. you can use the same name as the input file because the RDKit_2D class will ensure to add \"_RDKit_2D.csv\" as part of the output file.\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://greglandrum.github.io/rdkit-blog/page2/\n",
    "dataset = pd.read_csv(\"./SMILES.csv\")[['SMILES']]\n",
    "#dataset = pd.read_csv(\"./qm9.csv\")[['SMILES']]\n",
    "#print(list(dataset))\n",
    "#list(dataset)\n",
    "#dataset\n",
    "#dataset.head()\n",
    "PandasTools.AddMoleculeColumnToFrame(dataset,'SMILES', 'Molecules' )\n",
    "dataset = dataset\n",
    "#Descriptors2D\n",
    "dataset['MolWt'] = [Descriptors.MolWt(mol) for mol in dataset['Molecules']]\n",
    "dataset['exactmw'] = [Descriptors.ExactMolWt(mol) for mol in dataset['Molecules']]\n",
    "dataset['FpDensityMorgan1'] = [Descriptors.FpDensityMorgan1(mol) for mol in dataset['Molecules']]\n",
    "dataset['FpDensityMorgan2'] = [Descriptors.FpDensityMorgan2(mol) for mol in dataset['Molecules']]\n",
    "dataset['FpDensityMorgan3'] = [Descriptors.FpDensityMorgan3(mol) for mol in dataset['Molecules']]\n",
    "dataset['HeavyAtomMolWt'] = [Descriptors.HeavyAtomMolWt(mol) for mol in dataset['Molecules']]\n",
    "dataset['MaxAbsPartialCharge'] = [Descriptors.MaxAbsPartialCharge(mol) for mol in dataset['Molecules']]\n",
    "dataset['MaxPartialCharge'] = [Descriptors.MaxPartialCharge(mol) for mol in dataset['Molecules']]\n",
    "dataset['MinAbsPartialCharge'] = [Descriptors.MinAbsPartialCharge(mol) for mol in dataset['Molecules']]\n",
    "dataset['NumRadicalElectrons'] = [Descriptors.NumRadicalElectrons(mol) for mol in dataset['Molecules']]\n",
    "dataset['NumValenceElectrons'] = [Descriptors.NumValenceElectrons(mol) for mol in dataset['Molecules']]\n",
    "#dataset['setupAUTOCorrDescriptors'] = [Descriptors.setupAUTOCorrDescriptors(mol) for mol in dataset['Molecules']]\n",
    "\n",
    "#Descriptors3D\n",
    "#dataset['Asphericity'] = [Chem.Descriptors3D.PMI1(mol) for mol in dataset['Molecules']]\n",
    "\n",
    "dataset['LOGP'] = [Crippen.MolLogP(mol) for mol in dataset['Molecules']]\n",
    "dataset['HBA'] = [Lipinski.NumHAcceptors(mol) for mol in dataset['Molecules']]\n",
    "dataset['HBD'] = [Lipinski.NumHDonors(mol) for mol in dataset['Molecules']]\n",
    "dataset['rotable'] = [Lipinski.NumRotatableBonds(mol) for mol in dataset['Molecules']]\n",
    "dataset['amide'] = [AllChem.CalcNumAmideBonds(mol) for mol in dataset['Molecules']]\n",
    "dataset['bridge'] = [AllChem.CalcNumBridgeheadAtoms(mol) for mol in dataset['Molecules']]\n",
    "dataset['heteroA'] = [Lipinski.NumHeteroatoms(mol) for mol in dataset['Molecules']]\n",
    "dataset['heavy'] = [Lipinski.HeavyAtomCount(mol) for mol in dataset['Molecules']]\n",
    "dataset['spiro'] = [AllChem.CalcNumSpiroAtoms(mol) for mol in dataset['Molecules']]\n",
    "dataset['FCSP3'] = [AllChem.CalcFractionCSP3(mol) for mol in dataset['Molecules']]\n",
    "dataset['ring'] = [Lipinski.RingCount(mol) for mol in dataset['Molecules']]\n",
    "dataset['Aliphatic'] = [AllChem.CalcNumAliphaticRings(mol) for mol in dataset['Molecules']]\n",
    "dataset['aromatic'] = [AllChem.CalcNumAromaticRings(mol) for mol in dataset['Molecules']]\n",
    "dataset['saturated'] = [AllChem.CalcNumSaturatedRings(mol) for mol in dataset['Molecules']]\n",
    "dataset['heteroR'] = [AllChem.CalcNumHeterocycles(mol) for mol in dataset['Molecules']]\n",
    "dataset['TPSA'] = [MolSurf.TPSA(mol) for mol in dataset['Molecules']]\n",
    "dataset['valence'] = [Descriptors.NumValenceElectrons(mol) for mol in dataset['Molecules']]\n",
    "dataset['mr'] = [Crippen.MolMR(mol) for mol in dataset['Molecules']]\n",
    "dataset['charge'] = [AllChem.ComputeGasteigerCharges(mol) for mol in dataset['Molecules']]\n",
    "\n",
    "# 'lipinskiHBA' \n",
    "# 'lipinskiHBD' \n",
    "# 'NumRotatableBonds' \n",
    "# 'NumHBD' \n",
    "# 'NumHBA' \n",
    "# 'NumHeteroatoms' \n",
    "# 'NumAmideBonds' \n",
    "# 'FractionCSP3' \n",
    "# 'NumRings' \n",
    "# 'NumAromaticRings' \n",
    "# 'NumAliphaticRings' \n",
    "# 'NumSaturatedRings' \n",
    "# 'NumHeterocycles' \n",
    "# 'NumAromaticHeterocycles' \n",
    "# 'NumSaturatedHeterocycles' \n",
    "# 'NumAliphaticHeterocycles' \n",
    "# 'NumSpiroAtoms' \n",
    "# 'NumBridgeheadAtoms' \n",
    "# 'NumAtomStereoCenters' \n",
    "# 'NumUnspecifiedAtomStereoCenters' \n",
    "# 'labuteASA' \n",
    "# 'tpsa'\n",
    "# 'CrippenClogP' \n",
    "# 'CrippenMR'\n",
    "\n",
    "dataset.head()\n",
    "dataset = dataset\n",
    "# dataset['fps-SmilesMolSupplier'] = [rdMolDescriptors.GetMorganFingerprintAsBitVect(mol,2,2048) for mol in dataset['Molecules']]\n",
    "# #dataset.head()\n",
    "fpsdataset = dataset\n",
    "# len(fpsdataset)\n",
    "fpsdataset = fpsdataset.drop(columns = 'Molecules')\n",
    "fpsdataset.head()\n",
    "# fpsdataset.to_csv(\"./fpsdatasetqm9.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smiles = 'CC1=C(C(O)=O)C2=CC(=CC=C2N=C1C3=CC=C(C=C3)C4=CC=CC=C4F)F'\n",
    "mol = Chem.MolFromSmiles(smiles)\n",
    "mol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "smi = Chem.MolToSmiles(mol)\n",
    "print(smi)\n",
    "print(Chem.MolToInchiKey(mol))\n",
    "mol_block = Chem.MolToMolBlock(mol)\n",
    "print(mol_block)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/XuhanLiu/DrugEx\n",
    "# https://www.programcreek.com/python/example/124114/rdkit.Chem.Descriptors.MolWt\n",
    "\n",
    "def PhyChem(smiles):\n",
    "    \"\"\" Calculating the 19D physicochemical descriptors for each molecules,\n",
    "    the value has been normalized with Gaussian distribution.\n",
    "\n",
    "    Arguments:\n",
    "        smiles (list): list of SMILES strings.\n",
    "    Returns:\n",
    "        props (ndarray): m X 19 matrix as nomalized PhysChem descriptors.\n",
    "            m is the No. of samples\n",
    "    \"\"\"\n",
    "    props = []\n",
    "    for smile in smiles:\n",
    "        mol = Chem.MolFromSmiles(smile)\n",
    "        try:\n",
    "            MW = Descriptors.MolWt(mol)\n",
    "            LOGP = Crippen.MolLogP(mol)\n",
    "            HBA = Lipinski.NumHAcceptors(mol)\n",
    "            HBD = Lipinski.NumHDonors(mol)\n",
    "            rotable = Lipinski.NumRotatableBonds(mol)\n",
    "            amide = AllChem.CalcNumAmideBonds(mol)\n",
    "            bridge = AllChem.CalcNumBridgeheadAtoms(mol)\n",
    "            heteroA = Lipinski.NumHeteroatoms(mol)\n",
    "            heavy = Lipinski.HeavyAtomCount(mol)\n",
    "            spiro = AllChem.CalcNumSpiroAtoms(mol)\n",
    "            FCSP3 = AllChem.CalcFractionCSP3(mol)\n",
    "            ring = Lipinski.RingCount(mol)\n",
    "            Aliphatic = AllChem.CalcNumAliphaticRings(mol)\n",
    "            aromatic = AllChem.CalcNumAromaticRings(mol)\n",
    "            saturated = AllChem.CalcNumSaturatedRings(mol)\n",
    "            heteroR = AllChem.CalcNumHeterocycles(mol)\n",
    "            TPSA = MolSurf.TPSA(mol)\n",
    "            valence = Descriptors.NumValenceElectrons(mol)\n",
    "            mr = Crippen.MolMR(mol)\n",
    "            # charge = AllChem.ComputeGasteigerCharges(mol)\n",
    "            prop = [MW, LOGP, HBA, HBD, rotable, amide, bridge, heteroA, heavy, spiro,\n",
    "                    FCSP3, ring, Aliphatic, aromatic, saturated, heteroR, TPSA, valence, mr]\n",
    "        except:\n",
    "            print(smile)\n",
    "            prop = [0] * 19\n",
    "        props.append(prop)\n",
    "    props = np.array(props)\n",
    "    props = Scaler().fit_transform(props)\n",
    "    return props "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#computing morgan fingerprints  vectors\n",
    "mol = Chem.MolFromSmiles('C/C1=C\\\\C[C@H]([C+](C)C)CC/C(C)=C/CC1')\n",
    "fp1 = AllChem.GetMorganFingerprintAsBitVect(mol, useChirality=True, radius=2, nBits=124)\n",
    "vec1 = np.array(fp1)\n",
    "print(vec1)\n",
    "morgan_fp_gen = rdFingerprintGenerator.GetMorganGenerator(includeChirality=True, radius=2, fpSize=124, useCountSimulation=False)\n",
    "fp2 = morgan_fp_gen.GetFingerprint(mol)\n",
    "vec2 = np.array(fp2)\n",
    "print(vec2)\n",
    "assert np.all(vec1 == vec2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/67302261/cant-convert-molecule-to-fingerprint-with-rdkit\n",
    "from rdkit.Chem import AllChem as Chem\n",
    "\n",
    "fragment = Chem.MolFromSmiles('Nc1cccc(N)n1')\n",
    "\n",
    "smiles = ['Nc1cc(CSc2ccc(O)cc2)cc(N)n1', 'Nc1cc(COc2ccc(O)cc2)cc(N)n1', 'CC1=CC=Cc2c(N)nc(N)cc12']\n",
    "\n",
    "for smi in smiles:\n",
    "    try:\n",
    "        mol = Chem.MolFromSmiles(smi)\n",
    "        f1 = Chem.DeleteSubstructs(mol, fragment)\n",
    "        f2 = Chem.MolFromSmiles(Chem.MolToSmiles(f1))\n",
    "        fp = Chem.GetMorganFingerprintAsBitVect(f2, 2)\n",
    "    except:\n",
    "        print('SMILES:', smi)\n",
    "        f = Chem.DeleteSubstructs(mol, fragment)\n",
    "        print('smiles_frag:', Chem.MolToSmiles(f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = 'somedata.smi'\n",
    "\n",
    "with open(file_name, \"r\") as ins:\n",
    "    smiles = []\n",
    "    for line in ins:\n",
    "        smiles.append(line.split('\\n')[0])\n",
    "print('# of SMILES:', len(smiles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# directly feed SMILE structures stored in a pandas dataframe into RDKit to calculate molecular fingerprint and \n",
    "\n",
    "df = pd.read_csv(\"./SMILES_feature.csv\")\n",
    "from rdkit import DataStructs\n",
    "\n",
    "target = Chem.RDKFingerprint(Chem.MolFromSmiles('CC1=C(C(O)=O)C2=CC(=CC=C2N=C1C3=CC=C(C=C3)C4=CC=CC=C4F)F'))\n",
    "df_smiles = pd.DataFrame(df.smiles)\n",
    "#display(df_smiles)\n",
    "df1 = pd.DataFrame(data=df.smiles)\n",
    "df1['Tanimoto'] = DataStructs.BulkTanimotoSimilarity(target, [Chem.RDKFingerprint(Chem.MolFromSmiles(s)) for s in df['smiles']])\n",
    "\n",
    "print(df1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Export pandas data frame with mol image\n",
    "import pandas as pd\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import PandasTools\n",
    "#DataFrame = pd.read_csv(\"./SMILES_feature.csv\")\n",
    "#smiles = [pd.DataFrame(df.smiles)]\n",
    "smiles = ['N#CC(c1ccccc1)C(Br)Oc1ccccc1','O=[N+]([O-])c1ccc(C(c2ccccc2)C(Br)Oc2ccccc2)cc1','CC(Oc1ccccc1)C(C#N)c1ccc(C#N)cc1 ','COC(C#N)C(C)(C)c1ccc(-c2ccccc2)cc1 ','COC(Oc1ccccc1)C(C)(C#N)c1ccc([N+](=O)[O-])cc1 ','COCC(C)(c1ccccc1)c1ccc([N+](=O)[O-])cc1 ','CCc1ccc(C(C#N)C(OC)OCOC)cc1 ','COC(C#N)C(C)(C)c1ccccc1 ','CCc1ccc(C(C)(c2ccccc2)C(Br)OC)cc1 ','COCOC(C)C(C)c1ccc(C(=O)OC)cc1 ','CCc1ccc(C(C#N)COc2ccccc2)cc1 ','COC(=O)c1ccc(C(C)(c2ccccc2)C(OC)OC)cc1 ','CCc1ccc(C(C)(C#N)C(C)OC)cc1 ','COC(C#N)C(C#N)c1ccc(C#N)cc1 ','COCOC(C#N)C(C)(C#N)c1ccc(C#N)cc1 ','COCC(C)(C#N)c1ccc(C#N)cc1 ','CC(C)(COc1ccccc1)c1ccc([N+](=O)[O-])cc1 ','COc1ccc(C(C#N)(C#N)C(C)OC)cc1 ','COC(Br)C(C)(C#N)c1ccc([N+](=O)[O-])cc1 ','COC(c1ccccc1)C(C#N)(C#N)c1ccc(-c2ccccc2)cc1 ','COC(c1ccccc1)C(C#N)c1ccc([N+](=O)[O-])cc1 ','CCc1ccc(CCOC)cc1 ','COCOCC(C#N)(C#N)c1ccccc1 ','COCOCC(C)c1ccccc1 ','COC(C#N)C(c1ccccc1)c1ccc(C#N)cc1 ','COC(=O)c1ccc(C(C)(C#N)C(C#N)OC)cc1 ','COc1ccc(C(C#N)(c2ccccc2)C(OC)OC)cc1 ','CCc1ccc(C(C#N)C(C#N)OCOC)cc1 ','COC(=O)c1ccc(C(C#N)C(Br)Oc2ccccc2)cc1 ','COCOC(Br)C(C)(C#N)c1ccc([N+](=O)[O-])cc1 ','COC(Br)C(C)(C)c1ccc(C#N)cc1 ','COC(Oc1ccccc1)C(C#N)c1ccccc1 ','N#Cc1ccc(C(c2ccccc2)(c2ccccc2)C(Oc2ccccc2)c2ccccc2)cc1 ','COC(c1ccccc1)C(c1ccccc1)c1ccc([N+](=O)[O-])cc1 ','COCC(c1ccccc1)(c1ccccc1)c1ccccc1 ','COC(=O)c1ccc(C(C)(C#N)C(OC)OC)cc1 ','COCOC(Br)C(c1ccccc1)c1ccccc1 ','CCc1ccc(CCOCOC)cc1 ','COCC(C)(C#N)c1ccccc1 ','COCOC(C)C(c1ccccc1)c1ccccc1 ','N#Cc1ccc(C(c2ccccc2)(c2ccccc2)C(C#N)Oc2ccccc2)cc1 ','COC(=O)c1ccc(C(C)(c2ccccc2)C(Br)Oc2ccccc2)cc1 ','COCOCC(C)(C)c1ccc(C#N)cc1 ','CCc1ccc(C(C#N)C(Br)OCOC)cc1 ','COCOC(Br)C(C)(C#N)c1ccc(C#N)cc1 ','O=[N+]([O-])c1ccc(CC(Oc2ccccc2)c2ccccc2)cc1 ','CCc1ccc(C(C#N)(C#N)C(Br)OCOC)cc1 ','COCC(C#N)(C#N)c1ccc(C(=O)OC)cc1 ','CCc1ccc(C(C)(C#N)COCOC)cc1 ','N#CC(Oc1ccccc1)C(C#N)c1ccc(-c2ccccc2)cc1 ','COC(C#N)C(c1ccccc1)c1ccc(-c2ccccc2)cc1 ','COCOCC(C)(C)c1ccc([N+](=O)[O-])cc1 ','COC(C#N)C(C#N)c1ccc(-c2ccccc2)cc1 ','CCc1ccc(C(C)(C)C(OCOC)c2ccccc2)cc1 ','COCOC(Br)C(C)c1ccc([N+](=O)[O-])cc1 ','COCOCC(C#N)(c1ccccc1)c1ccc(C#N)cc1 ','COC(=O)c1ccc(C(C#N)C(C)OC)cc1 ','COc1ccc(C(C#N)(C#N)C(Br)OC)cc1 ','COC(=O)c1ccc(C(C)(C#N)C(Br)Oc2ccccc2)cc1 ','COC(c1ccccc1)C(C#N)(C#N)c1ccc([N+](=O)[O-])cc1 ','COCOC(OC)C(C)(C#N)c1ccc(-c2ccccc2)cc1 ','COC(=O)c1ccc(C(C)(C)C(Br)Oc2ccccc2)cc1 ','CCc1ccc(C(C#N)(c2ccccc2)C(Br)Oc2ccccc2)cc1 ','COC(=O)c1ccc(C(C)(c2ccccc2)C(C#N)OC)cc1 ','COC(c1ccccc1)C(C#N)c1ccc(C#N)cc1 ','CCc1ccc(C(c2ccccc2)(c2ccccc2)C(Br)Oc2ccccc2)cc1 ','COC(C)C(C)(c1ccccc1)c1ccc([N+](=O)[O-])cc1 ','COC(C)C(C)c1ccc(C#N)cc1 ','COCOC(Br)C(c1ccccc1)(c1ccccc1)c1ccc(C#N)cc1 ','CCc1ccc(C(C#N)(C#N)COCOC)cc1 ','COCC(c1ccccc1)(c1ccccc1)c1ccc(C#N)cc1 ','COCOC(Br)Cc1ccc(C#N)cc1 ','COCOC(OC)C(c1ccccc1)(c1ccccc1)c1ccc(C#N)cc1 ','CCc1ccc(C(C)(C)C(C#N)OCOC)cc1 ','COC(Br)C(C)c1ccccc1 ','COC(C#N)Cc1ccc([N+](=O)[O-])cc1 ','CC(c1ccccc1)(c1ccc(-c2ccccc2)cc1)C(C#N)Oc1ccccc1 ','CC(c1ccc([N+](=O)[O-])cc1)C(C#N)Oc1ccccc1 ','COCOC(C)C(C)c1ccc([N+](=O)[O-])cc1 ','COC(=O)c1ccc(C(C#N)C(Oc2ccccc2)c2ccccc2)cc1 ','COC(C)C(C#N)c1ccc(-c2ccccc2)cc1 ','COCOC(OC)C(C#N)c1ccccc1 ','COC(C)C(C#N)(C#N)c1ccc([N+](=O)[O-])cc1 ','COCC(C)c1ccccc1 ','CCc1ccc(C(C)(C#N)C(OCOC)c2ccccc2)cc1 ','COCC(C#N)(C#N)c1ccc(OC)cc1 ','CCc1ccc(C(C)(C#N)C(OC)OCOC)cc1 ','c1ccc(OCC(c2ccccc2)c2ccccc2)cc1 ','COC(=O)c1ccc(C(c2ccccc2)C(C#N)OC)cc1 ','COC(=O)c1ccc(C(C#N)(C#N)C(OC)OC)cc1 ','COC(C#N)C(c1ccccc1)c1ccc([N+](=O)[O-])cc1 ','COC(Oc1ccccc1)C(C)(C)c1ccc(C#N)cc1 ','CCc1ccc(CC(OCOC)c2ccccc2)cc1 ','N#CC(Oc1ccccc1)C(C#N)(c1ccccc1)c1ccc([N+](=O)[O-])cc1 ','CCc1ccc(C(c2ccccc2)(c2ccccc2)C(OC)Oc2ccccc2)cc1 ','COCOC(C#N)C(C#N)(C#N)c1ccccc1 ','COC(=O)c1ccc(CC(C#N)OC)cc1 ','CCc1ccc(CC(C#N)OC)cc1 ','CC(Oc1ccccc1)C(C)(C#N)c1ccc(C#N)cc1 ','COC(Oc1ccccc1)C(C#N)(C#N)c1ccccc1 ','COc1ccc(C(C#N)COc2ccccc2)cc1 ','CCc1ccc(C(C#N)(c2ccccc2)C(Br)OCOC)cc1 ','CCc1ccc(C(C)(COCOC)c2ccccc2)cc1 ','CCc1ccc(C(C#N)C(Br)OC)cc1 ','COCOCC(c1ccccc1)(c1ccccc1)c1ccccc1 ','COCOC(C)Cc1ccccc1 ','CC(c1ccccc1)C(Br)Oc1ccccc1 ','CC(C#N)(c1ccccc1)C(Oc1ccccc1)c1ccccc1 ','COC(c1ccccc1)C(C#N)c1ccc(-c2ccccc2)cc1 ','CCc1ccc(C(C)(c2ccccc2)C(C#N)OCOC)cc1 ','N#CC(COc1ccccc1)(c1ccccc1)c1ccc([N+](=O)[O-])cc1 ','COCOC(C)C(C)(c1ccccc1)c1ccc(C(=O)OC)cc1 ','CCc1ccc(C(C)(C)C(C#N)Oc2ccccc2)cc1 ','COCOC(c1ccccc1)C(C)(C)c1ccccc1 ','COC(C)C(C)(C#N)c1ccc([N+](=O)[O-])cc1 ','COC(Oc1ccccc1)C(C#N)(C#N)c1ccc(-c2ccccc2)cc1 ','CC(c1ccc(C#N)cc1)C(Oc1ccccc1)c1ccccc1 ',\n",
    "          'COC(=O)c1ccc(C(C)C(C)OC)cc1 ','COC(=O)c1ccc(C(C)(C)C(C#N)Oc2ccccc2)cc1 ','COC(Oc1ccccc1)C(c1ccccc1)c1ccc(C#N)cc1 ','COCOC(C#N)C(C)(c1ccccc1)c1ccccc1 ','CCc1ccc(C(c2ccccc2)C(C#N)OC)cc1 ','COC(=O)c1ccc(C(C#N)(C#N)C(C)Oc2ccccc2)cc1 ','COCC(C#N)(C#N)c1ccc([N+](=O)[O-])cc1 ','COCOC(C)C(C#N)(C#N)c1ccc(-c2ccccc2)cc1 ','COCOC(C#N)C(C)(C#N)c1ccc(OC)cc1 ','COC(C#N)C(C#N)(c1ccccc1)c1ccc(-c2ccccc2)cc1 ','CCc1ccc(C(C#N)(C#N)C(OCOC)c2ccccc2)cc1 ','COC(=O)c1ccc(CCOc2ccccc2)cc1 ','BrC(Cc1ccccc1)Oc1ccccc1 ','CCc1ccc(C(COCOC)c2ccccc2)cc1 ','COC(C)C(c1ccccc1)(c1ccccc1)c1ccc(C#N)cc1 ','COCOC(c1ccccc1)C(c1ccccc1)c1ccc(C(=O)OC)cc1 ','COCOCC(C)(c1ccccc1)c1ccc([N+](=O)[O-])cc1 ','COC(Cc1ccc(C#N)cc1)c1ccccc1 ','CCc1ccc(C(C)(c2ccccc2)C(OC)OC)cc1 ','COC(=O)c1ccc(C(C)C(Br)Oc2ccccc2)cc1 ','COC(Br)Cc1ccc([N+](=O)[O-])cc1 ','COC(=O)c1ccc(C(C)(C#N)C(Br)OC)cc1 ','COCOC(C)C(C#N)c1ccc(-c2ccccc2)cc1 ','COCOC(Br)C(C#N)c1ccc(C#N)cc1 ','BrC(Oc1ccccc1)C(c1ccccc1)c1ccccc1 ','COC(c1ccccc1)C(c1ccccc1)c1ccc(C#N)cc1 ','CCc1ccc(C(C)(C)C(C)OCOC)cc1 ','COCOC(OC)C(C)(c1ccccc1)c1ccc([N+](=O)[O-])cc1 ','COCOCC(c1ccccc1)c1ccccc1 ','COC(Cc1ccccc1)OC ','N#CC(c1ccccc1)(c1ccc(-c2ccccc2)cc1)C(Br)Oc1ccccc1 ','COCOC(C#N)C(C)(c1ccccc1)c1ccc(C#N)cc1 ','CCc1ccc(C(C#N)(COC)c2ccccc2)cc1 ','COC(C#N)C(C)c1ccc([N+](=O)[O-])cc1 ','COC(=O)c1ccc(C(C)C(C#N)Oc2ccccc2)cc1 ','COC(C)C(c1ccccc1)c1ccc(C#N)cc1 ','COC(c1ccccc1)C(c1ccccc1)(c1ccccc1)c1ccc([N+](=O)[O-])cc1 ','CC(Oc1ccccc1)C(C)(C#N)c1ccc(-c2ccccc2)cc1 ','COC(=O)c1ccc(C(C)C(C#N)OC)cc1 ','N#Cc1ccc(C(C#N)(c2ccccc2)C(Br)Oc2ccccc2)cc1 ','O=[N+]([O-])c1ccc(C(COc2ccccc2)(c2ccccc2)c2ccccc2)cc1 ','COCOC(OC)C(C#N)(c1ccccc1)c1ccc([N+](=O)[O-])cc1 ','CCc1ccc(C(C)(C#N)C(Br)Oc2ccccc2)cc1 ','CCc1ccc(C(C#N)C(C)OCOC)cc1 ','COC(=O)c1ccc(C(COc2ccccc2)c2ccccc2)cc1 ','COC(C)C(C#N)c1ccccc1 ','COC(=O)c1ccc(C(C#N)C(C#N)OC)cc1 ','COCOC(c1ccccc1)C(C#N)(c1ccccc1)c1ccc([N+](=O)[O-])cc1 ','COC(Br)C(c1ccccc1)(c1ccccc1)c1ccccc1 ','COC(Br)C(C#N)c1ccc([N+](=O)[O-])cc1 ','COC(OC)C(C#N)c1ccccc1 ','CCc1ccc(C(C)(c2ccccc2)C(Br)OCOC)cc1 ','CC(c1ccccc1)(c1ccc(C#N)cc1)C(C#N)Oc1ccccc1 ','COCOC(c1ccccc1)C(c1ccccc1)(c1ccccc1)c1ccc(C#N)cc1 ','CCc1ccc(C(C#N)C(OC)Oc2ccccc2)cc1 ','COCOC(Br)C(C)c1ccc(C(=O)OC)cc1 ','N#CC(C#N)(c1ccc([N+](=O)[O-])cc1)C(Br)Oc1ccccc1 ','N#Cc1ccc(C(C#N)C(Br)Oc2ccccc2)cc1 ','COCC(C#N)(c1ccccc1)c1ccc(C(=O)OC)cc1 ','O=[N+]([O-])c1ccc(CCOc2ccccc2)cc1 ','COCOC(C#N)C(C)(C#N)c1ccc(C(=O)OC)cc1 ','COC(=O)c1ccc(C(C#N)(C#N)C(Br)OC)cc1 ','COC(=O)c1ccc(C(c2ccccc2)(c2ccccc2)C(Br)Oc2ccccc2)cc1 ','COCOC(Br)C(C#N)(c1ccccc1)c1ccccc1 ','COCOC(Br)C(c1ccccc1)c1ccc(C#N)cc1 ','N#CC(C#N)(c1ccc(-c2ccccc2)cc1)C(Oc1ccccc1)c1ccccc1 ','COCOC(C#N)C(C)(C)c1ccc(C(=O)OC)cc1 ','CCc1ccc(C(C#N)C(Oc2ccccc2)c2ccccc2)cc1 ','CCc1ccc(C(C)(C)C(C#N)OC)cc1 ','COC(Br)C(C#N)(c1ccccc1)c1ccc(-c2ccccc2)cc1 ','CCc1ccc(C(C#N)C(OC)OC)cc1 ','N#Cc1ccc(C(C#N)(COc2ccccc2)c2ccccc2)cc1 ','COCOC(c1ccccc1)C(C)(C)c1ccc(C(=O)OC)cc1 ','COCOC(C)C(C)(C#N)c1ccc(-c2ccccc2)cc1 ','COC(Br)C(C)(C#N)c1ccc(-c2ccccc2)cc1 ','CCc1ccc(C(C)(C)C(Br)OC)cc1 ','COC(C#N)C(c1ccccc1)(c1ccccc1)c1ccccc1 ','CCc1ccc(C(C)(C)C(Br)OCOC)cc1 ','COC(=O)c1ccc(C(C)(C)C(C#N)OC)cc1 ','COC(OC)C(c1ccccc1)c1ccccc1 ','COC(C)C(C#N)c1ccc(C#N)cc1 ','COCOC(Br)C(C)(C)c1ccc(C#N)cc1 ','CCc1ccc(C(c2ccccc2)C(OC)Oc2ccccc2)cc1 ','CC(Cc1ccc([N+](=O)[O-])cc1)Oc1ccccc1 ','COCOC(C)C(C#N)(C#N)c1ccc([N+](=O)[O-])cc1 ','COCOC(Br)C(C)(C#N)c1ccccc1 ','COCOC(C#N)C(C#N)(C#N)c1ccc(C(=O)OC)cc1 ','CCc1ccc(C(C#N)(C#N)C(C#N)OCOC)cc1 ','N#Cc1ccc(C(c2ccccc2)C(C#N)Oc2ccccc2)cc1 ','COCOC(C)Cc1ccc([N+](=O)[O-])cc1 ','COC(Br)C(c1ccccc1)c1ccc([N+](=O)[O-])cc1 ','CC(C#N)(c1ccc(-c2ccccc2)cc1)C(Oc1ccccc1)c1ccccc1 ','CC(Oc1ccccc1)C(C)(C#N)c1ccc([N+](=O)[O-])cc1 ','COC(OC)C(C)(C#N)c1ccccc1 ','COc1ccc(C(C)(C#N)COc2ccccc2)cc1 ','COC(c1ccccc1)C(C)(C#N)c1ccc(-c2ccccc2)cc1 ','CCc1ccc(C(C#N)(c2ccccc2)C(C)Oc2ccccc2)cc1 ','COC(=O)c1ccc(C(C#N)(C#N)C(Br)Oc2ccccc2)cc1 ','COC(=O)c1ccc(C(c2ccccc2)C(C#N)Oc2ccccc2)cc1 ','COC(Cc1ccc([N+](=O)[O-])cc1)OC ','COC(c1ccccc1)C(C#N)c1ccccc1 ','COCOCCc1ccc(C(=O)OC)cc1 ','COC(Oc1ccccc1)C(C#N)(c1ccccc1)c1ccc([N+](=O)[O-])cc1 ','COC(Oc1ccccc1)C(c1ccccc1)(c1ccccc1)c1ccccc1 ','COCC(C#N)(c1ccccc1)c1ccccc1 ','CC(C)(c1ccc(C#N)cc1)C(C#N)Oc1ccccc1 ','O=[N+]([O-])c1ccc(C(c2ccccc2)(c2ccccc2)C(Oc2ccccc2)c2ccccc2)cc1 ','COCOC(C)C(C#N)(C#N)c1ccc(OC)cc1 ','COC(Oc1ccccc1)C(C)c1ccc(C#N)cc1 ','COC(=O)c1ccc(CC(Br)OC)cc1 ','COCOC(Cc1ccc(C(=O)OC)cc1)c1ccccc1 ','COCC(C)(c1ccccc1)c1ccc(C#N)cc1 ',\n",
    "          'COCOC(OC)C(C)(c1ccccc1)c1ccc(C#N)cc1 ','COC(Cc1ccc([N+](=O)[O-])cc1)Oc1ccccc1 ','COCC(C)(C)c1ccc(C#N)cc1 ','COCOC(C)C(C)(C#N)c1ccc(C(=O)OC)cc1 ','COC(c1ccccc1)C(c1ccccc1)c1ccccc1 ','COC(Cc1ccc(C#N)cc1)Oc1ccccc1 ','COc1ccc(C(C#N)(C#N)C(OC)c2ccccc2)cc1 ','N#CC(c1ccccc1)(c1ccc([N+](=O)[O-])cc1)C(Oc1ccccc1)c1ccccc1 ','COc1ccc(C(C#N)C(C#N)Oc2ccccc2)cc1 ','N#Cc1ccc(C(C#N)(c2ccccc2)C(C#N)Oc2ccccc2)cc1 ','COCOC(Br)C(C)(C)c1ccc(C(=O)OC)cc1 ','COC(Br)C(C)(C)c1ccc([N+](=O)[O-])cc1 ','N#CC(Oc1ccccc1)C(c1ccccc1)c1ccccc1 ','COC(C)C(C)(c1ccccc1)c1ccc(C#N)cc1 ','CCc1ccc(C(C)(C#N)C(C#N)OCOC)cc1 ','COCOC(C#N)C(C)(C)c1ccc(C#N)cc1 ','COCOC(C#N)C(C)c1ccccc1 ','BrC(Oc1ccccc1)C(c1ccccc1)(c1ccccc1)c1ccccc1 ','COCOC(C)C(c1ccccc1)(c1ccccc1)c1ccc([N+](=O)[O-])cc1 ','CCc1ccc(C(C)C(OC)OCOC)cc1 ','CC(C)(c1ccc(C#N)cc1)C(Br)Oc1ccccc1 ','COC(=O)c1ccc(CC(OC)c2ccccc2)cc1 ','CCc1ccc(C(C)(C#N)COc2ccccc2)cc1 ','CCc1ccc(C(C)C(C#N)OCOC)cc1 ','COCOC(C#N)C(C)(C#N)c1ccc([N+](=O)[O-])cc1 ','COCOC(Cc1ccccc1)OC ','COCOCC(C#N)c1ccc([N+](=O)[O-])cc1 ','CCc1ccc(C(C)C(C)OC)cc1 ','COCOC(C#N)C(C#N)(c1ccccc1)c1ccccc1 ','COC(Oc1ccccc1)C(C#N)(c1ccccc1)c1ccc(-c2ccccc2)cc1 ','COCOCC(C)c1ccc([N+](=O)[O-])cc1 ','COCOC(OC)C(C#N)(C#N)c1ccc(C(=O)OC)cc1 ', 'COCOC(C#N)Cc1ccc(OC)cc1','COC(=O)c1ccc(C(C#N)C(Br)OC)cc1 ','CCc1ccc(C(C)(C#N)COC)cc1 ','COC(Br)C(C#N)c1ccccc1 ','COC(=O)c1ccc(CC(C)OC)cc1 ','COC(C)C(C)(c1ccccc1)c1ccccc1 ','COc1ccc(C(C#N)(C#N)COc2ccccc2)cc1 ','COc1ccc(C(C#N)(C#N)C(Br)Oc2ccccc2)cc1 ','COC(C)Cc1ccccc1 ','COC(Br)C(C#N)(C#N)c1ccc(-c2ccccc2)cc1 ','COC(=O)c1ccc(C(C#N)(c2ccccc2)C(C#N)OC)cc1 ','CCc1ccc(C(C#N)(C#N)C(OC)OC)cc1 ','CCc1ccc(C(C)(COC)c2ccccc2)cc1 ','COC(Br)C(C#N)(c1ccccc1)c1ccc(C#N)cc1 ','CCc1ccc(C(C)(c2ccccc2)C(OCOC)c2ccccc2)cc1 ','COC(C#N)C(C#N)(C#N)c1ccc(C#N)cc1 ','COCOC(Cc1ccccc1)c1ccccc1 ','COCOC(C#N)C(C#N)(C#N)c1ccc(C#N)cc1 ','COC(C)C(C#N)(C#N)c1ccc(-c2ccccc2)cc1 ','COCOC(Br)C(C#N)(c1ccccc1)c1ccc(C#N)cc1 ','COCOC(Br)C(c1ccccc1)(c1ccccc1)c1ccccc1 ','COCOCC(C#N)c1ccc(OC)cc1 ','COCOC(C#N)C(c1ccccc1)(c1ccccc1)c1ccc(-c2ccccc2)cc1 ','COCOC(C#N)C(C)(C#N)c1ccc(-c2ccccc2)cc1 ','N#CC(C#N)(COc1ccccc1)c1ccccc1 ','COCOC(OC)C(C#N)(C#N)c1ccc([N+](=O)[O-])cc1 ','COC(=O)c1ccc(C(C#N)(c2ccccc2)C(OC)c2ccccc2)cc1 ','COC(=O)c1ccc(C(C)(c2ccccc2)C(OC)c2ccccc2)cc1 ','COC(OC)C(C#N)(c1ccccc1)c1ccc(-c2ccccc2)cc1 ','COC(=O)c1ccc(C(C#N)(C#N)C(OC)Oc2ccccc2)cc1 ','COCOC(Br)C(C#N)(c1ccccc1)c1ccc(-c2ccccc2)cc1 ','N#CC(Oc1ccccc1)C(c1ccccc1)(c1ccccc1)c1ccc([N+](=O)[O-])cc1 ','COCOC(C)C(C)c1ccccc1 ','COC(=O)c1ccc(C(C)C(Br)OC)cc1 ','COCC(C)(c1ccccc1)c1ccccc1 ','N#Cc1ccc(C(c2ccccc2)C(Br)Oc2ccccc2)cc1 ','COCOC(OC)C(c1ccccc1)c1ccc(C(=O)OC)cc1 ','COCOC(c1ccccc1)C(c1ccccc1)c1ccc([N+](=O)[O-])cc1 ','COC(=O)c1ccc(C(C)(c2ccccc2)C(C)OC)cc1 ','COCOC(Cc1ccc([N+](=O)[O-])cc1)OC ','COCOC(c1ccccc1)C(C#N)(c1ccccc1)c1ccc(-c2ccccc2)cc1 ','COCOC(C#N)C(C#N)c1ccc(C(=O)OC)cc1 ','COC(=O)c1ccc(C(C#N)(c2ccccc2)C(OC)OC)cc1 ','COCOC(OC)C(c1ccccc1)c1ccc([N+](=O)[O-])cc1 ','CC(COc1ccccc1)c1ccc([N+](=O)[O-])cc1 ','c1ccc(OC(c2ccccc2)C(c2ccccc2)c2ccccc2)cc1 ','COCOC(c1ccccc1)C(C#N)(C#N)c1ccc(-c2ccccc2)cc1 ','COCOC(Br)C(C#N)(c1ccccc1)c1ccc([N+](=O)[O-])cc1 ','COC(Br)C(c1ccccc1)(c1ccccc1)c1ccc(C#N)cc1 ','COC(=O)c1ccc(C(C)(C#N)C(C)Oc2ccccc2)cc1 ','COCOCC(C#N)(c1ccccc1)c1ccccc1 ','COC(Oc1ccccc1)C(C#N)(c1ccccc1)c1ccccc1 ','COCOCC(C)(C)c1ccccc1 ','CCc1ccc(C(c2ccccc2)C(OC)OCOC)cc1 ','CC(Oc1ccccc1)C(C#N)(C#N)c1ccccc1 ','COC(Br)C(C#N)(C#N)c1ccccc1 ','COCC(C)(C#N)c1ccc(OC)cc1 ','COCOC(Br)C(C#N)(C#N)c1ccc(C#N)cc1 ','COC(Oc1ccccc1)C(c1ccccc1)c1ccccc1 ','COCOC(C)C(c1ccccc1)c1ccc(C(=O)OC)cc1 ','COCOC(C)C(c1ccccc1)(c1ccccc1)c1ccc(C#N)cc1 ','COCC(C)(C#N)c1ccc(-c2ccccc2)cc1 ','COC(C#N)C(c1ccccc1)(c1ccccc1)c1ccc([N+](=O)[O-])cc1 ','COC(Oc1ccccc1)C(C)(C)c1ccc([N+](=O)[O-])cc1 ','COC(Br)C(c1ccccc1)c1ccccc1 ','COCOC(Cc1ccc([N+](=O)[O-])cc1)c1ccccc1 ','COC(C)C(C)(C#N)c1ccc(-c2ccccc2)cc1 ','CC(Oc1ccccc1)C(C#N)c1ccccc1 ','COCCc1ccc(C(=O)OC)cc1 ','CC(c1ccccc1)(c1ccc(C#N)cc1)C(Br)Oc1ccccc1 ','CCc1ccc(CC(C)OC)cc1 ','CCc1ccc(C(C)(C#N)C(C#N)OC)cc1 ','COCOC(C#N)C(c1ccccc1)(c1ccccc1)c1ccc(C#N)cc1 ','COCOC(Br)C(C#N)(C#N)c1ccc([N+](=O)[O-])cc1 ','COC(Oc1ccccc1)C(c1ccccc1)(c1ccccc1)c1ccc([N+](=O)[O-])cc1 ','COCC(c1ccccc1)c1ccc(C#N)cc1 ','COCOC(c1ccccc1)C(C)(C)c1ccc([N+](=O)[O-])cc1 ','COc1ccc(C(C#N)(C#N)C(C#N)OC)cc1 ','CCc1ccc(C(c2ccccc2)C(Br)OCOC)cc1 ','COC(=O)c1ccc(C(c2ccccc2)(c2ccccc2)C(Oc2ccccc2)c2ccccc2)cc1 ','CCc1ccc(C(C#N)(C#N)COc2ccccc2)cc1 ',\n",
    "\t\t  'COCOCC(C)(c1ccccc1)c1ccc(C#N)cc1 ','CCc1ccc(CC(C#N)Oc2ccccc2)cc1 ','COC(Br)C(C)(c1ccccc1)c1ccccc1 ','COCOCC(C#N)(C#N)c1ccc([N+](=O)[O-])cc1 ','COC(OC)C(C#N)c1ccc(-c2ccccc2)cc1 ','COC(OC)C(C)(C)c1ccccc1 ','COCOC(OC)C(C)(C#N)c1ccc(OC)cc1 ','COC(OC)C(c1ccccc1)(c1ccccc1)c1ccc(C#N)cc1 ','COCOCC(C)(C)c1ccc(C(=O)OC)cc1 ','COCOC(c1ccccc1)C(C)c1ccc(C#N)cc1 ','COC(OC)C(C)(C#N)c1ccc(-c2ccccc2)cc1 ','COC(=O)c1ccc(C(C)(c2ccccc2)C(Br)OC)cc1 ','CCc1ccc(C(C)C(Br)OC)cc1 ','CCc1ccc(C(C#N)C(C)Oc2ccccc2)cc1 ','COC(c1ccccc1)C(C#N)(C#N)c1ccccc1 ','COC(=O)c1ccc(C(C)C(OC)Oc2ccccc2)cc1 ','N#CC(Oc1ccccc1)C(c1ccccc1)c1ccc([N+](=O)[O-])cc1 ','N#Cc1ccc(C(C#N)(C#N)C(Oc2ccccc2)c2ccccc2)cc1 ','COCC(C)(C#N)c1ccc([N+](=O)[O-])cc1 ','COC(C#N)C(c1ccccc1)(c1ccccc1)c1ccc(-c2ccccc2)cc1 ','COC(c1ccccc1)C(c1ccccc1)(c1ccccc1)c1ccc(C#N)cc1 ','COC(c1ccccc1)C(C)(C#N)c1ccc(C#N)cc1 ','CC(C)(c1ccccc1)C(C#N)Oc1ccccc1 ','COCOCCc1ccccc1 ','CC(c1ccc([N+](=O)[O-])cc1)C(Oc1ccccc1)c1ccccc1 ','COC(Oc1ccccc1)C(C#N)(c1ccccc1)c1ccc(C#N)cc1 ','COCC(C)(c1ccccc1)c1ccc(C(=O)OC)cc1 ','COCOC(C)C(C)(c1ccccc1)c1ccccc1 ','COCOC(OC)C(C)(C#N)c1ccccc1 ','COC(OC)C(C)(C#N)c1ccc(C#N)cc1 ','CC(C#N)(c1ccc(C#N)cc1)C(C#N)Oc1ccccc1 ','COC(=O)c1ccc(C(C)(C#N)C(OC)c2ccccc2)cc1 ',\n",
    "          'COCOC(c1ccccc1)C(C#N)(C#N)c1ccc(OC)cc1 ','COCOC(OC)C(C)c1ccc(C(=O)OC)cc1 ','CC(COc1ccccc1)(c1ccccc1)c1ccccc1 ','COC(=O)c1ccc(C(C#N)(c2ccccc2)C(C)OC)cc1 ','CCc1ccc(C(C#N)(c2ccccc2)C(OC)Oc2ccccc2)cc1 ','COCOC(C#N)C(C#N)c1ccc([N+](=O)[O-])cc1 ','CCc1ccc(C(C#N)(C#N)C(C)OC)cc1 ','CCc1ccc(C(C#N)(c2ccccc2)C(C#N)Oc2ccccc2)cc1 ','N#Cc1ccc(C(C#N)C(C#N)Oc2ccccc2)cc1 ','COCOCC(C#N)(C#N)c1ccc(C#N)cc1 ','COc1ccc(C(C#N)C(C#N)OC)cc1 ','N#CC(C#N)(COc1ccccc1)c1ccc([N+](=O)[O-])cc1 ','CC(C#N)(COc1ccccc1)c1ccc(C#N)cc1 ','CCc1ccc(C(C)(C)COC)cc1 ','CC(C)(c1ccc(-c2ccccc2)cc1)C(C#N)Oc1ccccc1 ','CCc1ccc(C(C#N)(C#N)C(Oc2ccccc2)c2ccccc2)cc1 ','COCOC(Cc1ccc(C#N)cc1)OC ','COCOC(c1ccccc1)C(c1ccccc1)c1ccccc1 ','COCOC(C#N)C(C)c1ccc(C(=O)OC)cc1 ','N#CC(C#N)(COc1ccccc1)c1ccc(-c2ccccc2)cc1 ','COC(C#N)C(C#N)(C#N)c1ccccc1 ','CCc1ccc(C(COC)c2ccccc2)cc1 ','CCc1ccc(C(C)(C)COCOC)cc1 ','COCOC(c1ccccc1)C(C#N)(C#N)c1ccccc1 ','N#CC(COc1ccccc1)(c1ccccc1)c1ccccc1 ','CCc1ccc(C(C)(C#N)C(Oc2ccccc2)c2ccccc2)cc1 ','CC(C#N)(COc1ccccc1)c1ccc([N+](=O)[O-])cc1 ','COC(=O)c1ccc(C(C#N)(c2ccccc2)C(C#N)Oc2ccccc2)cc1 ','COC(C#N)C(C#N)c1ccc([N+](=O)[O-])cc1 ','COCOC(Br)C(C)(C)c1ccccc1 ','COCOC(OC)C(C)c1ccc([N+](=O)[O-])cc1 ','COC(C#N)C(C#N)(c1ccccc1)c1ccc(C#N)cc1 ','COC(c1ccccc1)C(C#N)(c1ccccc1)c1ccc(C#N)cc1 ','N#CC(c1ccc(-c2ccccc2)cc1)C(Br)Oc1ccccc1 ','COC(=O)c1ccc(C(C#N)(COc2ccccc2)c2ccccc2)cc1 ','COCOC(c1ccccc1)C(C)(c1ccccc1)c1ccc(C(=O)OC)cc1 ','N#Cc1ccc(CC(Oc2ccccc2)c2ccccc2)cc1 ','COC(Br)C(C#N)(c1ccccc1)c1ccc([N+](=O)[O-])cc1 ','CCc1ccc(C(C#N)(c2ccccc2)C(C#N)OC)cc1 ','COC(Oc1ccccc1)C(C#N)(C#N)c1ccc([N+](=O)[O-])cc1 ','CCc1ccc(C(C#N)(c2ccccc2)C(C)OCOC)cc1 ','COC(OC)C(C)c1ccccc1 ','CCc1ccc(C(C)(C#N)C(C)OCOC)cc1 ','COCOC(C)Cc1ccc(C#N)cc1 ','CC(Oc1ccccc1)C(C#N)c1ccc(-c2ccccc2)cc1 ','COC(=O)c1ccc(C(C#N)(C#N)C(OC)c2ccccc2)cc1 ','COC(C#N)C(C)(C#N)c1ccccc1 ','COCOC(OC)C(C#N)c1ccc(C(=O)OC)cc1 ','COCOC(C)C(C)(C#N)c1ccccc1 ','COC(C)C(C#N)(C#N)c1ccc(C#N)cc1 ','COCOC(Br)C(c1ccccc1)c1ccc([N+](=O)[O-])cc1 ','COCCc1ccc([N+](=O)[O-])cc1 ','N#CC(c1ccccc1)(c1ccc([N+](=O)[O-])cc1)C(Br)Oc1ccccc1 ','COCOC(C#N)C(C)c1ccc([N+](=O)[O-])cc1 ','N#CC(C#N)(c1ccc([N+](=O)[O-])cc1)C(Oc1ccccc1)c1ccccc1 ','COCOC(OC)C(C)(C#N)c1ccc(C(=O)OC)cc1 ','COCC(C#N)c1ccc([N+](=O)[O-])cc1 ','COC(=O)c1ccc(C(C#N)(c2ccccc2)C(C)Oc2ccccc2)cc1 ','N#CC(Oc1ccccc1)C(C#N)(C#N)c1ccc([N+](=O)[O-])cc1 ','O=[N+]([O-])c1ccc(C(COc2ccccc2)c2ccccc2)cc1 ','CC(C#N)(c1ccc(C#N)cc1)C(Br)Oc1ccccc1 ','COCOC(C#N)C(c1ccccc1)(c1ccccc1)c1ccccc1 ','N#CC(C#N)(c1ccccc1)C(Br)Oc1ccccc1 ','COC(c1ccccc1)C(C)(c1ccccc1)c1ccc([N+](=O)[O-])cc1 ','COC(c1ccccc1)C(C)(C)c1ccccc1 ','COCOC(OC)C(C#N)(c1ccccc1)c1ccc(C(=O)OC)cc1 ','CC(c1ccccc1)(c1ccccc1)C(C#N)Oc1ccccc1 ','COC(=O)c1ccc(C(c2ccccc2)(c2ccccc2)C(OC)Oc2ccccc2)cc1 ','COCOCC(C#N)(C#N)c1ccc(OC)cc1 ','CCc1ccc(C(C#N)(c2ccccc2)C(OCOC)c2ccccc2)cc1 ','COCOC(C)C(C#N)c1ccc(C(=O)OC)cc1 ','COC(OC)C(c1ccccc1)(c1ccccc1)c1ccc([N+](=O)[O-])cc1 ','COC(OC)C(C#N)(C#N)c1ccc(-c2ccccc2)cc1 ','CCc1ccc(C(C)(c2ccccc2)C(Br)Oc2ccccc2)cc1 ','CCc1ccc(C(C)(c2ccccc2)C(OC)c2ccccc2)cc1 ','COCOC(C#N)C(c1ccccc1)(c1ccccc1)c1ccc([N+](=O)[O-])cc1 ','N#CC(C#N)(c1ccccc1)C(Oc1ccccc1)c1ccccc1 ','CCc1ccc(C(C)COC)cc1 ','N#Cc1ccc(C(c2ccccc2)(c2ccccc2)C(Br)Oc2ccccc2)cc1 ','CC(c1ccccc1)(c1ccc([N+](=O)[O-])cc1)C(C#N)Oc1ccccc1 ','COCOC(C#N)C(C#N)(C#N)c1ccc(-c2ccccc2)cc1 ','CCc1ccc(C(COC)(c2ccccc2)c2ccccc2)cc1 ','COCOC(C#N)C(C#N)c1ccccc1 ','CCc1ccc(C(C#N)(C#N)C(C#N)OC)cc1 ','COC(c1ccccc1)C(C)(C#N)c1ccccc1 ','CC(C#N)(c1ccc(-c2ccccc2)cc1)C(Br)Oc1ccccc1 ','COCOC(C)Cc1ccc(C(=O)OC)cc1 ','N#Cc1ccc(CCOc2ccccc2)cc1 ','COC(C#N)Cc1ccc(C#N)cc1 ','COC(C#N)C(C)(C#N)c1ccc(-c2ccccc2)cc1 ','CCc1ccc(C(COc2ccccc2)(c2ccccc2)c2ccccc2)cc1 ','CCc1ccc(C(c2ccccc2)C(OCOC)c2ccccc2)cc1 ','CCc1ccc(C(C)(C#N)C(OC)Oc2ccccc2)cc1 ','COc1ccc(C(C#N)(C#N)C(Oc2ccccc2)c2ccccc2)cc1 ','COCOC(c1ccccc1)C(C)(c1ccccc1)c1ccc(C#N)cc1 ','COCOC(c1ccccc1)C(C)(C#N)c1ccc(C(=O)OC)cc1 ','CCc1ccc(C(C)C(Br)OCOC)cc1 ','COC(C)C(c1ccccc1)(c1ccccc1)c1ccc([N+](=O)[O-])cc1 ','COC(Br)C(C)(C#N)c1ccccc1 ','COCC(c1ccccc1)c1ccccc1 ','COCOC(Br)C(C)(c1ccccc1)c1ccc(C(=O)OC)cc1 ','CCc1ccc(C(C#N)(C#N)C(Br)OC)cc1 ','c1ccc(CC(Oc2ccccc2)c2ccccc2)cc1 ','N#CC(c1ccccc1)(c1ccccc1)C(Oc1ccccc1)c1ccccc1 ','CC(COc1ccccc1)c1ccc(C#N)cc1 ','COCOC(c1ccccc1)C(C)(C#N)c1ccc(C#N)cc1 ','CC(c1ccccc1)(c1ccc([N+](=O)[O-])cc1)C(Oc1ccccc1)c1ccccc1 ','COCOC(c1ccccc1)C(c1ccccc1)(c1ccccc1)c1ccccc1 ','N#CC(Oc1ccccc1)C(C#N)(c1ccccc1)c1ccc(-c2ccccc2)cc1 ',\n",
    "          'CC(C)(c1ccc([N+](=O)[O-])cc1)C(C#N)Oc1ccccc1 ','CCc1ccc(C(COc2ccccc2)c2ccccc2)cc1 ','CCc1ccc(C(C#N)(c2ccccc2)C(OC)OCOC)cc1 ','CC(C#N)(c1ccc(-c2ccccc2)cc1)C(C#N)Oc1ccccc1 ','COC(C)C(c1ccccc1)c1ccc([N+](=O)[O-])cc1 ','N#Cc1ccc(C(C#N)(C#N)C(C#N)Oc2ccccc2)cc1 ','COc1ccc(C(C#N)C(C)OC)cc1 ','CC(Oc1ccccc1)C(C#N)(C#N)c1ccc(-c2ccccc2)cc1 ','CCc1ccc(CC(OC)c2ccccc2)cc1 ','COC(C#N)C(C)c1ccc(C#N)cc1 ','COCOC(C)C(c1ccccc1)c1ccc(C#N)cc1 ','COCOC(C)C(C)(C#N)c1ccc(OC)cc1 ','N#CC(Cc1ccccc1)Oc1ccccc1 ','COCOC(C)C(C)(C#N)c1ccc(C#N)cc1 ','COC(C)C(c1ccccc1)c1ccccc1 ','COCOC(c1ccccc1)C(C#N)(c1ccccc1)c1ccccc1 ','COC(C#N)C(C)(C)c1ccc([N+](=O)[O-])cc1 ','COCOC(C#N)C(C)(C)c1ccc(-c2ccccc2)cc1 ','COCC(c1ccccc1)c1ccc(C(=O)OC)cc1 ','COC(Oc1ccccc1)C(C)(C#N)c1ccccc1 ','COCC(c1ccccc1)(c1ccccc1)c1ccc([N+](=O)[O-])cc1 ','COC(Br)C(C)(C)c1ccccc1 ','COCOCCc1ccc([N+](=O)[O-])cc1 ','COCOC(OC)C(C#N)(c1ccccc1)c1ccc(-c2ccccc2)cc1 ','CC(C)(COc1ccccc1)c1ccc(C#N)cc1 ','COCOC(C#N)C(C)c1ccc(-c2ccccc2)cc1 ','COC(c1ccccc1)C(C)(C)c1ccc(C#N)cc1 ','COC(=O)c1ccc(C(C)C(OC)c2ccccc2)cc1 ','COC(c1ccccc1)C(C)(c1ccccc1)c1ccccc1 ','COC(c1ccccc1)C(C)(C#N)c1ccc([N+](=O)[O-])cc1 ','COC(Br)C(C#N)(C#N)c1ccc(C#N)cc1 ','CCc1ccc(C(c2ccccc2)C(OC)OC)cc1 ','CCc1ccc(C(C)(C)C(C)OC)cc1 ','COCOC(c1ccccc1)C(C#N)(C#N)c1ccc(C#N)cc1 ','COC(C#N)Cc1ccccc1 ','COCC(C)c1ccc(C(=O)OC)cc1 ','COc1ccc(C(C)(C#N)C(OC)Oc2ccccc2)cc1 ','COCOC(Br)C(C#N)c1ccc(C(=O)OC)cc1 ','COCOCC(C#N)c1ccc(-c2ccccc2)cc1 ','CC(c1ccccc1)(c1ccccc1)C(Oc1ccccc1)c1ccccc1 ','CCc1ccc(C(C#N)(c2ccccc2)C(Oc2ccccc2)c2ccccc2)cc1 ','COCOC(OC)C(C)c1ccccc1 ','COC(=O)c1ccc(C(C)(C)C(C)OC)cc1 ','COCOC(c1ccccc1)C(C#N)(c1ccccc1)c1ccc(C(=O)OC)cc1 ','CCc1ccc(C(C)(COc2ccccc2)c2ccccc2)cc1 ','CCc1ccc(C(C#N)C(OC)c2ccccc2)cc1 ','CC(c1ccccc1)C(C#N)Oc1ccccc1 ','COC(=O)c1ccc(C(C)(C)C(Br)OC)cc1 ','COC(=O)c1ccc(C(C)(C)C(Oc2ccccc2)c2ccccc2)cc1 ','COC(Oc1ccccc1)C(C)(C#N)c1ccc(-c2ccccc2)cc1 ','N#Cc1ccc(C(C#N)(C#N)C(Br)Oc2ccccc2)cc1 ','COCOC(C#N)C(C#N)(C#N)c1ccc([N+](=O)[O-])cc1 ','CC(C#N)(c1ccc([N+](=O)[O-])cc1)C(C#N)Oc1ccccc1 ','COCOCC(C)(c1ccccc1)c1ccccc1 ','COc1ccc(C(C#N)(c2ccccc2)C(Br)OC)cc1 ','COC(Br)C(C)c1ccc([N+](=O)[O-])cc1 ','COCOC(C#N)C(C)(c1ccccc1)c1ccc([N+](=O)[O-])cc1 ','COCOCC(C)(C#N)c1ccc(OC)cc1 ','CCc1ccc(C(C#N)C(OCOC)c2ccccc2)cc1 ','COCOC(Br)C(C)(C)c1ccc([N+](=O)[O-])cc1 ','COCOC(c1ccccc1)C(C#N)(c1ccccc1)c1ccc(OC)cc1 ','COCC(c1ccccc1)c1ccc([N+](=O)[O-])cc1 ','N#Cc1ccc(CC(C#N)Oc2ccccc2)cc1 ','COCOC(c1ccccc1)C(C#N)(C#N)c1ccc([N+](=O)[O-])cc1 ','CCc1ccc(C(c2ccccc2)C(C)OC)cc1 ','COc1ccc(C(C#N)(c2ccccc2)C(OC)Oc2ccccc2)cc1 ','COC(C#N)C(C#N)(C#N)c1ccc(-c2ccccc2)cc1 ','CC(Oc1ccccc1)C(c1ccccc1)(c1ccccc1)c1ccc([N+](=O)[O-])cc1 ','CCc1ccc(C(C#N)(c2ccccc2)C(C)OC)cc1 ','COC(=O)c1ccc(C(c2ccccc2)C(Br)OC)cc1 ','COCOC(OC)C(C)(C)c1ccc(C(=O)OC)cc1 ','CCc1ccc(C(C#N)(C#N)C(OC)c2ccccc2)cc1 ','COCOC(Br)C(C)(c1ccccc1)c1ccc([N+](=O)[O-])cc1 ','COC(=O)c1ccc(C(C#N)(C#N)C(C#N)OC)cc1 ','COCOC(OC)C(C)(C#N)c1ccc(C#N)cc1 ','COC(Oc1ccccc1)C(c1ccccc1)c1ccc([N+](=O)[O-])cc1 ','COC(Br)C(C#N)c1ccc(C#N)cc1 ','COC(=O)c1ccc(C(C#N)C(OC)OC)cc1 ','COCOC(C)C(C#N)(c1ccccc1)c1ccc([N+](=O)[O-])cc1 ','COCOC(c1ccccc1)C(c1ccccc1)c1ccc(C#N)cc1 ','COc1ccc(C(C#N)C(Br)Oc2ccccc2)cc1 ','COCC(C#N)(c1ccccc1)c1ccc(C#N)cc1 ','COCOC(C#N)C(C#N)c1ccc(-c2ccccc2)cc1 ','COCOC(c1ccccc1)C(C)(c1ccccc1)c1ccc([N+](=O)[O-])cc1 ','COC(=O)c1ccc(C(c2ccccc2)(c2ccccc2)C(OC)OC)cc1 ','CC(c1ccc(-c2ccccc2)cc1)C(C#N)Oc1ccccc1 ','COC(=O)c1ccc(C(C)(C#N)C(Oc2ccccc2)c2ccccc2)cc1 ','CCc1ccc(C(C#N)(C#N)C(OC)Oc2ccccc2)cc1 ','COC(=O)c1ccc(C(c2ccccc2)(c2ccccc2)C(C)OC)cc1 ','COCOC(c1ccccc1)C(C#N)c1ccc([N+](=O)[O-])cc1 ','CC(c1ccc(C#N)cc1)C(Br)Oc1ccccc1 ','COCOC(C#N)C(C#N)(c1ccccc1)c1ccc([N+](=O)[O-])cc1 ','N#Cc1ccc(C(C#N)(c2ccccc2)C(Oc2ccccc2)c2ccccc2)cc1 ','CCc1ccc(CC(C)OCOC)cc1 ','COC(=O)c1ccc(C(C#N)(c2ccccc2)C(Br)Oc2ccccc2)cc1 ','COC(=O)c1ccc(C(C#N)(C#N)C(C)OC)cc1 ','O=[N+]([O-])c1ccc(CC(Br)Oc2ccccc2)cc1 ','COC(=O)c1ccc(CC(C#N)Oc2ccccc2)cc1 ','COCOC(OC)C(c1ccccc1)(c1ccccc1)c1ccc(C(=O)OC)cc1 ','COCC(C)(C)c1ccccc1 ','COC(c1ccccc1)C(C#N)(c1ccccc1)c1ccccc1 ','COCC(C)c1ccc([N+](=O)[O-])cc1 ','COCOC(C#N)C(c1ccccc1)c1ccccc1 ','CCc1ccc(C(C#N)COCOC)cc1 ','COC(C#N)C(C#N)(c1ccccc1)c1ccc([N+](=O)[O-])cc1 ','CCc1ccc(C(C#N)(C#N)C(C#N)Oc2ccccc2)cc1 ','COCOC(Br)Cc1ccc(C(=O)OC)cc1 ','COCC(C#N)c1ccc(-c2ccccc2)cc1 ','COCOCC(C#N)c1ccc(C(=O)OC)cc1 ','CCc1ccc(C(C#N)(c2ccccc2)C(OC)c2ccccc2)cc1 ','COCCc1ccc(C#N)cc1 ','COC(c1ccccc1)C(C#N)(c1ccccc1)c1ccc(-c2ccccc2)cc1 ',\n",
    "\t\t  'COc1ccc(C(C)(C#N)C(OC)c2ccccc2)cc1 ','COCOC(c1ccccc1)C(C)c1ccc([N+](=O)[O-])cc1 ','CCc1ccc(C(c2ccccc2)(c2ccccc2)C(OC)OCOC)cc1 ','COC(Oc1ccccc1)C(C)(c1ccccc1)c1ccccc1 ','COCOCC(C)(C#N)c1ccc([N+](=O)[O-])cc1 ','COC(OC)C(C)(c1ccccc1)c1ccccc1 ','CCc1ccc(CC(Br)OCOC)cc1 ','COC(OC)C(C)(c1ccccc1)c1ccc(C#N)cc1 ','COC(Br)C(C)(c1ccccc1)c1ccc(C#N)cc1 ','COC(=O)c1ccc(C(C)(C#N)C(C)OC)cc1 ','COC(c1ccccc1)C(C)c1ccc([N+](=O)[O-])cc1 ','CC(Oc1ccccc1)C(C#N)(c1ccccc1)c1ccc([N+](=O)[O-])cc1 ','COC(Br)Cc1ccc(C#N)cc1 ','COC(=O)c1ccc(C(COc2ccccc2)(c2ccccc2)c2ccccc2)cc1 ','COC(Br)C(C#N)c1ccc(-c2ccccc2)cc1 ','CCc1ccc(C(C#N)(C#N)C(Br)Oc2ccccc2)cc1 ','CCc1ccc(C(C)(C#N)C(C#N)Oc2ccccc2)cc1 ','COC(OC)C(c1ccccc1)c1ccc(C#N)cc1 ','COCOC(Br)C(C#N)(C#N)c1ccc(-c2ccccc2)cc1 ','CCc1ccc(C(c2ccccc2)C(Br)OC)cc1 ','COc1ccc(C(C#N)(c2ccccc2)C(C)OC)cc1 ','CCc1ccc(C(c2ccccc2)(c2ccccc2)C(C)OC)cc1 ','CCc1ccc(C(C)(C#N)C(OC)c2ccccc2)cc1 ','COC(OC)C(C#N)(C#N)c1ccc(C#N)cc1 ','COCOC(Br)Cc1ccc([N+](=O)[O-])cc1 ','CCc1ccc(C(c2ccccc2)C(Br)Oc2ccccc2)cc1 ','COC(=O)c1ccc(C(c2ccccc2)C(OC)c2ccccc2)cc1 ','COCOC(c1ccccc1)C(C)(c1ccccc1)c1ccccc1 ','CCc1ccc(C(c2ccccc2)(c2ccccc2)C(Oc2ccccc2)c2ccccc2)cc1 ','CCc1ccc(C(C#N)(COCOC)c2ccccc2)cc1 ',\n",
    "          'COCC(C)(C)c1ccc(C(=O)OC)cc1 ','COCOC(c1ccccc1)C(C)(C)c1ccc(C#N)cc1 ','CCc1ccc(C(C)C(Br)Oc2ccccc2)cc1 ','COC(=O)c1ccc(C(C#N)C(C)Oc2ccccc2)cc1 ','CC(c1ccccc1)(c1ccc([N+](=O)[O-])cc1)C(Br)Oc1ccccc1 ','COC(Br)C(C#N)(C#N)c1ccc([N+](=O)[O-])cc1 ','COc1ccc(C(C#N)C(OC)OC)cc1 ','COC(=O)c1ccc(CC(Oc2ccccc2)c2ccccc2)cc1 ','CC(Oc1ccccc1)C(C#N)(C#N)c1ccc(C#N)cc1 ','BrC(Oc1ccccc1)C(c1ccccc1)c1ccc(-c2ccccc2)cc1 ','CC(C#N)(c1ccccc1)C(Br)Oc1ccccc1 ','COCOC(OC)C(C#N)(C#N)c1ccccc1 ','COC(C)C(C#N)(c1ccccc1)c1ccc([N+](=O)[O-])cc1 ','N#CC(c1ccc([N+](=O)[O-])cc1)C(Oc1ccccc1)c1ccccc1 ','COC(=O)c1ccc(C(C)(C#N)COc2ccccc2)cc1 ','COC(Oc1ccccc1)C(C#N)c1ccc(C#N)cc1 ','CCc1ccc(C(C)(C)C(Br)Oc2ccccc2)cc1 ','COC(=O)c1ccc(C(C)(C#N)C(C#N)Oc2ccccc2)cc1 ','COCC(C)(C)c1ccc([N+](=O)[O-])cc1 ','COc1ccc(C(C)(C#N)C(C#N)OC)cc1 ','COc1ccc(C(C#N)(C#N)C(OC)OC)cc1 ','COC(=O)c1ccc(C(C)(c2ccccc2)C(OC)Oc2ccccc2)cc1 ','COCOC(C#N)Cc1ccccc1 ','COCCc1ccccc1 ','COCOC(OC)C(C)(C#N)c1ccc([N+](=O)[O-])cc1 ','COC(Oc1ccccc1)C(C)(C#N)c1ccc(C#N)cc1 ','COCOC(Br)C(c1ccccc1)(c1ccccc1)c1ccc(-c2ccccc2)cc1 ','COCOC(c1ccccc1)C(C)(C#N)c1ccc(-c2ccccc2)cc1 ','BrC(Cc1ccc(-c2ccccc2)cc1)Oc1ccccc1 ','CCc1ccc(CC(OC)OCOC)cc1 ','CCc1ccc(C(c2ccccc2)(c2ccccc2)C(C)OCOC)cc1 ','COC(OC)C(C#N)c1ccc(C#N)cc1 ','CCc1ccc(C(C)(c2ccccc2)C(C#N)OC)cc1 ','O=[N+]([O-])c1ccc(C(c2ccccc2)C(Oc2ccccc2)c2ccccc2)cc1 ','CCc1ccc(C(C#N)(C#N)COC)cc1 ','COC(=O)c1ccc(C(c2ccccc2)C(C)OC)cc1 ','COCOC(c1ccccc1)C(C#N)(c1ccccc1)c1ccc(C#N)cc1 ','CC(Oc1ccccc1)C(C)(c1ccccc1)c1ccc([N+](=O)[O-])cc1 ','N#Cc1ccc(C(COc2ccccc2)(c2ccccc2)c2ccccc2)cc1 ','COCOCC(C)(C#N)c1ccc(C#N)cc1 ','COCOCC(C#N)c1ccc(C#N)cc1 ','CCc1ccc(C(c2ccccc2)(c2ccccc2)C(Br)OCOC)cc1 ','COC(=O)c1ccc(C(C)(COc2ccccc2)c2ccccc2)cc1 ','COCOC(C#N)C(C#N)(c1ccccc1)c1ccc(OC)cc1 ','COC(Br)C(c1ccccc1)(c1ccccc1)c1ccc([N+](=O)[O-])cc1 ','CCc1ccc(C(C)(c2ccccc2)C(OC)OCOC)cc1 ','COCC(C#N)(c1ccccc1)c1ccc(OC)cc1 ','COCOCC(C#N)(c1ccccc1)c1ccc(C(=O)OC)cc1 ','COCOC(Br)C(C#N)(c1ccccc1)c1ccc(C(=O)OC)cc1 ','CCc1ccc(C(COCOC)(c2ccccc2)c2ccccc2)cc1 ','COCOC(Br)C(C#N)(C#N)c1ccc(C(=O)OC)cc1 ','COC(OC)C(C#N)(C#N)c1ccccc1 ','COCOC(C#N)C(C)(C)c1ccccc1 ','COC(C#N)C(C)(c1ccccc1)c1ccc([N+](=O)[O-])cc1 ','COC(C)Cc1ccc([N+](=O)[O-])cc1 ','COCOC(C)C(C#N)(c1ccccc1)c1ccc(-c2ccccc2)cc1 ','COC(C#N)Cc1ccc(-c2ccccc2)cc1 ','COc1ccc(C(C#N)(C#N)C(C)Oc2ccccc2)cc1 ','COC(=O)c1ccc(C(C)(C)COc2ccccc2)cc1 ','N#CC(c1ccccc1)(c1ccccc1)C(Br)Oc1ccccc1 ','COCC(C#N)(c1ccccc1)c1ccc(-c2ccccc2)cc1 ','COCOC(C#N)C(C#N)(c1ccccc1)c1ccc(-c2ccccc2)cc1 ','CC(C#N)(COc1ccccc1)c1ccccc1 ','CCc1ccc(C(C#N)(C#N)C(C)Oc2ccccc2)cc1 ','COC(OC)C(C)(c1ccccc1)c1ccc([N+](=O)[O-])cc1 ','COc1ccc(C(C#N)(C#N)C(C#N)Oc2ccccc2)cc1 ','COCOC(Br)C(C#N)c1ccc(OC)cc1 ','COCOC(C#N)C(C#N)(C#N)c1ccc(OC)cc1 ','COCC(C#N)(C#N)c1ccc(-c2ccccc2)cc1 ','COC(Oc1ccccc1)C(C#N)c1ccc([N+](=O)[O-])cc1 ','COCOC(C#N)C(C#N)c1ccc(C#N)cc1 ','CCc1ccc(C(C#N)(c2ccccc2)C(Br)OC)cc1 ','N#CC(Oc1ccccc1)C(C#N)c1ccccc1 ','CCc1ccc(C(C)(C#N)C(Br)OCOC)cc1 ','COC(c1ccccc1)C(c1ccccc1)(c1ccccc1)c1ccccc1 ','COCC(C#N)c1ccc(OC)cc1 ','COC(=O)c1ccc(C(C)COc2ccccc2)cc1 ','COC(C#N)C(C#N)(c1ccccc1)c1ccccc1 ','COCC(C#N)c1ccc(C(=O)OC)cc1 ','COCOC(OC)C(C)(c1ccccc1)c1ccccc1 ','c1ccc(OC(c2ccccc2)C(c2ccccc2)(c2ccccc2)c2ccccc2)cc1 ','CCc1ccc(C(C)(C)C(OC)c2ccccc2)cc1 ','COC(C)C(C)(C)c1ccccc1 ','CCc1ccc(C(C#N)C(Br)Oc2ccccc2)cc1 ','COCOC(Br)C(C#N)(C#N)c1ccc(OC)cc1 ','COC(Cc1ccccc1)Oc1ccccc1 ','COc1ccc(C(C#N)(COc2ccccc2)c2ccccc2)cc1 ','COC(c1ccccc1)C(C#N)(c1ccccc1)c1ccc([N+](=O)[O-])cc1 ','COC(Br)C(C#N)(c1ccccc1)c1ccccc1 ','CC(Oc1ccccc1)C(C)(C#N)c1ccccc1 ','COCOC(Br)C(C#N)c1ccccc1 ','COCOC(Br)C(c1ccccc1)c1ccc(C(=O)OC)cc1 ','COCOC(C)C(C)(C)c1ccccc1 ','COCOC(C)C(C#N)c1ccc(C#N)cc1 ','COC(Oc1ccccc1)C(c1ccccc1)(c1ccccc1)c1ccc(C#N)cc1 ','COCOCC(c1ccccc1)(c1ccccc1)c1ccc(C(=O)OC)cc1 ','COCOC(c1ccccc1)C(C)(C#N)c1ccc([N+](=O)[O-])cc1 ','COCOC(c1ccccc1)C(C)c1ccccc1 ','COCOC(OC)C(c1ccccc1)(c1ccccc1)c1ccc([N+](=O)[O-])cc1 ','COCOCC(C)c1ccc(C#N)cc1 ','COCOCCc1ccc(C#N)cc1 ','N#CC(COc1ccccc1)c1ccc([N+](=O)[O-])cc1 ','COCOC(Br)C(C#N)c1ccc(-c2ccccc2)cc1 ','COC(=O)c1ccc(C(C#N)COc2ccccc2)cc1 ','CC(C#N)(c1ccc([N+](=O)[O-])cc1)C(Br)Oc1ccccc1 ','COC(Br)C(c1ccccc1)c1ccc(C#N)cc1 ','COCOC(C)C(C#N)(c1ccccc1)c1ccc(C#N)cc1 ','COC(=O)c1ccc(C(C#N)C(OC)c2ccccc2)cc1 ','CC(Oc1ccccc1)C(c1ccccc1)(c1ccccc1)c1ccccc1 ','COC(Cc1ccccc1)c1ccccc1 ','COC(=O)c1ccc(C(C#N)(C#N)COc2ccccc2)cc1 ','COCOC(C#N)Cc1ccc(C#N)cc1 ','COC(OC)C(C#N)(c1ccccc1)c1ccc(C#N)cc1 ',\n",
    "          'N#CC(C#N)(c1ccc(-c2ccccc2)cc1)C(Br)Oc1ccccc1 ','COC(=O)c1ccc(C(c2ccccc2)(c2ccccc2)C(Br)OC)cc1 ','COC(c1ccccc1)C(C)(C)c1ccc([N+](=O)[O-])cc1 ','COCC(C#N)c1ccccc1 ','COc1ccc(CC(C#N)OC)cc1 ','CC(COc1ccccc1)(c1ccccc1)c1ccc(C#N)cc1 ','COCOC(OC)C(C)(c1ccccc1)c1ccc(C(=O)OC)cc1 ','N#CC(Oc1ccccc1)C(C#N)(C#N)c1ccc(-c2ccccc2)cc1 ','COCOCC(C)(C#N)c1ccccc1 ','COC(C)C(C)c1ccc([N+](=O)[O-])cc1 ','N#Cc1ccc(C(C#N)COc2ccccc2)cc1 ','CCc1ccc(C(c2ccccc2)C(C)OCOC)cc1 ','CCc1ccc(CC(Br)OC)cc1 ','COCOC(Br)C(c1ccccc1)(c1ccccc1)c1ccc([N+](=O)[O-])cc1 ','COCOC(Br)C(C#N)(c1ccccc1)c1ccc(OC)cc1 ','COc1ccc(C(C#N)(c2ccccc2)C(C#N)Oc2ccccc2)cc1 ','COC(=O)c1ccc(CC(OC)OC)cc1 ','N#CC(c1ccccc1)(c1ccc(-c2ccccc2)cc1)C(Oc1ccccc1)c1ccccc1 ','CC(c1ccccc1)(c1ccccc1)C(Br)Oc1ccccc1 ','COCOC(OC)C(C#N)c1ccc(OC)cc1 ','COCOC(C#N)Cc1ccc([N+](=O)[O-])cc1 ','COC(=O)c1ccc(C(c2ccccc2)(c2ccccc2)C(OC)c2ccccc2)cc1 ','COC(C)C(c1ccccc1)(c1ccccc1)c1ccccc1 ','COC(OC)C(C#N)(c1ccccc1)c1ccc([N+](=O)[O-])cc1 ','CCc1ccc(C(C#N)C(C#N)OC)cc1 ','COC(Oc1ccccc1)C(C#N)c1ccc(-c2ccccc2)cc1 ','COCOC(C)C(C)(c1ccccc1)c1ccc(C#N)cc1 ','COCOC(C#N)C(c1ccccc1)c1ccc(C#N)cc1 ','COCOC(c1ccccc1)C(C#N)c1ccc(OC)cc1 ','COCOC(c1ccccc1)C(c1ccccc1)(c1ccccc1)c1ccc(C(=O)OC)cc1 ','COCOC(C)C(c1ccccc1)(c1ccccc1)c1ccc(C(=O)OC)cc1 ','N#CC(c1ccccc1)C(Oc1ccccc1)c1ccccc1 ','CCc1ccc(C(c2ccccc2)(c2ccccc2)C(Br)OC)cc1 ','COCOC(c1ccccc1)C(C#N)c1ccc(C(=O)OC)cc1 ','COC(Oc1ccccc1)C(C)c1ccc([N+](=O)[O-])cc1 ','N#Cc1ccc(C(C#N)(C#N)COc2ccccc2)cc1 ','CC(c1ccc(C#N)cc1)C(C#N)Oc1ccccc1 ','COC(C#N)C(C#N)(C#N)c1ccc([N+](=O)[O-])cc1 ','COCOCC(c1ccccc1)c1ccc(C(=O)OC)cc1 ','COCOC(OC)C(c1ccccc1)c1ccc(C#N)cc1 ','CCc1ccc(C(C#N)(COc2ccccc2)c2ccccc2)cc1 ','CC(C)(c1ccc(-c2ccccc2)cc1)C(Br)Oc1ccccc1 ','COCOC(C)C(C)(C#N)c1ccc([N+](=O)[O-])cc1 ','CCc1ccc(C(C#N)(C#N)C(OC)OCOC)cc1 ','COCOC(Br)C(C)(C#N)c1ccc(C(=O)OC)cc1 ','COCOC(OC)C(C)(C)c1ccccc1 ','CCc1ccc(C(C)(C#N)C(Br)OC)cc1 ','CCc1ccc(C(C)(c2ccccc2)C(OC)Oc2ccccc2)cc1 ','COC(=O)c1ccc(C(C#N)(c2ccccc2)C(OC)Oc2ccccc2)cc1 ','COCOC(OC)C(c1ccccc1)(c1ccccc1)c1ccccc1 ','COCOCC(C#N)(c1ccccc1)c1ccc(OC)cc1 ','COc1ccc(C(C#N)C(OC)Oc2ccccc2)cc1 ','COC(Oc1ccccc1)C(C)(c1ccccc1)c1ccc(C#N)cc1 ','CCc1ccc(C(C#N)C(C#N)Oc2ccccc2)cc1 ','COC(=O)c1ccc(C(C)(c2ccccc2)C(Oc2ccccc2)c2ccccc2)cc1 ','CC(C)(c1ccc([N+](=O)[O-])cc1)C(Br)Oc1ccccc1 ','COC(c1ccccc1)C(C#N)(C#N)c1ccc(C#N)cc1 ','COCOC(C)C(C#N)(c1ccccc1)c1ccc(C(=O)OC)cc1 ','COCOC(C)C(C#N)c1ccc(OC)cc1 ','CCc1ccc(CC(Br)Oc2ccccc2)cc1 ','N#CC(c1ccc([N+](=O)[O-])cc1)C(Br)Oc1ccccc1 ','COC(=O)c1ccc(C(C)(C#N)C(OC)Oc2ccccc2)cc1 ','COCOC(C#N)C(C)c1ccc(C#N)cc1 ','CCc1ccc(C(C#N)C(C)OC)cc1 ','COc1ccc(C(C#N)(c2ccccc2)C(Oc2ccccc2)c2ccccc2)cc1 ','COCOC(C#N)C(c1ccccc1)c1ccc(C(=O)OC)cc1 ','N#CC(Oc1ccccc1)C(C#N)c1ccc([N+](=O)[O-])cc1 ','CC(C#N)(c1ccccc1)C(C#N)Oc1ccccc1 ','COCOC(Br)C(C)c1ccccc1 ','COCOC(C)C(c1ccccc1)c1ccc([N+](=O)[O-])cc1 ','CC(C#N)(c1ccc(C#N)cc1)C(Oc1ccccc1)c1ccccc1 ','CCc1ccc(C(C)(C)C(OC)OC)cc1 ','COC(Br)C(C)(c1ccccc1)c1ccc([N+](=O)[O-])cc1 ','COC(C#N)C(C)(C)c1ccc(C#N)cc1 ','COCOC(Cc1ccc(C#N)cc1)c1ccccc1 ','O=[N+]([O-])c1ccc(C(c2ccccc2)(c2ccccc2)C(Br)Oc2ccccc2)cc1 ','COc1ccc(C(C)(C#N)C(Br)OC)cc1 ','COCOC(OC)C(C#N)c1ccc([N+](=O)[O-])cc1 ','CC(COc1ccccc1)(c1ccccc1)c1ccc([N+](=O)[O-])cc1 ','N#CC(COc1ccccc1)(c1ccccc1)c1ccc(-c2ccccc2)cc1 ','COCOC(OC)C(C#N)c1ccc(C#N)cc1 ','COC(=O)c1ccc(C(C#N)(C#N)C(C#N)Oc2ccccc2)cc1 ','COCOC(c1ccccc1)C(C#N)(C#N)c1ccc(C(=O)OC)cc1 ','CC(c1ccc([N+](=O)[O-])cc1)C(Br)Oc1ccccc1 ','COCOC(C#N)C(C)(C)c1ccc([N+](=O)[O-])cc1 ','COCOC(Br)C(C)(c1ccccc1)c1ccc(C#N)cc1 ','COCC(C)(C#N)c1ccc(C(=O)OC)cc1 ','CC(Oc1ccccc1)C(C#N)c1ccc([N+](=O)[O-])cc1 ','COCOC(C)C(C)c1ccc(C#N)cc1 ','COCOC(Br)Cc1ccccc1 ','COCOC(Br)C(C)c1ccc(C#N)cc1 ','COC(C#N)C(c1ccccc1)(c1ccccc1)c1ccc(C#N)cc1 ','COCOC(C#N)C(c1ccccc1)c1ccc([N+](=O)[O-])cc1 ','COC(=O)c1ccc(C(c2ccccc2)C(Br)Oc2ccccc2)cc1 ','COCOCC(c1ccccc1)(c1ccccc1)c1ccc(C#N)cc1 ','CC(c1ccccc1)(c1ccc(C#N)cc1)C(Oc1ccccc1)c1ccccc1 ','COC(C#N)C(C)(c1ccccc1)c1ccccc1 ','COC(=O)c1ccc(C(C)(C)C(OC)c2ccccc2)cc1 ','COC(=O)c1ccc(C(C#N)C(C#N)Oc2ccccc2)cc1 ','CCc1ccc(CC(OC)OC)cc1 ','COCOC(C#N)C(C)(c1ccccc1)c1ccc(C(=O)OC)cc1 ','CC(C#N)(c1ccc([N+](=O)[O-])cc1)C(Oc1ccccc1)c1ccccc1 ','COC(C#N)C(c1ccccc1)c1ccccc1 ','CCc1ccc(C(C)(c2ccccc2)C(C)OCOC)cc1 ','N#CC(Oc1ccccc1)C(c1ccccc1)(c1ccccc1)c1ccccc1 ','CC(Oc1ccccc1)C(c1ccccc1)(c1ccccc1)c1ccc(C#N)cc1 ','COC(c1ccccc1)C(C)(c1ccccc1)c1ccc(C#N)cc1 ','COC(=O)c1ccc(C(C#N)(c2ccccc2)C(Oc2ccccc2)c2ccccc2)cc1 ',\n",
    "\t\t  'CCc1ccc(C(C)(C#N)C(OC)OC)cc1 ','COC(OC)C(C)c1ccc(C#N)cc1 ','CCc1ccc(C(C)(c2ccccc2)C(C)OC)cc1 ','COCOC(C#N)Cc1ccc(C(=O)OC)cc1 ','COc1ccc(C(C#N)(c2ccccc2)C(Br)Oc2ccccc2)cc1 ','CC(c1ccccc1)(c1ccc(-c2ccccc2)cc1)C(Br)Oc1ccccc1 ','COc1ccc(C(C)(C#N)C(C#N)Oc2ccccc2)cc1 ','CC(C)(c1ccc([N+](=O)[O-])cc1)C(Oc1ccccc1)c1ccccc1 ','N#Cc1ccc(C(C#N)C(Oc2ccccc2)c2ccccc2)cc1 ','COCOC(Br)C(c1ccccc1)(c1ccccc1)c1ccc(C(=O)OC)cc1 ','COC(C#N)C(C)(C#N)c1ccc([N+](=O)[O-])cc1 ','COC(Br)C(C)(C)c1ccc(-c2ccccc2)cc1 ','COc1ccc(C(C#N)(C#N)C(OC)Oc2ccccc2)cc1 ','CCc1ccc(C(C)C(C)OCOC)cc1 ','COC(C#N)C(C)(C#N)c1ccc(C#N)cc1 ','COCOC(C)C(C)(C)c1ccc(C#N)cc1 ','CCc1ccc(C(C#N)(c2ccccc2)C(OC)OC)cc1 ','CC(Oc1ccccc1)C(C#N)(c1ccccc1)c1ccc(C#N)cc1 ','COCOC(Br)C(C)c1ccc(-c2ccccc2)cc1 ','COC(Br)C(C)(C#N)c1ccc(C#N)cc1 ','N#CC(Oc1ccccc1)C(C#N)(C#N)c1ccccc1 ','CCc1ccc(CC(C#N)OCOC)cc1 ','COC(Oc1ccccc1)C(C#N)(C#N)c1ccc(C#N)cc1 ','COCOCC(C#N)(c1ccccc1)c1ccc(-c2ccccc2)cc1 ','COC(=O)c1ccc(C(C#N)C(OC)Oc2ccccc2)cc1 ','COCOCC(c1ccccc1)(c1ccccc1)c1ccc([N+](=O)[O-])cc1 ','COCOC(C)C(C#N)c1ccccc1 ','COC(C)Cc1ccc(C#N)cc1 ','N#Cc1ccc(CC(Br)Oc2ccccc2)cc1 ','N#CC(COc1ccccc1)c1ccccc1 ','COCOC(C#N)C(c1ccccc1)(c1ccccc1)c1ccc(C(=O)OC)cc1 ','CC(C#N)(COc1ccccc1)c1ccc(-c2ccccc2)cc1 ',\n",
    "          'CC(c1ccc(-c2ccccc2)cc1)C(Br)Oc1ccccc1 ','COCOC(C#N)C(C#N)(c1ccccc1)c1ccc(C(=O)OC)cc1 ','COCOCC(c1ccccc1)c1ccc([N+](=O)[O-])cc1 ','COC(C#N)C(C)(c1ccccc1)c1ccc(C#N)cc1 ','COC(Cc1ccc(C#N)cc1)OC ','COCOC(c1ccccc1)C(C#N)c1ccc(-c2ccccc2)cc1 ','COCOC(c1ccccc1)C(C#N)c1ccccc1 ','N#CC(Cc1ccc([N+](=O)[O-])cc1)Oc1ccccc1 ','COCOCC(C#N)(C#N)c1ccc(C(=O)OC)cc1 ','COC(c1ccccc1)C(C)c1ccc(C#N)cc1 ','COC(OC)C(c1ccccc1)c1ccc([N+](=O)[O-])cc1 ','COCOC(OC)C(C#N)(C#N)c1ccc(C#N)cc1 ','COC(=O)c1ccc(C(c2ccccc2)(c2ccccc2)C(C#N)Oc2ccccc2)cc1 ','CCc1ccc(C(C)(C)C(OC)OCOC)cc1 ','COc1ccc(C(C#N)(c2ccccc2)C(OC)c2ccccc2)cc1 ','COC(C)C(C)(C#N)c1ccc(C#N)cc1 ','N#CC(c1ccc(-c2ccccc2)cc1)C(Oc1ccccc1)c1ccccc1 ','COCOC(Br)C(C#N)c1ccc([N+](=O)[O-])cc1 ','COCC(C#N)(c1ccccc1)c1ccc([N+](=O)[O-])cc1 ','COCC(C#N)(C#N)c1ccccc1 ','COCOC(C)C(c1ccccc1)(c1ccccc1)c1ccccc1 ','COCOC(Br)C(C)(c1ccccc1)c1ccccc1 ','COCOCC(C)(C#N)c1ccc(-c2ccccc2)cc1 ','COC(OC)C(c1ccccc1)(c1ccccc1)c1ccccc1 ','COc1ccc(C(C)(C#N)C(C)OC)cc1 ','CCc1ccc(C(C)(c2ccccc2)C(Oc2ccccc2)c2ccccc2)cc1 ','COCOC(c1ccccc1)C(C#N)c1ccc(C#N)cc1 ','CCc1ccc(C(c2ccccc2)(c2ccccc2)C(OC)c2ccccc2)cc1 ','CC(C)(c1ccccc1)C(Br)Oc1ccccc1 ','CCc1ccc(C(C)(c2ccccc2)C(C#N)Oc2ccccc2)cc1 ','COCOCC(C#N)(C#N)c1ccc(-c2ccccc2)cc1 ','COC(C#N)C(C)c1ccccc1 ','COCOC(OC)C(C#N)(c1ccccc1)c1ccccc1 ','COC(=O)c1ccc(C(C)(c2ccccc2)C(C#N)Oc2ccccc2)cc1 ','CCc1ccc(C(C)C(C#N)Oc2ccccc2)cc1 ','COC(C)C(C#N)c1ccc([N+](=O)[O-])cc1 ','COC(C)C(C)(C)c1ccc(C#N)cc1 ','COC(=O)c1ccc(C(c2ccccc2)C(OC)OC)cc1 ','CCc1ccc(C(c2ccccc2)C(C#N)Oc2ccccc2)cc1 ','COC(OC)C(C)(C#N)c1ccc([N+](=O)[O-])cc1 ','COCOC(C)C(C#N)(c1ccccc1)c1ccc(OC)cc1 ','COCOC(Br)Cc1ccc(-c2ccccc2)cc1 ','COCOC(c1ccccc1)C(c1ccccc1)(c1ccccc1)c1ccc([N+](=O)[O-])cc1 ','COC(C)C(C)c1ccccc1 ','COCC(C)c1ccc(C#N)cc1 ','COC(=O)c1ccc(C(C#N)(c2ccccc2)C(Br)OC)cc1 ','COCOC(OC)C(C#N)(c1ccccc1)c1ccc(C#N)cc1 ','COCOC(Br)C(C#N)(C#N)c1ccccc1 ','COCC(C#N)c1ccc(C#N)cc1 ','COCOC(C#N)Cc1ccc(-c2ccccc2)cc1 ','CCc1ccc(C(C#N)COC)cc1 ','COC(=O)c1ccc(C(C#N)(C#N)C(Oc2ccccc2)c2ccccc2)cc1 ','N#CC(COc1ccccc1)c1ccc(-c2ccccc2)cc1 ','CCc1ccc(C(c2ccccc2)(c2ccccc2)C(C#N)OCOC)cc1 ','CCc1ccc(C(c2ccccc2)(c2ccccc2)C(C#N)Oc2ccccc2)cc1 ','COCOCC(C#N)c1ccccc1 ','CCc1ccc(C(C)C(C#N)OC)cc1 ','COC(Br)C(C)c1ccc(C#N)cc1 ','COCOC(Br)C(C)(C#N)c1ccc(-c2ccccc2)cc1 ','COC(OC)C(C#N)(C#N)c1ccc([N+](=O)[O-])cc1 ','COC(C)C(C#N)(c1ccccc1)c1ccccc1 ','CCc1ccc(C(C)COCOC)cc1 ','COCOC(C#N)C(C#N)(c1ccccc1)c1ccc(C#N)cc1 ','COC(C)C(C#N)(c1ccccc1)c1ccc(-c2ccccc2)cc1 ','COc1ccc(C(C#N)C(Br)OC)cc1 ','COCOCC(C)(C#N)c1ccc(C(=O)OC)cc1 ','CCc1ccc(C(C)C(OC)OC)cc1 ','CC(Oc1ccccc1)C(C#N)(c1ccccc1)c1ccccc1 ','COc1ccc(C(C#N)C(Oc2ccccc2)c2ccccc2)cc1 ','COCOC(C#N)C(C#N)c1ccc(OC)cc1 ','COCOC(OC)C(C#N)(C#N)c1ccc(OC)cc1 ','COC(=O)c1ccc(C(C)C(OC)OC)cc1 ','CCc1ccc(C(C#N)(C#N)C(C)OCOC)cc1 ','COCOCC(C)(c1ccccc1)c1ccc(C(=O)OC)cc1 ','CCc1ccc(C(C#N)(c2ccccc2)C(C#N)OCOC)cc1 ','COCOC(C#N)C(C)(C#N)c1ccccc1 ','COC(OC)C(C#N)(c1ccccc1)c1ccccc1 ','CCc1ccc(C(c2ccccc2)(c2ccccc2)C(OC)OC)cc1 ','COC(C#N)C(C#N)c1ccccc1 ','COC(C)C(C#N)(c1ccccc1)c1ccc(C#N)cc1 ','COCOC(OC)C(C)c1ccc(C#N)cc1 ','COCOC(C#N)C(c1ccccc1)c1ccc(-c2ccccc2)cc1 ','CC(Oc1ccccc1)C(C#N)(c1ccccc1)c1ccc(-c2ccccc2)cc1 ','COC(Oc1ccccc1)C(C)(c1ccccc1)c1ccc([N+](=O)[O-])cc1 ','COCOC(OC)C(C#N)c1ccc(-c2ccccc2)cc1 ','COCOC(C)C(C#N)(C#N)c1ccccc1 ','CCc1ccc(C(c2ccccc2)(c2ccccc2)C(C#N)OC)cc1 ','CC(Oc1ccccc1)C(c1ccccc1)c1ccc([N+](=O)[O-])cc1 ','COCOC(OC)C(C#N)(C#N)c1ccc(-c2ccccc2)cc1 ','COCOC(C)C(C)(c1ccccc1)c1ccc([N+](=O)[O-])cc1 ','CC(Oc1ccccc1)C(C#N)(C#N)c1ccc([N+](=O)[O-])cc1 ','CCc1ccc(C(C)C(OCOC)c2ccccc2)cc1 ','COc1ccc(C(C)(C#N)C(Br)Oc2ccccc2)cc1 ','COC(Br)Cc1ccccc1 ','N#CC(Cc1ccc(-c2ccccc2)cc1)Oc1ccccc1 ','N#CC(Oc1ccccc1)C(C#N)(c1ccccc1)c1ccccc1 ','CCc1ccc(C(c2ccccc2)C(C#N)OCOC)cc1 ','COC(=O)c1ccc(CC(OC)Oc2ccccc2)cc1 ','COC(Cc1ccc([N+](=O)[O-])cc1)c1ccccc1 ','CCc1ccc(C(c2ccccc2)C(OC)c2ccccc2)cc1 ','COC(C#N)C(C)c1ccc(-c2ccccc2)cc1 ','N#Cc1ccc(C(c2ccccc2)C(Oc2ccccc2)c2ccccc2)cc1 ','c1ccc(OCC(c2ccccc2)(c2ccccc2)c2ccccc2)cc1 ','COC(=O)c1ccc(CC(Br)Oc2ccccc2)cc1 ','CCc1ccc(C(C)(C#N)C(C)Oc2ccccc2)cc1 ','COC(C)C(C#N)(C#N)c1ccccc1 ','COc1ccc(C(C#N)(c2ccccc2)C(C#N)OC)cc1 ','COC(=O)c1ccc(C(c2ccccc2)(c2ccccc2)C(C#N)OC)cc1 ','COCOC(C)C(C#N)(c1ccccc1)c1ccccc1 ','COCOCC(c1ccccc1)c1ccc(C#N)cc1 ','COCOC(c1ccccc1)C(C)(C#N)c1ccccc1 ','COCC(c1ccccc1)(c1ccccc1)c1ccc(C(=O)OC)cc1 ','COCOC(C)C(C#N)(C#N)c1ccc(C(=O)OC)cc1 ','COC(C)C(C)(C#N)c1ccccc1 ',\n",
    "\t\t  'COCOC(Cc1ccc(C(=O)OC)cc1)OC ','N#CC(Oc1ccccc1)C(c1ccccc1)c1ccc(-c2ccccc2)cc1 ','N#Cc1ccc(C(COc2ccccc2)c2ccccc2)cc1 ','COCOC(OC)C(c1ccccc1)c1ccccc1 ','COCOC(OC)C(C#N)(c1ccccc1)c1ccc(OC)cc1 ','COCOCC(C)c1ccc(C(=O)OC)cc1 ','COCOC(Br)C(C)(C#N)c1ccc(OC)cc1 ','COCOCC(C#N)(c1ccccc1)c1ccc([N+](=O)[O-])cc1']\n",
    "#print(smiles)\n",
    "df = pd.DataFrame({'SMILES':smiles})\n",
    "#df = pd.DataFrame['ID':clogp, 'SMILES':smiles]# ({'ID':clogp, 'SMILES':smiles})\n",
    "df['Mol Image'] = [Chem.MolFromSmiles(s) for s in df['SMILES']]\n",
    "#ChangeMoleculeRendering(renderer='PNG')\n",
    "PandasTools.SaveXlsxFromFrame(df, 'smile_Mol_Image.xlsx', molCol='Mol Image')\n",
    "\n",
    "mols = [Chem.MolFromSmiles(smi) for smi in smiles]\n",
    "#Draw.MolsToGridImage(mols, molsPerRow=4, subImgSize=(200, 200))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from rdkit.Chem import PandasTools\n",
    "esol_data = pd.read_csv(\"./SMILES_feature.csv\")\n",
    "#esol_data.head(1)\n",
    "#Add ROMol to data\n",
    "PandasTools.AddMoleculeColumnToFrame(esol_data, smilesCol='smiles')\n",
    "esol_data.head(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(esol_data.ROMol[0]))\n",
    "PandasTools.FrameToGridImage(esol_data.head(8), legendsCol=\"clogp\", molsPerRow=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding new columns of properites use Pandas map method\n",
    "esol_data[\"n_Atoms\"] = esol_data['ROMol'].map(lambda x: x.GetNumAtoms())\n",
    "esol_data.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Before saving the dataframe as csv file, it is recommanded to drop the ROMol column.\n",
    "esol_data = esol_data.drop(['ROMol'], axis=1)\n",
    "esol_data.head(1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RDKit has avariety of built-in functionality for generating molecular fingerprints/descriptors\n",
    "#url = 'https://raw.githubusercontent.com/XinhaoLi74/molds/master/clean_data/ESOL.csv'\n",
    "#esol_data = pd.read_csv(url)\n",
    "esol_data = pd.read_csv(\"./SMILES_feature.csv\")\n",
    "PandasTools.AddMoleculeColumnToFrame(esol_data, smilesCol='smiles')\n",
    "esol_data.head(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Chem.AllChem.GetMorganFingerprintAsBitVect\n",
    "radius=3\n",
    "nBits=2048\n",
    "ECFP6 = [Chem.AllChem.GetMorganFingerprintAsBitVect(x,radius=radius, nBits=nBits) for x in esol_data['ROMol']]\n",
    "print(ECFP6[0])\n",
    "print(len(ECFP6[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecfp6_name = [f'Bit_{i}' for i in range(nBits)]\n",
    "ecfp6_bits = [list(l) for l in ECFP6]\n",
    "df_morgan = pd.DataFrame(ecfp6_bits, index = esol_data.smiles, columns=ecfp6_name)\n",
    "df_morgan.head(1)\n",
    "df_morgan.to_csv(\"./SMILES_ecfp6_feature_add.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Similarity Search\n",
    "ref_smiles = 'N#CC(c1ccccc1)C(Br)Oc1ccccc1'\n",
    "ref_mol = Chem.MolFromSmiles(ref_smiles)\n",
    "ref_ECFP4_fps = Chem.AllChem.GetMorganFingerprintAsBitVect(ref_mol,2)\n",
    "ref_mol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bulk_ECFP4_fps = [Chem.AllChem.GetMorganFingerprintAsBitVect(x,2) for x in esol_data['ROMol']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit import DataStructs\n",
    "\n",
    "similarity_efcp4 = [DataStructs.FingerprintSimilarity(ref_ECFP4_fps,x) for x in bulk_ECFP4_fps]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "esol_data['Tanimoto_Similarity (ECFP4)'] = similarity_efcp4\n",
    "PandasTools.FrameToGridImage(esol_data.head(8), legendsCol=\"Tanimoto_Similarity (ECFP4)\", molsPerRow=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "esol_data = esol_data.sort_values(['Tanimoto_Similarity (ECFP4)'], ascending=False)\n",
    "PandasTools.FrameToGridImage(esol_data.head(8), legendsCol=\"Tanimoto_Similarity (ECFP4)\", molsPerRow=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RDKit\n",
    "# https://github.com/XinhaoLi74/Hierarchical-QSAR-Modeling/blob/master/notebooks/descriptors.ipynb\n",
    "generator = MakeGenerator((\"RDKit2D\",)) \n",
    "train = pd.read_csv(\"./SMILES_feature.csv\")\n",
    "PandasTools.AddMoleculeColumnToFrame(train,smilesCol='smiles')\n",
    "train_rdkit2d = [generator.process(x)[1:] for x in train['smiles']]\n",
    "# morgan fingerprint\n",
    "train_ECFP6 = [Chem.GetMorganFingerprintAsBitVect(x,3) for x in train['ROMol']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdkit2d_name = []\n",
    "for name, numpy_type in generator.GetColumns():\n",
    "    rdkit2d_name.append(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_rdkit2d_df = pd.DataFrame(train_rdkit2d, index = train.index, columns=rdkit2d_name[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_rdkit2d_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_rdkit2d_df.to_csv('./train_rdkit2d.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RDKit to calculte molecular fingerprint and similarity of a list of SMILE structures?\n",
    "from rdkit import Chem\n",
    "from rdkit import DataStructs\n",
    "from rdkit.Chem.Fingerprints import FingerprintMols\n",
    "import pandas as pd\n",
    "\n",
    "# read and Conconate the csv's\n",
    "#df_1 = pd.read_csv('first.csv')\n",
    "#df_2 = pd.read_csv('second.csv')\n",
    "df_3 = pd.read_csv(\"./SMILES_feature.csv\")\n",
    "\n",
    "# proof and make a list of SMILES\n",
    "df_smiles = df_3['smiles']\n",
    "c_smiles = []\n",
    "for ds in df_smiles:\n",
    "    try:\n",
    "        cs = Chem.CanonSmiles(ds)\n",
    "        c_smiles.append(cs)\n",
    "    except:\n",
    "        print('Invalid SMILES:', ds)\n",
    "print()\n",
    "\n",
    "# make a list of mols\n",
    "ms = [Chem.MolFromSmiles(x) for x in c_smiles]\n",
    "\n",
    "# make a list of fingerprints (fp)\n",
    "fps = [FingerprintMols.FingerprintMol(x) for x in ms]\n",
    "\n",
    "# the list for the dataframe\n",
    "qu, ta, sim = [], [], []\n",
    "\n",
    "# compare all fp pairwise without duplicates\n",
    "for n in range(len(fps)-1): # -1 so the last fp will not be used\n",
    "    s = DataStructs.BulkTanimotoSimilarity(fps[n], fps[n+1:]) # +1 compare with the next to the last fp\n",
    "    print(c_smiles[n], c_smiles[n+1:]) # witch mol is compared with what group\n",
    "    # collect the SMILES and values\n",
    "    for m in range(len(s)):\n",
    "        qu.append(c_smiles[n])\n",
    "        ta.append(c_smiles[n+1:][m])\n",
    "        sim.append(s[m])\n",
    "print()\n",
    "\n",
    "# build the dataframe and sort it\n",
    "d = {'query':qu, 'target':ta, 'Similarity':sim}\n",
    "df_final = pd.DataFrame(data=d)\n",
    "df_final = df_final.sort_values('Similarity', ascending=False)\n",
    "#print(df_final)\n",
    "\n",
    "# save as csv\n",
    "df_final.to_csv('third.csv', index=False, sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit import Chem\n",
    "from rdkit import DataStructs \n",
    "from rdkit.Chem.Fingerprints import FingerprintMols\n",
    "\n",
    "template = Chem.MolFromSmiles('CC(C)(c1ccc([N+](=O)[O-])cc1)C(C#N)Oc1ccccc1')\n",
    "Chem.AllChem.Compute2DCoords(template)\n",
    "\n",
    "ms = [Chem.MolFromSmiles(smi) for smi in ('N#CC(c1ccccc1)C(Br)Oc1ccccc1','O=[N+]([O-])c1ccc(C(c2ccccc2)C(Br)Oc2ccccc2)cc1','CC(Oc1ccccc1)C(C#N)c1ccc(C#N)cc1 ','COC(C#N)C(C)(C)c1ccc(-c2ccccc2)cc1 ','COC(Oc1ccccc1)C(C)(C#N)c1ccc([N+](=O)[O-])cc1 ','COCC(C)(c1ccccc1)c1ccc([N+](=O)[O-])cc1 ','CCc1ccc(C(C#N)C(OC)OCOC)cc1 ','COC(C#N)C(C)(C)c1ccccc1 ','CCc1ccc(C(C)(c2ccccc2)C(Br)OC)cc1 ','COCOC(C)C(C)c1ccc(C(=O)OC)cc1 ','CCc1ccc(C(C#N)COc2ccccc2)cc1 ','COC(=O)c1ccc(C(C)(c2ccccc2)C(OC)OC)cc1 ','CCc1ccc(C(C)(C#N)C(C)OC)cc1 ','COC(C#N)C(C#N)c1ccc(C#N)cc1 ','COCOC(C#N)C(C)(C#N)c1ccc(C#N)cc1 ','COCC(C)(C#N)c1ccc(C#N)cc1 ','CC(C)(COc1ccccc1)c1ccc([N+](=O)[O-])cc1 ','COc1ccc(C(C#N)(C#N)C(C)OC)cc1 ','COC(Br)C(C)(C#N)c1ccc([N+](=O)[O-])cc1 ','COC(c1ccccc1)C(C#N)(C#N)c1ccc(-c2ccccc2)cc1 ','COC(c1ccccc1)C(C#N)c1ccc([N+](=O)[O-])cc1 ','CCc1ccc(CCOC)cc1 ','COCOCC(C#N)(C#N)c1ccccc1 ','COCOCC(C)c1ccccc1 ','COC(C#N)C(c1ccccc1)c1ccc(C#N)cc1 ','COC(=O)c1ccc(C(C)(C#N)C(C#N)OC)cc1 ','COc1ccc(C(C#N)(c2ccccc2)C(OC)OC)cc1 ','CCc1ccc(C(C#N)C(C#N)OCOC)cc1 ','COC(=O)c1ccc(C(C#N)C(Br)Oc2ccccc2)cc1 ','COCOC(Br)C(C)(C#N)c1ccc([N+](=O)[O-])cc1 ','COC(Br)C(C)(C)c1ccc(C#N)cc1 ','COC(Oc1ccccc1)C(C#N)c1ccccc1 ','N#Cc1ccc(C(c2ccccc2)(c2ccccc2)C(Oc2ccccc2)c2ccccc2)cc1 ','COC(c1ccccc1)C(c1ccccc1)c1ccc([N+](=O)[O-])cc1 ','COCC(c1ccccc1)(c1ccccc1)c1ccccc1 ','COC(=O)c1ccc(C(C)(C#N)C(OC)OC)cc1 ','COCOC(Br)C(c1ccccc1)c1ccccc1 ','CCc1ccc(CCOCOC)cc1 ','COCC(C)(C#N)c1ccccc1 ','COCOC(C)C(c1ccccc1)c1ccccc1 ','N#Cc1ccc(C(c2ccccc2)(c2ccccc2)C(C#N)Oc2ccccc2)cc1 ','COC(=O)c1ccc(C(C)(c2ccccc2)C(Br)Oc2ccccc2)cc1 ','COCOCC(C)(C)c1ccc(C#N)cc1 ','CCc1ccc(C(C#N)C(Br)OCOC)cc1 ','COCOC(Br)C(C)(C#N)c1ccc(C#N)cc1 ','O=[N+]([O-])c1ccc(CC(Oc2ccccc2)c2ccccc2)cc1 ','CCc1ccc(C(C#N)(C#N)C(Br)OCOC)cc1 ','COCC(C#N)(C#N)c1ccc(C(=O)OC)cc1 ','CCc1ccc(C(C)(C#N)COCOC)cc1 ','N#CC(Oc1ccccc1)C(C#N)c1ccc(-c2ccccc2)cc1 ','COC(C#N)C(c1ccccc1)c1ccc(-c2ccccc2)cc1 ','COCOCC(C)(C)c1ccc([N+](=O)[O-])cc1 ','COC(C#N)C(C#N)c1ccc(-c2ccccc2)cc1 ','CCc1ccc(C(C)(C)C(OCOC)c2ccccc2)cc1 ','COCOC(Br)C(C)c1ccc([N+](=O)[O-])cc1 ','COCOCC(C#N)(c1ccccc1)c1ccc(C#N)cc1 ','COC(=O)c1ccc(C(C#N)C(C)OC)cc1 ','COc1ccc(C(C#N)(C#N)C(Br)OC)cc1 ','COC(=O)c1ccc(C(C)(C#N)C(Br)Oc2ccccc2)cc1 ','COC(c1ccccc1)C(C#N)(C#N)c1ccc([N+](=O)[O-])cc1 ','COCOC(OC)C(C)(C#N)c1ccc(-c2ccccc2)cc1 ','COC(=O)c1ccc(C(C)(C)C(Br)Oc2ccccc2)cc1 ','CCc1ccc(C(C#N)(c2ccccc2)C(Br)Oc2ccccc2)cc1 ','COC(=O)c1ccc(C(C)(c2ccccc2)C(C#N)OC)cc1 ','COC(c1ccccc1)C(C#N)c1ccc(C#N)cc1 ','CCc1ccc(C(c2ccccc2)(c2ccccc2)C(Br)Oc2ccccc2)cc1 ','COC(C)C(C)(c1ccccc1)c1ccc([N+](=O)[O-])cc1 ','COC(C)C(C)c1ccc(C#N)cc1 ','COCOC(Br)C(c1ccccc1)(c1ccccc1)c1ccc(C#N)cc1 ','CCc1ccc(C(C#N)(C#N)COCOC)cc1 ','COCC(c1ccccc1)(c1ccccc1)c1ccc(C#N)cc1 ','COCOC(Br)Cc1ccc(C#N)cc1 ','COCOC(OC)C(c1ccccc1)(c1ccccc1)c1ccc(C#N)cc1 ','CCc1ccc(C(C)(C)C(C#N)OCOC)cc1 ','COC(Br)C(C)c1ccccc1 ','COC(C#N)Cc1ccc([N+](=O)[O-])cc1 ','CC(c1ccccc1)(c1ccc(-c2ccccc2)cc1)C(C#N)Oc1ccccc1 ','CC(c1ccc([N+](=O)[O-])cc1)C(C#N)Oc1ccccc1 ','COCOC(C)C(C)c1ccc([N+](=O)[O-])cc1 ','COC(=O)c1ccc(C(C#N)C(Oc2ccccc2)c2ccccc2)cc1 ','COC(C)C(C#N)c1ccc(-c2ccccc2)cc1 ','COCOC(OC)C(C#N)c1ccccc1 ','COC(C)C(C#N)(C#N)c1ccc([N+](=O)[O-])cc1 ','COCC(C)c1ccccc1 ','CCc1ccc(C(C)(C#N)C(OCOC)c2ccccc2)cc1 ','COCC(C#N)(C#N)c1ccc(OC)cc1 ','CCc1ccc(C(C)(C#N)C(OC)OCOC)cc1 ','c1ccc(OCC(c2ccccc2)c2ccccc2)cc1 ','COC(=O)c1ccc(C(c2ccccc2)C(C#N)OC)cc1 ','COC(=O)c1ccc(C(C#N)(C#N)C(OC)OC)cc1 ','COC(C#N)C(c1ccccc1)c1ccc([N+](=O)[O-])cc1 ','COC(Oc1ccccc1)C(C)(C)c1ccc(C#N)cc1 ','CCc1ccc(CC(OCOC)c2ccccc2)cc1 ','N#CC(Oc1ccccc1)C(C#N)(c1ccccc1)c1ccc([N+](=O)[O-])cc1 ','CCc1ccc(C(c2ccccc2)(c2ccccc2)C(OC)Oc2ccccc2)cc1 ','COCOC(C#N)C(C#N)(C#N)c1ccccc1 ','COC(=O)c1ccc(CC(C#N)OC)cc1 ','CCc1ccc(CC(C#N)OC)cc1 ','CC(Oc1ccccc1)C(C)(C#N)c1ccc(C#N)cc1 ','COC(Oc1ccccc1)C(C#N)(C#N)c1ccccc1 ','COc1ccc(C(C#N)COc2ccccc2)cc1 ','CCc1ccc(C(C#N)(c2ccccc2)C(Br)OCOC)cc1 ','CCc1ccc(C(C)(COCOC)c2ccccc2)cc1 ','CCc1ccc(C(C#N)C(Br)OC)cc1 ','COCOCC(c1ccccc1)(c1ccccc1)c1ccccc1 ','COCOC(C)Cc1ccccc1 ','CC(c1ccccc1)C(Br)Oc1ccccc1 ','CC(C#N)(c1ccccc1)C(Oc1ccccc1)c1ccccc1 ','COC(c1ccccc1)C(C#N)c1ccc(-c2ccccc2)cc1 ','CCc1ccc(C(C)(c2ccccc2)C(C#N)OCOC)cc1 ','N#CC(COc1ccccc1)(c1ccccc1)c1ccc([N+](=O)[O-])cc1 ','COCOC(C)C(C)(c1ccccc1)c1ccc(C(=O)OC)cc1 ','CCc1ccc(C(C)(C)C(C#N)Oc2ccccc2)cc1 ','COCOC(c1ccccc1)C(C)(C)c1ccccc1 ','COC(C)C(C)(C#N)c1ccc([N+](=O)[O-])cc1 ','COC(Oc1ccccc1)C(C#N)(C#N)c1ccc(-c2ccccc2)cc1 ','CC(c1ccc(C#N)cc1)C(Oc1ccccc1)c1ccccc1 ',\n",
    "          'COC(=O)c1ccc(C(C)C(C)OC)cc1 ','COC(=O)c1ccc(C(C)(C)C(C#N)Oc2ccccc2)cc1 ','COC(Oc1ccccc1)C(c1ccccc1)c1ccc(C#N)cc1 ','COCOC(C#N)C(C)(c1ccccc1)c1ccccc1 ','CCc1ccc(C(c2ccccc2)C(C#N)OC)cc1 ','COC(=O)c1ccc(C(C#N)(C#N)C(C)Oc2ccccc2)cc1 ','COCC(C#N)(C#N)c1ccc([N+](=O)[O-])cc1 ','COCOC(C)C(C#N)(C#N)c1ccc(-c2ccccc2)cc1 ','COCOC(C#N)C(C)(C#N)c1ccc(OC)cc1 ','COC(C#N)C(C#N)(c1ccccc1)c1ccc(-c2ccccc2)cc1 ','CCc1ccc(C(C#N)(C#N)C(OCOC)c2ccccc2)cc1 ','COC(=O)c1ccc(CCOc2ccccc2)cc1 ','BrC(Cc1ccccc1)Oc1ccccc1 ','CCc1ccc(C(COCOC)c2ccccc2)cc1 ','COC(C)C(c1ccccc1)(c1ccccc1)c1ccc(C#N)cc1 ','COCOC(c1ccccc1)C(c1ccccc1)c1ccc(C(=O)OC)cc1 ','COCOCC(C)(c1ccccc1)c1ccc([N+](=O)[O-])cc1 ','COC(Cc1ccc(C#N)cc1)c1ccccc1 ','CCc1ccc(C(C)(c2ccccc2)C(OC)OC)cc1 ','COC(=O)c1ccc(C(C)C(Br)Oc2ccccc2)cc1 ','COC(Br)Cc1ccc([N+](=O)[O-])cc1 ','COC(=O)c1ccc(C(C)(C#N)C(Br)OC)cc1 ','COCOC(C)C(C#N)c1ccc(-c2ccccc2)cc1 ','COCOC(Br)C(C#N)c1ccc(C#N)cc1 ','BrC(Oc1ccccc1)C(c1ccccc1)c1ccccc1 ','COC(c1ccccc1)C(c1ccccc1)c1ccc(C#N)cc1 ','CCc1ccc(C(C)(C)C(C)OCOC)cc1 ','COCOC(OC)C(C)(c1ccccc1)c1ccc([N+](=O)[O-])cc1 ','COCOCC(c1ccccc1)c1ccccc1 ','COC(Cc1ccccc1)OC ','N#CC(c1ccccc1)(c1ccc(-c2ccccc2)cc1)C(Br)Oc1ccccc1 ','COCOC(C#N)C(C)(c1ccccc1)c1ccc(C#N)cc1 ','CCc1ccc(C(C#N)(COC)c2ccccc2)cc1 ','COC(C#N)C(C)c1ccc([N+](=O)[O-])cc1 ','COC(=O)c1ccc(C(C)C(C#N)Oc2ccccc2)cc1 ','COC(C)C(c1ccccc1)c1ccc(C#N)cc1 ','COC(c1ccccc1)C(c1ccccc1)(c1ccccc1)c1ccc([N+](=O)[O-])cc1 ','CC(Oc1ccccc1)C(C)(C#N)c1ccc(-c2ccccc2)cc1 ','COC(=O)c1ccc(C(C)C(C#N)OC)cc1 ','N#Cc1ccc(C(C#N)(c2ccccc2)C(Br)Oc2ccccc2)cc1 ','O=[N+]([O-])c1ccc(C(COc2ccccc2)(c2ccccc2)c2ccccc2)cc1 ','COCOC(OC)C(C#N)(c1ccccc1)c1ccc([N+](=O)[O-])cc1 ','CCc1ccc(C(C)(C#N)C(Br)Oc2ccccc2)cc1 ','CCc1ccc(C(C#N)C(C)OCOC)cc1 ','COC(=O)c1ccc(C(COc2ccccc2)c2ccccc2)cc1 ','COC(C)C(C#N)c1ccccc1 ','COC(=O)c1ccc(C(C#N)C(C#N)OC)cc1 ','COCOC(c1ccccc1)C(C#N)(c1ccccc1)c1ccc([N+](=O)[O-])cc1 ','COC(Br)C(c1ccccc1)(c1ccccc1)c1ccccc1 ','COC(Br)C(C#N)c1ccc([N+](=O)[O-])cc1 ','COC(OC)C(C#N)c1ccccc1 ','CCc1ccc(C(C)(c2ccccc2)C(Br)OCOC)cc1 ','CC(c1ccccc1)(c1ccc(C#N)cc1)C(C#N)Oc1ccccc1 ','COCOC(c1ccccc1)C(c1ccccc1)(c1ccccc1)c1ccc(C#N)cc1 ','CCc1ccc(C(C#N)C(OC)Oc2ccccc2)cc1 ','COCOC(Br)C(C)c1ccc(C(=O)OC)cc1 ','N#CC(C#N)(c1ccc([N+](=O)[O-])cc1)C(Br)Oc1ccccc1 ','N#Cc1ccc(C(C#N)C(Br)Oc2ccccc2)cc1 ','COCC(C#N)(c1ccccc1)c1ccc(C(=O)OC)cc1 ','O=[N+]([O-])c1ccc(CCOc2ccccc2)cc1 ','COCOC(C#N)C(C)(C#N)c1ccc(C(=O)OC)cc1 ','COC(=O)c1ccc(C(C#N)(C#N)C(Br)OC)cc1 ','COC(=O)c1ccc(C(c2ccccc2)(c2ccccc2)C(Br)Oc2ccccc2)cc1 ','COCOC(Br)C(C#N)(c1ccccc1)c1ccccc1 ','COCOC(Br)C(c1ccccc1)c1ccc(C#N)cc1 ','N#CC(C#N)(c1ccc(-c2ccccc2)cc1)C(Oc1ccccc1)c1ccccc1 ','COCOC(C#N)C(C)(C)c1ccc(C(=O)OC)cc1 ','CCc1ccc(C(C#N)C(Oc2ccccc2)c2ccccc2)cc1 ','CCc1ccc(C(C)(C)C(C#N)OC)cc1 ','COC(Br)C(C#N)(c1ccccc1)c1ccc(-c2ccccc2)cc1 ','CCc1ccc(C(C#N)C(OC)OC)cc1 ','N#Cc1ccc(C(C#N)(COc2ccccc2)c2ccccc2)cc1 ','COCOC(c1ccccc1)C(C)(C)c1ccc(C(=O)OC)cc1 ','COCOC(C)C(C)(C#N)c1ccc(-c2ccccc2)cc1 ','COC(Br)C(C)(C#N)c1ccc(-c2ccccc2)cc1 ','CCc1ccc(C(C)(C)C(Br)OC)cc1 ','COC(C#N)C(c1ccccc1)(c1ccccc1)c1ccccc1 ','CCc1ccc(C(C)(C)C(Br)OCOC)cc1 ','COC(=O)c1ccc(C(C)(C)C(C#N)OC)cc1 ','COC(OC)C(c1ccccc1)c1ccccc1 ','COC(C)C(C#N)c1ccc(C#N)cc1 ','COCOC(Br)C(C)(C)c1ccc(C#N)cc1 ','CCc1ccc(C(c2ccccc2)C(OC)Oc2ccccc2)cc1 ','CC(Cc1ccc([N+](=O)[O-])cc1)Oc1ccccc1 ','COCOC(C)C(C#N)(C#N)c1ccc([N+](=O)[O-])cc1 ','COCOC(Br)C(C)(C#N)c1ccccc1 ','COCOC(C#N)C(C#N)(C#N)c1ccc(C(=O)OC)cc1 ','CCc1ccc(C(C#N)(C#N)C(C#N)OCOC)cc1 ','N#Cc1ccc(C(c2ccccc2)C(C#N)Oc2ccccc2)cc1 ','COCOC(C)Cc1ccc([N+](=O)[O-])cc1 ','COC(Br)C(c1ccccc1)c1ccc([N+](=O)[O-])cc1 ','CC(C#N)(c1ccc(-c2ccccc2)cc1)C(Oc1ccccc1)c1ccccc1 ','CC(Oc1ccccc1)C(C)(C#N)c1ccc([N+](=O)[O-])cc1 ','COC(OC)C(C)(C#N)c1ccccc1 ','COc1ccc(C(C)(C#N)COc2ccccc2)cc1 ','COC(c1ccccc1)C(C)(C#N)c1ccc(-c2ccccc2)cc1 ','CCc1ccc(C(C#N)(c2ccccc2)C(C)Oc2ccccc2)cc1 ','COC(=O)c1ccc(C(C#N)(C#N)C(Br)Oc2ccccc2)cc1 ','COC(=O)c1ccc(C(c2ccccc2)C(C#N)Oc2ccccc2)cc1 ','COC(Cc1ccc([N+](=O)[O-])cc1)OC ','COC(c1ccccc1)C(C#N)c1ccccc1 ','COCOCCc1ccc(C(=O)OC)cc1 ','COC(Oc1ccccc1)C(C#N)(c1ccccc1)c1ccc([N+](=O)[O-])cc1 ','COC(Oc1ccccc1)C(c1ccccc1)(c1ccccc1)c1ccccc1 ','COCC(C#N)(c1ccccc1)c1ccccc1 ','CC(C)(c1ccc(C#N)cc1)C(C#N)Oc1ccccc1 ','O=[N+]([O-])c1ccc(C(c2ccccc2)(c2ccccc2)C(Oc2ccccc2)c2ccccc2)cc1 ','COCOC(C)C(C#N)(C#N)c1ccc(OC)cc1 ','COC(Oc1ccccc1)C(C)c1ccc(C#N)cc1 ','COC(=O)c1ccc(CC(Br)OC)cc1 ','COCOC(Cc1ccc(C(=O)OC)cc1)c1ccccc1 ','COCC(C)(c1ccccc1)c1ccc(C#N)cc1 ',\n",
    "          'COCOC(OC)C(C)(c1ccccc1)c1ccc(C#N)cc1 ','COC(Cc1ccc([N+](=O)[O-])cc1)Oc1ccccc1 ','COCC(C)(C)c1ccc(C#N)cc1 ','COCOC(C)C(C)(C#N)c1ccc(C(=O)OC)cc1 ','COC(c1ccccc1)C(c1ccccc1)c1ccccc1 ','COC(Cc1ccc(C#N)cc1)Oc1ccccc1 ','COc1ccc(C(C#N)(C#N)C(OC)c2ccccc2)cc1 ','N#CC(c1ccccc1)(c1ccc([N+](=O)[O-])cc1)C(Oc1ccccc1)c1ccccc1 ','COc1ccc(C(C#N)C(C#N)Oc2ccccc2)cc1 ','N#Cc1ccc(C(C#N)(c2ccccc2)C(C#N)Oc2ccccc2)cc1 ','COCOC(Br)C(C)(C)c1ccc(C(=O)OC)cc1 ','COC(Br)C(C)(C)c1ccc([N+](=O)[O-])cc1 ','N#CC(Oc1ccccc1)C(c1ccccc1)c1ccccc1 ','COC(C)C(C)(c1ccccc1)c1ccc(C#N)cc1 ','CCc1ccc(C(C)(C#N)C(C#N)OCOC)cc1 ','COCOC(C#N)C(C)(C)c1ccc(C#N)cc1 ','COCOC(C#N)C(C)c1ccccc1 ','BrC(Oc1ccccc1)C(c1ccccc1)(c1ccccc1)c1ccccc1 ','COCOC(C)C(c1ccccc1)(c1ccccc1)c1ccc([N+](=O)[O-])cc1 ','CCc1ccc(C(C)C(OC)OCOC)cc1 ','CC(C)(c1ccc(C#N)cc1)C(Br)Oc1ccccc1 ','COC(=O)c1ccc(CC(OC)c2ccccc2)cc1 ','CCc1ccc(C(C)(C#N)COc2ccccc2)cc1 ','CCc1ccc(C(C)C(C#N)OCOC)cc1 ','COCOC(C#N)C(C)(C#N)c1ccc([N+](=O)[O-])cc1 ','COCOC(Cc1ccccc1)OC ','COCOCC(C#N)c1ccc([N+](=O)[O-])cc1 ','CCc1ccc(C(C)C(C)OC)cc1 ','COCOC(C#N)C(C#N)(c1ccccc1)c1ccccc1 ','COC(Oc1ccccc1)C(C#N)(c1ccccc1)c1ccc(-c2ccccc2)cc1 ','COCOCC(C)c1ccc([N+](=O)[O-])cc1 ','COCOC(OC)C(C#N)(C#N)c1ccc(C(=O)OC)cc1 ', 'COCOC(C#N)Cc1ccc(OC)cc1','COC(=O)c1ccc(C(C#N)C(Br)OC)cc1 ','CCc1ccc(C(C)(C#N)COC)cc1 ','COC(Br)C(C#N)c1ccccc1 ','COC(=O)c1ccc(CC(C)OC)cc1 ','COC(C)C(C)(c1ccccc1)c1ccccc1 ','COc1ccc(C(C#N)(C#N)COc2ccccc2)cc1 ','COc1ccc(C(C#N)(C#N)C(Br)Oc2ccccc2)cc1 ','COC(C)Cc1ccccc1 ','COC(Br)C(C#N)(C#N)c1ccc(-c2ccccc2)cc1 ','COC(=O)c1ccc(C(C#N)(c2ccccc2)C(C#N)OC)cc1 ','CCc1ccc(C(C#N)(C#N)C(OC)OC)cc1 ','CCc1ccc(C(C)(COC)c2ccccc2)cc1 ','COC(Br)C(C#N)(c1ccccc1)c1ccc(C#N)cc1 ','CCc1ccc(C(C)(c2ccccc2)C(OCOC)c2ccccc2)cc1 ','COC(C#N)C(C#N)(C#N)c1ccc(C#N)cc1 ','COCOC(Cc1ccccc1)c1ccccc1 ','COCOC(C#N)C(C#N)(C#N)c1ccc(C#N)cc1 ','COC(C)C(C#N)(C#N)c1ccc(-c2ccccc2)cc1 ','COCOC(Br)C(C#N)(c1ccccc1)c1ccc(C#N)cc1 ','COCOC(Br)C(c1ccccc1)(c1ccccc1)c1ccccc1 ','COCOCC(C#N)c1ccc(OC)cc1 ','COCOC(C#N)C(c1ccccc1)(c1ccccc1)c1ccc(-c2ccccc2)cc1 ','COCOC(C#N)C(C)(C#N)c1ccc(-c2ccccc2)cc1 ','N#CC(C#N)(COc1ccccc1)c1ccccc1 ','COCOC(OC)C(C#N)(C#N)c1ccc([N+](=O)[O-])cc1 ','COC(=O)c1ccc(C(C#N)(c2ccccc2)C(OC)c2ccccc2)cc1 ','COC(=O)c1ccc(C(C)(c2ccccc2)C(OC)c2ccccc2)cc1 ','COC(OC)C(C#N)(c1ccccc1)c1ccc(-c2ccccc2)cc1 ','COC(=O)c1ccc(C(C#N)(C#N)C(OC)Oc2ccccc2)cc1 ','COCOC(Br)C(C#N)(c1ccccc1)c1ccc(-c2ccccc2)cc1 ','N#CC(Oc1ccccc1)C(c1ccccc1)(c1ccccc1)c1ccc([N+](=O)[O-])cc1 ','COCOC(C)C(C)c1ccccc1 ','COC(=O)c1ccc(C(C)C(Br)OC)cc1 ','COCC(C)(c1ccccc1)c1ccccc1 ','N#Cc1ccc(C(c2ccccc2)C(Br)Oc2ccccc2)cc1 ','COCOC(OC)C(c1ccccc1)c1ccc(C(=O)OC)cc1 ','COCOC(c1ccccc1)C(c1ccccc1)c1ccc([N+](=O)[O-])cc1 ','COC(=O)c1ccc(C(C)(c2ccccc2)C(C)OC)cc1 ','COCOC(Cc1ccc([N+](=O)[O-])cc1)OC ','COCOC(c1ccccc1)C(C#N)(c1ccccc1)c1ccc(-c2ccccc2)cc1 ','COCOC(C#N)C(C#N)c1ccc(C(=O)OC)cc1 ','COC(=O)c1ccc(C(C#N)(c2ccccc2)C(OC)OC)cc1 ','COCOC(OC)C(c1ccccc1)c1ccc([N+](=O)[O-])cc1 ','CC(COc1ccccc1)c1ccc([N+](=O)[O-])cc1 ','c1ccc(OC(c2ccccc2)C(c2ccccc2)c2ccccc2)cc1 ','COCOC(c1ccccc1)C(C#N)(C#N)c1ccc(-c2ccccc2)cc1 ','COCOC(Br)C(C#N)(c1ccccc1)c1ccc([N+](=O)[O-])cc1 ','COC(Br)C(c1ccccc1)(c1ccccc1)c1ccc(C#N)cc1 ','COC(=O)c1ccc(C(C)(C#N)C(C)Oc2ccccc2)cc1 ','COCOCC(C#N)(c1ccccc1)c1ccccc1 ','COC(Oc1ccccc1)C(C#N)(c1ccccc1)c1ccccc1 ','COCOCC(C)(C)c1ccccc1 ','CCc1ccc(C(c2ccccc2)C(OC)OCOC)cc1 ','CC(Oc1ccccc1)C(C#N)(C#N)c1ccccc1 ','COC(Br)C(C#N)(C#N)c1ccccc1 ','COCC(C)(C#N)c1ccc(OC)cc1 ','COCOC(Br)C(C#N)(C#N)c1ccc(C#N)cc1 ','COC(Oc1ccccc1)C(c1ccccc1)c1ccccc1 ','COCOC(C)C(c1ccccc1)c1ccc(C(=O)OC)cc1 ','COCOC(C)C(c1ccccc1)(c1ccccc1)c1ccc(C#N)cc1 ','COCC(C)(C#N)c1ccc(-c2ccccc2)cc1 ','COC(C#N)C(c1ccccc1)(c1ccccc1)c1ccc([N+](=O)[O-])cc1 ','COC(Oc1ccccc1)C(C)(C)c1ccc([N+](=O)[O-])cc1 ','COC(Br)C(c1ccccc1)c1ccccc1 ','COCOC(Cc1ccc([N+](=O)[O-])cc1)c1ccccc1 ','COC(C)C(C)(C#N)c1ccc(-c2ccccc2)cc1 ','CC(Oc1ccccc1)C(C#N)c1ccccc1 ','COCCc1ccc(C(=O)OC)cc1 ','CC(c1ccccc1)(c1ccc(C#N)cc1)C(Br)Oc1ccccc1 ','CCc1ccc(CC(C)OC)cc1 ','CCc1ccc(C(C)(C#N)C(C#N)OC)cc1 ','COCOC(C#N)C(c1ccccc1)(c1ccccc1)c1ccc(C#N)cc1 ','COCOC(Br)C(C#N)(C#N)c1ccc([N+](=O)[O-])cc1 ','COC(Oc1ccccc1)C(c1ccccc1)(c1ccccc1)c1ccc([N+](=O)[O-])cc1 ','COCC(c1ccccc1)c1ccc(C#N)cc1 ','COCOC(c1ccccc1)C(C)(C)c1ccc([N+](=O)[O-])cc1 ','COc1ccc(C(C#N)(C#N)C(C#N)OC)cc1 ','CCc1ccc(C(c2ccccc2)C(Br)OCOC)cc1 ','COC(=O)c1ccc(C(c2ccccc2)(c2ccccc2)C(Oc2ccccc2)c2ccccc2)cc1 ','CCc1ccc(C(C#N)(C#N)COc2ccccc2)cc1 ',\n",
    "\t\t  'COCOCC(C)(c1ccccc1)c1ccc(C#N)cc1 ','CCc1ccc(CC(C#N)Oc2ccccc2)cc1 ','COC(Br)C(C)(c1ccccc1)c1ccccc1 ','COCOCC(C#N)(C#N)c1ccc([N+](=O)[O-])cc1 ','COC(OC)C(C#N)c1ccc(-c2ccccc2)cc1 ','COC(OC)C(C)(C)c1ccccc1 ','COCOC(OC)C(C)(C#N)c1ccc(OC)cc1 ','COC(OC)C(c1ccccc1)(c1ccccc1)c1ccc(C#N)cc1 ','COCOCC(C)(C)c1ccc(C(=O)OC)cc1 ','COCOC(c1ccccc1)C(C)c1ccc(C#N)cc1 ','COC(OC)C(C)(C#N)c1ccc(-c2ccccc2)cc1 ','COC(=O)c1ccc(C(C)(c2ccccc2)C(Br)OC)cc1 ','CCc1ccc(C(C)C(Br)OC)cc1 ','CCc1ccc(C(C#N)C(C)Oc2ccccc2)cc1 ','COC(c1ccccc1)C(C#N)(C#N)c1ccccc1 ','COC(=O)c1ccc(C(C)C(OC)Oc2ccccc2)cc1 ','N#CC(Oc1ccccc1)C(c1ccccc1)c1ccc([N+](=O)[O-])cc1 ','N#Cc1ccc(C(C#N)(C#N)C(Oc2ccccc2)c2ccccc2)cc1 ','COCC(C)(C#N)c1ccc([N+](=O)[O-])cc1 ','COC(C#N)C(c1ccccc1)(c1ccccc1)c1ccc(-c2ccccc2)cc1 ','COC(c1ccccc1)C(c1ccccc1)(c1ccccc1)c1ccc(C#N)cc1 ','COC(c1ccccc1)C(C)(C#N)c1ccc(C#N)cc1 ','CC(C)(c1ccccc1)C(C#N)Oc1ccccc1 ','COCOCCc1ccccc1 ','CC(c1ccc([N+](=O)[O-])cc1)C(Oc1ccccc1)c1ccccc1 ','COC(Oc1ccccc1)C(C#N)(c1ccccc1)c1ccc(C#N)cc1 ','COCC(C)(c1ccccc1)c1ccc(C(=O)OC)cc1 ','COCOC(C)C(C)(c1ccccc1)c1ccccc1 ','COCOC(OC)C(C)(C#N)c1ccccc1 ','COC(OC)C(C)(C#N)c1ccc(C#N)cc1 ','CC(C#N)(c1ccc(C#N)cc1)C(C#N)Oc1ccccc1 ','COC(=O)c1ccc(C(C)(C#N)C(OC)c2ccccc2)cc1 ',\n",
    "          'COCOC(c1ccccc1)C(C#N)(C#N)c1ccc(OC)cc1 ','COCOC(OC)C(C)c1ccc(C(=O)OC)cc1 ','CC(COc1ccccc1)(c1ccccc1)c1ccccc1 ','COC(=O)c1ccc(C(C#N)(c2ccccc2)C(C)OC)cc1 ','CCc1ccc(C(C#N)(c2ccccc2)C(OC)Oc2ccccc2)cc1 ','COCOC(C#N)C(C#N)c1ccc([N+](=O)[O-])cc1 ','CCc1ccc(C(C#N)(C#N)C(C)OC)cc1 ','CCc1ccc(C(C#N)(c2ccccc2)C(C#N)Oc2ccccc2)cc1 ','N#Cc1ccc(C(C#N)C(C#N)Oc2ccccc2)cc1 ','COCOCC(C#N)(C#N)c1ccc(C#N)cc1 ','COc1ccc(C(C#N)C(C#N)OC)cc1 ','N#CC(C#N)(COc1ccccc1)c1ccc([N+](=O)[O-])cc1 ','CC(C#N)(COc1ccccc1)c1ccc(C#N)cc1 ','CCc1ccc(C(C)(C)COC)cc1 ','CC(C)(c1ccc(-c2ccccc2)cc1)C(C#N)Oc1ccccc1 ','CCc1ccc(C(C#N)(C#N)C(Oc2ccccc2)c2ccccc2)cc1 ','COCOC(Cc1ccc(C#N)cc1)OC ','COCOC(c1ccccc1)C(c1ccccc1)c1ccccc1 ','COCOC(C#N)C(C)c1ccc(C(=O)OC)cc1 ','N#CC(C#N)(COc1ccccc1)c1ccc(-c2ccccc2)cc1 ','COC(C#N)C(C#N)(C#N)c1ccccc1 ','CCc1ccc(C(COC)c2ccccc2)cc1 ','CCc1ccc(C(C)(C)COCOC)cc1 ','COCOC(c1ccccc1)C(C#N)(C#N)c1ccccc1 ','N#CC(COc1ccccc1)(c1ccccc1)c1ccccc1 ','CCc1ccc(C(C)(C#N)C(Oc2ccccc2)c2ccccc2)cc1 ','CC(C#N)(COc1ccccc1)c1ccc([N+](=O)[O-])cc1 ','COC(=O)c1ccc(C(C#N)(c2ccccc2)C(C#N)Oc2ccccc2)cc1 ','COC(C#N)C(C#N)c1ccc([N+](=O)[O-])cc1 ','COCOC(Br)C(C)(C)c1ccccc1 ','COCOC(OC)C(C)c1ccc([N+](=O)[O-])cc1 ','COC(C#N)C(C#N)(c1ccccc1)c1ccc(C#N)cc1 ','COC(c1ccccc1)C(C#N)(c1ccccc1)c1ccc(C#N)cc1 ','N#CC(c1ccc(-c2ccccc2)cc1)C(Br)Oc1ccccc1 ','COC(=O)c1ccc(C(C#N)(COc2ccccc2)c2ccccc2)cc1 ','COCOC(c1ccccc1)C(C)(c1ccccc1)c1ccc(C(=O)OC)cc1 ','N#Cc1ccc(CC(Oc2ccccc2)c2ccccc2)cc1 ','COC(Br)C(C#N)(c1ccccc1)c1ccc([N+](=O)[O-])cc1 ','CCc1ccc(C(C#N)(c2ccccc2)C(C#N)OC)cc1 ','COC(Oc1ccccc1)C(C#N)(C#N)c1ccc([N+](=O)[O-])cc1 ','CCc1ccc(C(C#N)(c2ccccc2)C(C)OCOC)cc1 ','COC(OC)C(C)c1ccccc1 ','CCc1ccc(C(C)(C#N)C(C)OCOC)cc1 ','COCOC(C)Cc1ccc(C#N)cc1 ','CC(Oc1ccccc1)C(C#N)c1ccc(-c2ccccc2)cc1 ','COC(=O)c1ccc(C(C#N)(C#N)C(OC)c2ccccc2)cc1 ','COC(C#N)C(C)(C#N)c1ccccc1 ','COCOC(OC)C(C#N)c1ccc(C(=O)OC)cc1 ','COCOC(C)C(C)(C#N)c1ccccc1 ','COC(C)C(C#N)(C#N)c1ccc(C#N)cc1 ','COCOC(Br)C(c1ccccc1)c1ccc([N+](=O)[O-])cc1 ','COCCc1ccc([N+](=O)[O-])cc1 ','N#CC(c1ccccc1)(c1ccc([N+](=O)[O-])cc1)C(Br)Oc1ccccc1 ','COCOC(C#N)C(C)c1ccc([N+](=O)[O-])cc1 ','N#CC(C#N)(c1ccc([N+](=O)[O-])cc1)C(Oc1ccccc1)c1ccccc1 ','COCOC(OC)C(C)(C#N)c1ccc(C(=O)OC)cc1 ','COCC(C#N)c1ccc([N+](=O)[O-])cc1 ','COC(=O)c1ccc(C(C#N)(c2ccccc2)C(C)Oc2ccccc2)cc1 ','N#CC(Oc1ccccc1)C(C#N)(C#N)c1ccc([N+](=O)[O-])cc1 ','O=[N+]([O-])c1ccc(C(COc2ccccc2)c2ccccc2)cc1 ','CC(C#N)(c1ccc(C#N)cc1)C(Br)Oc1ccccc1 ','COCOC(C#N)C(c1ccccc1)(c1ccccc1)c1ccccc1 ','N#CC(C#N)(c1ccccc1)C(Br)Oc1ccccc1 ','COC(c1ccccc1)C(C)(c1ccccc1)c1ccc([N+](=O)[O-])cc1 ','COC(c1ccccc1)C(C)(C)c1ccccc1 ','COCOC(OC)C(C#N)(c1ccccc1)c1ccc(C(=O)OC)cc1 ','CC(c1ccccc1)(c1ccccc1)C(C#N)Oc1ccccc1 ','COC(=O)c1ccc(C(c2ccccc2)(c2ccccc2)C(OC)Oc2ccccc2)cc1 ','COCOCC(C#N)(C#N)c1ccc(OC)cc1 ','CCc1ccc(C(C#N)(c2ccccc2)C(OCOC)c2ccccc2)cc1 ','COCOC(C)C(C#N)c1ccc(C(=O)OC)cc1 ','COC(OC)C(c1ccccc1)(c1ccccc1)c1ccc([N+](=O)[O-])cc1 ','COC(OC)C(C#N)(C#N)c1ccc(-c2ccccc2)cc1 ','CCc1ccc(C(C)(c2ccccc2)C(Br)Oc2ccccc2)cc1 ','CCc1ccc(C(C)(c2ccccc2)C(OC)c2ccccc2)cc1 ','COCOC(C#N)C(c1ccccc1)(c1ccccc1)c1ccc([N+](=O)[O-])cc1 ','N#CC(C#N)(c1ccccc1)C(Oc1ccccc1)c1ccccc1 ','CCc1ccc(C(C)COC)cc1 ','N#Cc1ccc(C(c2ccccc2)(c2ccccc2)C(Br)Oc2ccccc2)cc1 ','CC(c1ccccc1)(c1ccc([N+](=O)[O-])cc1)C(C#N)Oc1ccccc1 ','COCOC(C#N)C(C#N)(C#N)c1ccc(-c2ccccc2)cc1 ','CCc1ccc(C(COC)(c2ccccc2)c2ccccc2)cc1 ','COCOC(C#N)C(C#N)c1ccccc1 ','CCc1ccc(C(C#N)(C#N)C(C#N)OC)cc1 ','COC(c1ccccc1)C(C)(C#N)c1ccccc1 ','CC(C#N)(c1ccc(-c2ccccc2)cc1)C(Br)Oc1ccccc1 ','COCOC(C)Cc1ccc(C(=O)OC)cc1 ','N#Cc1ccc(CCOc2ccccc2)cc1 ','COC(C#N)Cc1ccc(C#N)cc1 ','COC(C#N)C(C)(C#N)c1ccc(-c2ccccc2)cc1 ','CCc1ccc(C(COc2ccccc2)(c2ccccc2)c2ccccc2)cc1 ','CCc1ccc(C(c2ccccc2)C(OCOC)c2ccccc2)cc1 ','CCc1ccc(C(C)(C#N)C(OC)Oc2ccccc2)cc1 ','COc1ccc(C(C#N)(C#N)C(Oc2ccccc2)c2ccccc2)cc1 ','COCOC(c1ccccc1)C(C)(c1ccccc1)c1ccc(C#N)cc1 ','COCOC(c1ccccc1)C(C)(C#N)c1ccc(C(=O)OC)cc1 ','CCc1ccc(C(C)C(Br)OCOC)cc1 ','COC(C)C(c1ccccc1)(c1ccccc1)c1ccc([N+](=O)[O-])cc1 ','COC(Br)C(C)(C#N)c1ccccc1 ','COCC(c1ccccc1)c1ccccc1 ','COCOC(Br)C(C)(c1ccccc1)c1ccc(C(=O)OC)cc1 ','CCc1ccc(C(C#N)(C#N)C(Br)OC)cc1 ','c1ccc(CC(Oc2ccccc2)c2ccccc2)cc1 ','N#CC(c1ccccc1)(c1ccccc1)C(Oc1ccccc1)c1ccccc1 ','CC(COc1ccccc1)c1ccc(C#N)cc1 ','COCOC(c1ccccc1)C(C)(C#N)c1ccc(C#N)cc1 ','CC(c1ccccc1)(c1ccc([N+](=O)[O-])cc1)C(Oc1ccccc1)c1ccccc1 ','COCOC(c1ccccc1)C(c1ccccc1)(c1ccccc1)c1ccccc1 ','N#CC(Oc1ccccc1)C(C#N)(c1ccccc1)c1ccc(-c2ccccc2)cc1 ',\n",
    "          'CC(C)(c1ccc([N+](=O)[O-])cc1)C(C#N)Oc1ccccc1 ','CCc1ccc(C(COc2ccccc2)c2ccccc2)cc1 ','CCc1ccc(C(C#N)(c2ccccc2)C(OC)OCOC)cc1 ','CC(C#N)(c1ccc(-c2ccccc2)cc1)C(C#N)Oc1ccccc1 ','COC(C)C(c1ccccc1)c1ccc([N+](=O)[O-])cc1 ','N#Cc1ccc(C(C#N)(C#N)C(C#N)Oc2ccccc2)cc1 ','COc1ccc(C(C#N)C(C)OC)cc1 ','CC(Oc1ccccc1)C(C#N)(C#N)c1ccc(-c2ccccc2)cc1 ','CCc1ccc(CC(OC)c2ccccc2)cc1 ','COC(C#N)C(C)c1ccc(C#N)cc1 ','COCOC(C)C(c1ccccc1)c1ccc(C#N)cc1 ','COCOC(C)C(C)(C#N)c1ccc(OC)cc1 ','N#CC(Cc1ccccc1)Oc1ccccc1 ','COCOC(C)C(C)(C#N)c1ccc(C#N)cc1 ','COC(C)C(c1ccccc1)c1ccccc1 ','COCOC(c1ccccc1)C(C#N)(c1ccccc1)c1ccccc1 ','COC(C#N)C(C)(C)c1ccc([N+](=O)[O-])cc1 ','COCOC(C#N)C(C)(C)c1ccc(-c2ccccc2)cc1 ','COCC(c1ccccc1)c1ccc(C(=O)OC)cc1 ','COC(Oc1ccccc1)C(C)(C#N)c1ccccc1 ','COCC(c1ccccc1)(c1ccccc1)c1ccc([N+](=O)[O-])cc1 ','COC(Br)C(C)(C)c1ccccc1 ','COCOCCc1ccc([N+](=O)[O-])cc1 ','COCOC(OC)C(C#N)(c1ccccc1)c1ccc(-c2ccccc2)cc1 ','CC(C)(COc1ccccc1)c1ccc(C#N)cc1 ','COCOC(C#N)C(C)c1ccc(-c2ccccc2)cc1 ','COC(c1ccccc1)C(C)(C)c1ccc(C#N)cc1 ','COC(=O)c1ccc(C(C)C(OC)c2ccccc2)cc1 ','COC(c1ccccc1)C(C)(c1ccccc1)c1ccccc1 ','COC(c1ccccc1)C(C)(C#N)c1ccc([N+](=O)[O-])cc1 ','COC(Br)C(C#N)(C#N)c1ccc(C#N)cc1 ','CCc1ccc(C(c2ccccc2)C(OC)OC)cc1 ','CCc1ccc(C(C)(C)C(C)OC)cc1 ','COCOC(c1ccccc1)C(C#N)(C#N)c1ccc(C#N)cc1 ','COC(C#N)Cc1ccccc1 ','COCC(C)c1ccc(C(=O)OC)cc1 ','COc1ccc(C(C)(C#N)C(OC)Oc2ccccc2)cc1 ','COCOC(Br)C(C#N)c1ccc(C(=O)OC)cc1 ','COCOCC(C#N)c1ccc(-c2ccccc2)cc1 ','CC(c1ccccc1)(c1ccccc1)C(Oc1ccccc1)c1ccccc1 ','CCc1ccc(C(C#N)(c2ccccc2)C(Oc2ccccc2)c2ccccc2)cc1 ','COCOC(OC)C(C)c1ccccc1 ','COC(=O)c1ccc(C(C)(C)C(C)OC)cc1 ','COCOC(c1ccccc1)C(C#N)(c1ccccc1)c1ccc(C(=O)OC)cc1 ','CCc1ccc(C(C)(COc2ccccc2)c2ccccc2)cc1 ','CCc1ccc(C(C#N)C(OC)c2ccccc2)cc1 ','CC(c1ccccc1)C(C#N)Oc1ccccc1 ','COC(=O)c1ccc(C(C)(C)C(Br)OC)cc1 ','COC(=O)c1ccc(C(C)(C)C(Oc2ccccc2)c2ccccc2)cc1 ','COC(Oc1ccccc1)C(C)(C#N)c1ccc(-c2ccccc2)cc1 ','N#Cc1ccc(C(C#N)(C#N)C(Br)Oc2ccccc2)cc1 ','COCOC(C#N)C(C#N)(C#N)c1ccc([N+](=O)[O-])cc1 ','CC(C#N)(c1ccc([N+](=O)[O-])cc1)C(C#N)Oc1ccccc1 ','COCOCC(C)(c1ccccc1)c1ccccc1 ','COc1ccc(C(C#N)(c2ccccc2)C(Br)OC)cc1 ','COC(Br)C(C)c1ccc([N+](=O)[O-])cc1 ','COCOC(C#N)C(C)(c1ccccc1)c1ccc([N+](=O)[O-])cc1 ','COCOCC(C)(C#N)c1ccc(OC)cc1 ','CCc1ccc(C(C#N)C(OCOC)c2ccccc2)cc1 ','COCOC(Br)C(C)(C)c1ccc([N+](=O)[O-])cc1 ','COCOC(c1ccccc1)C(C#N)(c1ccccc1)c1ccc(OC)cc1 ','COCC(c1ccccc1)c1ccc([N+](=O)[O-])cc1 ','N#Cc1ccc(CC(C#N)Oc2ccccc2)cc1 ','COCOC(c1ccccc1)C(C#N)(C#N)c1ccc([N+](=O)[O-])cc1 ','CCc1ccc(C(c2ccccc2)C(C)OC)cc1 ','COc1ccc(C(C#N)(c2ccccc2)C(OC)Oc2ccccc2)cc1 ','COC(C#N)C(C#N)(C#N)c1ccc(-c2ccccc2)cc1 ','CC(Oc1ccccc1)C(c1ccccc1)(c1ccccc1)c1ccc([N+](=O)[O-])cc1 ','CCc1ccc(C(C#N)(c2ccccc2)C(C)OC)cc1 ','COC(=O)c1ccc(C(c2ccccc2)C(Br)OC)cc1 ','COCOC(OC)C(C)(C)c1ccc(C(=O)OC)cc1 ','CCc1ccc(C(C#N)(C#N)C(OC)c2ccccc2)cc1 ','COCOC(Br)C(C)(c1ccccc1)c1ccc([N+](=O)[O-])cc1 ','COC(=O)c1ccc(C(C#N)(C#N)C(C#N)OC)cc1 ','COCOC(OC)C(C)(C#N)c1ccc(C#N)cc1 ','COC(Oc1ccccc1)C(c1ccccc1)c1ccc([N+](=O)[O-])cc1 ','COC(Br)C(C#N)c1ccc(C#N)cc1 ','COC(=O)c1ccc(C(C#N)C(OC)OC)cc1 ','COCOC(C)C(C#N)(c1ccccc1)c1ccc([N+](=O)[O-])cc1 ','COCOC(c1ccccc1)C(c1ccccc1)c1ccc(C#N)cc1 ','COc1ccc(C(C#N)C(Br)Oc2ccccc2)cc1 ','COCC(C#N)(c1ccccc1)c1ccc(C#N)cc1 ','COCOC(C#N)C(C#N)c1ccc(-c2ccccc2)cc1 ','COCOC(c1ccccc1)C(C)(c1ccccc1)c1ccc([N+](=O)[O-])cc1 ','COC(=O)c1ccc(C(c2ccccc2)(c2ccccc2)C(OC)OC)cc1 ','CC(c1ccc(-c2ccccc2)cc1)C(C#N)Oc1ccccc1 ','COC(=O)c1ccc(C(C)(C#N)C(Oc2ccccc2)c2ccccc2)cc1 ','CCc1ccc(C(C#N)(C#N)C(OC)Oc2ccccc2)cc1 ','COC(=O)c1ccc(C(c2ccccc2)(c2ccccc2)C(C)OC)cc1 ','COCOC(c1ccccc1)C(C#N)c1ccc([N+](=O)[O-])cc1 ','CC(c1ccc(C#N)cc1)C(Br)Oc1ccccc1 ','COCOC(C#N)C(C#N)(c1ccccc1)c1ccc([N+](=O)[O-])cc1 ','N#Cc1ccc(C(C#N)(c2ccccc2)C(Oc2ccccc2)c2ccccc2)cc1 ','CCc1ccc(CC(C)OCOC)cc1 ','COC(=O)c1ccc(C(C#N)(c2ccccc2)C(Br)Oc2ccccc2)cc1 ','COC(=O)c1ccc(C(C#N)(C#N)C(C)OC)cc1 ','O=[N+]([O-])c1ccc(CC(Br)Oc2ccccc2)cc1 ','COC(=O)c1ccc(CC(C#N)Oc2ccccc2)cc1 ','COCOC(OC)C(c1ccccc1)(c1ccccc1)c1ccc(C(=O)OC)cc1 ','COCC(C)(C)c1ccccc1 ','COC(c1ccccc1)C(C#N)(c1ccccc1)c1ccccc1 ','COCC(C)c1ccc([N+](=O)[O-])cc1 ','COCOC(C#N)C(c1ccccc1)c1ccccc1 ','CCc1ccc(C(C#N)COCOC)cc1 ','COC(C#N)C(C#N)(c1ccccc1)c1ccc([N+](=O)[O-])cc1 ','CCc1ccc(C(C#N)(C#N)C(C#N)Oc2ccccc2)cc1 ','COCOC(Br)Cc1ccc(C(=O)OC)cc1 ','COCC(C#N)c1ccc(-c2ccccc2)cc1 ','COCOCC(C#N)c1ccc(C(=O)OC)cc1 ','CCc1ccc(C(C#N)(c2ccccc2)C(OC)c2ccccc2)cc1 ','COCCc1ccc(C#N)cc1 ','COC(c1ccccc1)C(C#N)(c1ccccc1)c1ccc(-c2ccccc2)cc1 ',\n",
    "\t\t  'COc1ccc(C(C)(C#N)C(OC)c2ccccc2)cc1 ','COCOC(c1ccccc1)C(C)c1ccc([N+](=O)[O-])cc1 ','CCc1ccc(C(c2ccccc2)(c2ccccc2)C(OC)OCOC)cc1 ','COC(Oc1ccccc1)C(C)(c1ccccc1)c1ccccc1 ','COCOCC(C)(C#N)c1ccc([N+](=O)[O-])cc1 ','COC(OC)C(C)(c1ccccc1)c1ccccc1 ','CCc1ccc(CC(Br)OCOC)cc1 ','COC(OC)C(C)(c1ccccc1)c1ccc(C#N)cc1 ','COC(Br)C(C)(c1ccccc1)c1ccc(C#N)cc1 ','COC(=O)c1ccc(C(C)(C#N)C(C)OC)cc1 ','COC(c1ccccc1)C(C)c1ccc([N+](=O)[O-])cc1 ','CC(Oc1ccccc1)C(C#N)(c1ccccc1)c1ccc([N+](=O)[O-])cc1 ','COC(Br)Cc1ccc(C#N)cc1 ','COC(=O)c1ccc(C(COc2ccccc2)(c2ccccc2)c2ccccc2)cc1 ','COC(Br)C(C#N)c1ccc(-c2ccccc2)cc1 ','CCc1ccc(C(C#N)(C#N)C(Br)Oc2ccccc2)cc1 ','CCc1ccc(C(C)(C#N)C(C#N)Oc2ccccc2)cc1 ','COC(OC)C(c1ccccc1)c1ccc(C#N)cc1 ','COCOC(Br)C(C#N)(C#N)c1ccc(-c2ccccc2)cc1 ','CCc1ccc(C(c2ccccc2)C(Br)OC)cc1 ','COc1ccc(C(C#N)(c2ccccc2)C(C)OC)cc1 ','CCc1ccc(C(c2ccccc2)(c2ccccc2)C(C)OC)cc1 ','CCc1ccc(C(C)(C#N)C(OC)c2ccccc2)cc1 ','COC(OC)C(C#N)(C#N)c1ccc(C#N)cc1 ','COCOC(Br)Cc1ccc([N+](=O)[O-])cc1 ','CCc1ccc(C(c2ccccc2)C(Br)Oc2ccccc2)cc1 ','COC(=O)c1ccc(C(c2ccccc2)C(OC)c2ccccc2)cc1 ','COCOC(c1ccccc1)C(C)(c1ccccc1)c1ccccc1 ','CCc1ccc(C(c2ccccc2)(c2ccccc2)C(Oc2ccccc2)c2ccccc2)cc1 ','CCc1ccc(C(C#N)(COCOC)c2ccccc2)cc1 ',\n",
    "          'COCC(C)(C)c1ccc(C(=O)OC)cc1 ','COCOC(c1ccccc1)C(C)(C)c1ccc(C#N)cc1 ','CCc1ccc(C(C)C(Br)Oc2ccccc2)cc1 ','COC(=O)c1ccc(C(C#N)C(C)Oc2ccccc2)cc1 ','CC(c1ccccc1)(c1ccc([N+](=O)[O-])cc1)C(Br)Oc1ccccc1 ','COC(Br)C(C#N)(C#N)c1ccc([N+](=O)[O-])cc1 ','COc1ccc(C(C#N)C(OC)OC)cc1 ','COC(=O)c1ccc(CC(Oc2ccccc2)c2ccccc2)cc1 ','CC(Oc1ccccc1)C(C#N)(C#N)c1ccc(C#N)cc1 ','BrC(Oc1ccccc1)C(c1ccccc1)c1ccc(-c2ccccc2)cc1 ','CC(C#N)(c1ccccc1)C(Br)Oc1ccccc1 ','COCOC(OC)C(C#N)(C#N)c1ccccc1 ','COC(C)C(C#N)(c1ccccc1)c1ccc([N+](=O)[O-])cc1 ','N#CC(c1ccc([N+](=O)[O-])cc1)C(Oc1ccccc1)c1ccccc1 ','COC(=O)c1ccc(C(C)(C#N)COc2ccccc2)cc1 ','COC(Oc1ccccc1)C(C#N)c1ccc(C#N)cc1 ','CCc1ccc(C(C)(C)C(Br)Oc2ccccc2)cc1 ','COC(=O)c1ccc(C(C)(C#N)C(C#N)Oc2ccccc2)cc1 ','COCC(C)(C)c1ccc([N+](=O)[O-])cc1 ','COc1ccc(C(C)(C#N)C(C#N)OC)cc1 ','COc1ccc(C(C#N)(C#N)C(OC)OC)cc1 ','COC(=O)c1ccc(C(C)(c2ccccc2)C(OC)Oc2ccccc2)cc1 ','COCOC(C#N)Cc1ccccc1 ','COCCc1ccccc1 ','COCOC(OC)C(C)(C#N)c1ccc([N+](=O)[O-])cc1 ','COC(Oc1ccccc1)C(C)(C#N)c1ccc(C#N)cc1 ','COCOC(Br)C(c1ccccc1)(c1ccccc1)c1ccc(-c2ccccc2)cc1 ','COCOC(c1ccccc1)C(C)(C#N)c1ccc(-c2ccccc2)cc1 ','BrC(Cc1ccc(-c2ccccc2)cc1)Oc1ccccc1 ','CCc1ccc(CC(OC)OCOC)cc1 ','CCc1ccc(C(c2ccccc2)(c2ccccc2)C(C)OCOC)cc1 ','COC(OC)C(C#N)c1ccc(C#N)cc1 ','CCc1ccc(C(C)(c2ccccc2)C(C#N)OC)cc1 ','O=[N+]([O-])c1ccc(C(c2ccccc2)C(Oc2ccccc2)c2ccccc2)cc1 ','CCc1ccc(C(C#N)(C#N)COC)cc1 ','COC(=O)c1ccc(C(c2ccccc2)C(C)OC)cc1 ','COCOC(c1ccccc1)C(C#N)(c1ccccc1)c1ccc(C#N)cc1 ','CC(Oc1ccccc1)C(C)(c1ccccc1)c1ccc([N+](=O)[O-])cc1 ','N#Cc1ccc(C(COc2ccccc2)(c2ccccc2)c2ccccc2)cc1 ','COCOCC(C)(C#N)c1ccc(C#N)cc1 ','COCOCC(C#N)c1ccc(C#N)cc1 ','CCc1ccc(C(c2ccccc2)(c2ccccc2)C(Br)OCOC)cc1 ','COC(=O)c1ccc(C(C)(COc2ccccc2)c2ccccc2)cc1 ','COCOC(C#N)C(C#N)(c1ccccc1)c1ccc(OC)cc1 ','COC(Br)C(c1ccccc1)(c1ccccc1)c1ccc([N+](=O)[O-])cc1 ','CCc1ccc(C(C)(c2ccccc2)C(OC)OCOC)cc1 ','COCC(C#N)(c1ccccc1)c1ccc(OC)cc1 ','COCOCC(C#N)(c1ccccc1)c1ccc(C(=O)OC)cc1 ','COCOC(Br)C(C#N)(c1ccccc1)c1ccc(C(=O)OC)cc1 ','CCc1ccc(C(COCOC)(c2ccccc2)c2ccccc2)cc1 ','COCOC(Br)C(C#N)(C#N)c1ccc(C(=O)OC)cc1 ','COC(OC)C(C#N)(C#N)c1ccccc1 ','COCOC(C#N)C(C)(C)c1ccccc1 ','COC(C#N)C(C)(c1ccccc1)c1ccc([N+](=O)[O-])cc1 ','COC(C)Cc1ccc([N+](=O)[O-])cc1 ','COCOC(C)C(C#N)(c1ccccc1)c1ccc(-c2ccccc2)cc1 ','COC(C#N)Cc1ccc(-c2ccccc2)cc1 ','COc1ccc(C(C#N)(C#N)C(C)Oc2ccccc2)cc1 ','COC(=O)c1ccc(C(C)(C)COc2ccccc2)cc1 ','N#CC(c1ccccc1)(c1ccccc1)C(Br)Oc1ccccc1 ','COCC(C#N)(c1ccccc1)c1ccc(-c2ccccc2)cc1 ','COCOC(C#N)C(C#N)(c1ccccc1)c1ccc(-c2ccccc2)cc1 ','CC(C#N)(COc1ccccc1)c1ccccc1 ','CCc1ccc(C(C#N)(C#N)C(C)Oc2ccccc2)cc1 ','COC(OC)C(C)(c1ccccc1)c1ccc([N+](=O)[O-])cc1 ','COc1ccc(C(C#N)(C#N)C(C#N)Oc2ccccc2)cc1 ','COCOC(Br)C(C#N)c1ccc(OC)cc1 ','COCOC(C#N)C(C#N)(C#N)c1ccc(OC)cc1 ','COCC(C#N)(C#N)c1ccc(-c2ccccc2)cc1 ','COC(Oc1ccccc1)C(C#N)c1ccc([N+](=O)[O-])cc1 ','COCOC(C#N)C(C#N)c1ccc(C#N)cc1 ','CCc1ccc(C(C#N)(c2ccccc2)C(Br)OC)cc1 ','N#CC(Oc1ccccc1)C(C#N)c1ccccc1 ','CCc1ccc(C(C)(C#N)C(Br)OCOC)cc1 ','COC(c1ccccc1)C(c1ccccc1)(c1ccccc1)c1ccccc1 ','COCC(C#N)c1ccc(OC)cc1 ','COC(=O)c1ccc(C(C)COc2ccccc2)cc1 ','COC(C#N)C(C#N)(c1ccccc1)c1ccccc1 ','COCC(C#N)c1ccc(C(=O)OC)cc1 ','COCOC(OC)C(C)(c1ccccc1)c1ccccc1 ','c1ccc(OC(c2ccccc2)C(c2ccccc2)(c2ccccc2)c2ccccc2)cc1 ','CCc1ccc(C(C)(C)C(OC)c2ccccc2)cc1 ','COC(C)C(C)(C)c1ccccc1 ','CCc1ccc(C(C#N)C(Br)Oc2ccccc2)cc1 ','COCOC(Br)C(C#N)(C#N)c1ccc(OC)cc1 ','COC(Cc1ccccc1)Oc1ccccc1 ','COc1ccc(C(C#N)(COc2ccccc2)c2ccccc2)cc1 ','COC(c1ccccc1)C(C#N)(c1ccccc1)c1ccc([N+](=O)[O-])cc1 ','COC(Br)C(C#N)(c1ccccc1)c1ccccc1 ','CC(Oc1ccccc1)C(C)(C#N)c1ccccc1 ','COCOC(Br)C(C#N)c1ccccc1 ','COCOC(Br)C(c1ccccc1)c1ccc(C(=O)OC)cc1 ','COCOC(C)C(C)(C)c1ccccc1 ','COCOC(C)C(C#N)c1ccc(C#N)cc1 ','COC(Oc1ccccc1)C(c1ccccc1)(c1ccccc1)c1ccc(C#N)cc1 ','COCOCC(c1ccccc1)(c1ccccc1)c1ccc(C(=O)OC)cc1 ','COCOC(c1ccccc1)C(C)(C#N)c1ccc([N+](=O)[O-])cc1 ','COCOC(c1ccccc1)C(C)c1ccccc1 ','COCOC(OC)C(c1ccccc1)(c1ccccc1)c1ccc([N+](=O)[O-])cc1 ','COCOCC(C)c1ccc(C#N)cc1 ','COCOCCc1ccc(C#N)cc1 ','N#CC(COc1ccccc1)c1ccc([N+](=O)[O-])cc1 ','COCOC(Br)C(C#N)c1ccc(-c2ccccc2)cc1 ','COC(=O)c1ccc(C(C#N)COc2ccccc2)cc1 ','CC(C#N)(c1ccc([N+](=O)[O-])cc1)C(Br)Oc1ccccc1 ','COC(Br)C(c1ccccc1)c1ccc(C#N)cc1 ','COCOC(C)C(C#N)(c1ccccc1)c1ccc(C#N)cc1 ','COC(=O)c1ccc(C(C#N)C(OC)c2ccccc2)cc1 ','CC(Oc1ccccc1)C(c1ccccc1)(c1ccccc1)c1ccccc1 ','COC(Cc1ccccc1)c1ccccc1 ','COC(=O)c1ccc(C(C#N)(C#N)COc2ccccc2)cc1 ','COCOC(C#N)Cc1ccc(C#N)cc1 ','COC(OC)C(C#N)(c1ccccc1)c1ccc(C#N)cc1 ',\n",
    "          'N#CC(C#N)(c1ccc(-c2ccccc2)cc1)C(Br)Oc1ccccc1 ','COC(=O)c1ccc(C(c2ccccc2)(c2ccccc2)C(Br)OC)cc1 ','COC(c1ccccc1)C(C)(C)c1ccc([N+](=O)[O-])cc1 ','COCC(C#N)c1ccccc1 ','COc1ccc(CC(C#N)OC)cc1 ','CC(COc1ccccc1)(c1ccccc1)c1ccc(C#N)cc1 ','COCOC(OC)C(C)(c1ccccc1)c1ccc(C(=O)OC)cc1 ','N#CC(Oc1ccccc1)C(C#N)(C#N)c1ccc(-c2ccccc2)cc1 ','COCOCC(C)(C#N)c1ccccc1 ','COC(C)C(C)c1ccc([N+](=O)[O-])cc1 ','N#Cc1ccc(C(C#N)COc2ccccc2)cc1 ','CCc1ccc(C(c2ccccc2)C(C)OCOC)cc1 ','CCc1ccc(CC(Br)OC)cc1 ','COCOC(Br)C(c1ccccc1)(c1ccccc1)c1ccc([N+](=O)[O-])cc1 ','COCOC(Br)C(C#N)(c1ccccc1)c1ccc(OC)cc1 ','COc1ccc(C(C#N)(c2ccccc2)C(C#N)Oc2ccccc2)cc1 ','COC(=O)c1ccc(CC(OC)OC)cc1 ','N#CC(c1ccccc1)(c1ccc(-c2ccccc2)cc1)C(Oc1ccccc1)c1ccccc1 ','CC(c1ccccc1)(c1ccccc1)C(Br)Oc1ccccc1 ','COCOC(OC)C(C#N)c1ccc(OC)cc1 ','COCOC(C#N)Cc1ccc([N+](=O)[O-])cc1 ','COC(=O)c1ccc(C(c2ccccc2)(c2ccccc2)C(OC)c2ccccc2)cc1 ','COC(C)C(c1ccccc1)(c1ccccc1)c1ccccc1 ','COC(OC)C(C#N)(c1ccccc1)c1ccc([N+](=O)[O-])cc1 ','CCc1ccc(C(C#N)C(C#N)OC)cc1 ','COC(Oc1ccccc1)C(C#N)c1ccc(-c2ccccc2)cc1 ','COCOC(C)C(C)(c1ccccc1)c1ccc(C#N)cc1 ','COCOC(C#N)C(c1ccccc1)c1ccc(C#N)cc1 ','COCOC(c1ccccc1)C(C#N)c1ccc(OC)cc1 ','COCOC(c1ccccc1)C(c1ccccc1)(c1ccccc1)c1ccc(C(=O)OC)cc1 ','COCOC(C)C(c1ccccc1)(c1ccccc1)c1ccc(C(=O)OC)cc1 ','N#CC(c1ccccc1)C(Oc1ccccc1)c1ccccc1 ','CCc1ccc(C(c2ccccc2)(c2ccccc2)C(Br)OC)cc1 ','COCOC(c1ccccc1)C(C#N)c1ccc(C(=O)OC)cc1 ','COC(Oc1ccccc1)C(C)c1ccc([N+](=O)[O-])cc1 ','N#Cc1ccc(C(C#N)(C#N)COc2ccccc2)cc1 ','CC(c1ccc(C#N)cc1)C(C#N)Oc1ccccc1 ','COC(C#N)C(C#N)(C#N)c1ccc([N+](=O)[O-])cc1 ','COCOCC(c1ccccc1)c1ccc(C(=O)OC)cc1 ','COCOC(OC)C(c1ccccc1)c1ccc(C#N)cc1 ','CCc1ccc(C(C#N)(COc2ccccc2)c2ccccc2)cc1 ','CC(C)(c1ccc(-c2ccccc2)cc1)C(Br)Oc1ccccc1 ','COCOC(C)C(C)(C#N)c1ccc([N+](=O)[O-])cc1 ','CCc1ccc(C(C#N)(C#N)C(OC)OCOC)cc1 ','COCOC(Br)C(C)(C#N)c1ccc(C(=O)OC)cc1 ','COCOC(OC)C(C)(C)c1ccccc1 ','CCc1ccc(C(C)(C#N)C(Br)OC)cc1 ','CCc1ccc(C(C)(c2ccccc2)C(OC)Oc2ccccc2)cc1 ','COC(=O)c1ccc(C(C#N)(c2ccccc2)C(OC)Oc2ccccc2)cc1 ','COCOC(OC)C(c1ccccc1)(c1ccccc1)c1ccccc1 ','COCOCC(C#N)(c1ccccc1)c1ccc(OC)cc1 ','COc1ccc(C(C#N)C(OC)Oc2ccccc2)cc1 ','COC(Oc1ccccc1)C(C)(c1ccccc1)c1ccc(C#N)cc1 ','CCc1ccc(C(C#N)C(C#N)Oc2ccccc2)cc1 ','COC(=O)c1ccc(C(C)(c2ccccc2)C(Oc2ccccc2)c2ccccc2)cc1 ','CC(C)(c1ccc([N+](=O)[O-])cc1)C(Br)Oc1ccccc1 ','COC(c1ccccc1)C(C#N)(C#N)c1ccc(C#N)cc1 ','COCOC(C)C(C#N)(c1ccccc1)c1ccc(C(=O)OC)cc1 ','COCOC(C)C(C#N)c1ccc(OC)cc1 ','CCc1ccc(CC(Br)Oc2ccccc2)cc1 ','N#CC(c1ccc([N+](=O)[O-])cc1)C(Br)Oc1ccccc1 ','COC(=O)c1ccc(C(C)(C#N)C(OC)Oc2ccccc2)cc1 ','COCOC(C#N)C(C)c1ccc(C#N)cc1 ','CCc1ccc(C(C#N)C(C)OC)cc1 ','COc1ccc(C(C#N)(c2ccccc2)C(Oc2ccccc2)c2ccccc2)cc1 ','COCOC(C#N)C(c1ccccc1)c1ccc(C(=O)OC)cc1 ','N#CC(Oc1ccccc1)C(C#N)c1ccc([N+](=O)[O-])cc1 ','CC(C#N)(c1ccccc1)C(C#N)Oc1ccccc1 ','COCOC(Br)C(C)c1ccccc1 ','COCOC(C)C(c1ccccc1)c1ccc([N+](=O)[O-])cc1 ','CC(C#N)(c1ccc(C#N)cc1)C(Oc1ccccc1)c1ccccc1 ','CCc1ccc(C(C)(C)C(OC)OC)cc1 ','COC(Br)C(C)(c1ccccc1)c1ccc([N+](=O)[O-])cc1 ','COC(C#N)C(C)(C)c1ccc(C#N)cc1 ','COCOC(Cc1ccc(C#N)cc1)c1ccccc1 ','O=[N+]([O-])c1ccc(C(c2ccccc2)(c2ccccc2)C(Br)Oc2ccccc2)cc1 ','COc1ccc(C(C)(C#N)C(Br)OC)cc1 ','COCOC(OC)C(C#N)c1ccc([N+](=O)[O-])cc1 ','CC(COc1ccccc1)(c1ccccc1)c1ccc([N+](=O)[O-])cc1 ','N#CC(COc1ccccc1)(c1ccccc1)c1ccc(-c2ccccc2)cc1 ','COCOC(OC)C(C#N)c1ccc(C#N)cc1 ','COC(=O)c1ccc(C(C#N)(C#N)C(C#N)Oc2ccccc2)cc1 ','COCOC(c1ccccc1)C(C#N)(C#N)c1ccc(C(=O)OC)cc1 ','CC(c1ccc([N+](=O)[O-])cc1)C(Br)Oc1ccccc1 ','COCOC(C#N)C(C)(C)c1ccc([N+](=O)[O-])cc1 ','COCOC(Br)C(C)(c1ccccc1)c1ccc(C#N)cc1 ','COCC(C)(C#N)c1ccc(C(=O)OC)cc1 ','CC(Oc1ccccc1)C(C#N)c1ccc([N+](=O)[O-])cc1 ','COCOC(C)C(C)c1ccc(C#N)cc1 ','COCOC(Br)Cc1ccccc1 ','COCOC(Br)C(C)c1ccc(C#N)cc1 ','COC(C#N)C(c1ccccc1)(c1ccccc1)c1ccc(C#N)cc1 ','COCOC(C#N)C(c1ccccc1)c1ccc([N+](=O)[O-])cc1 ','COC(=O)c1ccc(C(c2ccccc2)C(Br)Oc2ccccc2)cc1 ','COCOCC(c1ccccc1)(c1ccccc1)c1ccc(C#N)cc1 ','CC(c1ccccc1)(c1ccc(C#N)cc1)C(Oc1ccccc1)c1ccccc1 ','COC(C#N)C(C)(c1ccccc1)c1ccccc1 ','COC(=O)c1ccc(C(C)(C)C(OC)c2ccccc2)cc1 ','COC(=O)c1ccc(C(C#N)C(C#N)Oc2ccccc2)cc1 ','CCc1ccc(CC(OC)OC)cc1 ','COCOC(C#N)C(C)(c1ccccc1)c1ccc(C(=O)OC)cc1 ','CC(C#N)(c1ccc([N+](=O)[O-])cc1)C(Oc1ccccc1)c1ccccc1 ','COC(C#N)C(c1ccccc1)c1ccccc1 ','CCc1ccc(C(C)(c2ccccc2)C(C)OCOC)cc1 ','N#CC(Oc1ccccc1)C(c1ccccc1)(c1ccccc1)c1ccccc1 ','CC(Oc1ccccc1)C(c1ccccc1)(c1ccccc1)c1ccc(C#N)cc1 ','COC(c1ccccc1)C(C)(c1ccccc1)c1ccc(C#N)cc1 ','COC(=O)c1ccc(C(C#N)(c2ccccc2)C(Oc2ccccc2)c2ccccc2)cc1 ',\n",
    "\t\t  'CCc1ccc(C(C)(C#N)C(OC)OC)cc1 ','COC(OC)C(C)c1ccc(C#N)cc1 ','CCc1ccc(C(C)(c2ccccc2)C(C)OC)cc1 ','COCOC(C#N)Cc1ccc(C(=O)OC)cc1 ','COc1ccc(C(C#N)(c2ccccc2)C(Br)Oc2ccccc2)cc1 ','CC(c1ccccc1)(c1ccc(-c2ccccc2)cc1)C(Br)Oc1ccccc1 ','COc1ccc(C(C)(C#N)C(C#N)Oc2ccccc2)cc1 ','CC(C)(c1ccc([N+](=O)[O-])cc1)C(Oc1ccccc1)c1ccccc1 ','N#Cc1ccc(C(C#N)C(Oc2ccccc2)c2ccccc2)cc1 ','COCOC(Br)C(c1ccccc1)(c1ccccc1)c1ccc(C(=O)OC)cc1 ','COC(C#N)C(C)(C#N)c1ccc([N+](=O)[O-])cc1 ','COC(Br)C(C)(C)c1ccc(-c2ccccc2)cc1 ','COc1ccc(C(C#N)(C#N)C(OC)Oc2ccccc2)cc1 ','CCc1ccc(C(C)C(C)OCOC)cc1 ','COC(C#N)C(C)(C#N)c1ccc(C#N)cc1 ','COCOC(C)C(C)(C)c1ccc(C#N)cc1 ','CCc1ccc(C(C#N)(c2ccccc2)C(OC)OC)cc1 ','CC(Oc1ccccc1)C(C#N)(c1ccccc1)c1ccc(C#N)cc1 ','COCOC(Br)C(C)c1ccc(-c2ccccc2)cc1 ','COC(Br)C(C)(C#N)c1ccc(C#N)cc1 ','N#CC(Oc1ccccc1)C(C#N)(C#N)c1ccccc1 ','CCc1ccc(CC(C#N)OCOC)cc1 ','COC(Oc1ccccc1)C(C#N)(C#N)c1ccc(C#N)cc1 ','COCOCC(C#N)(c1ccccc1)c1ccc(-c2ccccc2)cc1 ','COC(=O)c1ccc(C(C#N)C(OC)Oc2ccccc2)cc1 ','COCOCC(c1ccccc1)(c1ccccc1)c1ccc([N+](=O)[O-])cc1 ','COCOC(C)C(C#N)c1ccccc1 ','COC(C)Cc1ccc(C#N)cc1 ','N#Cc1ccc(CC(Br)Oc2ccccc2)cc1 ','N#CC(COc1ccccc1)c1ccccc1 ','COCOC(C#N)C(c1ccccc1)(c1ccccc1)c1ccc(C(=O)OC)cc1 ','CC(C#N)(COc1ccccc1)c1ccc(-c2ccccc2)cc1 ',\n",
    "          'CC(c1ccc(-c2ccccc2)cc1)C(Br)Oc1ccccc1 ','COCOC(C#N)C(C#N)(c1ccccc1)c1ccc(C(=O)OC)cc1 ','COCOCC(c1ccccc1)c1ccc([N+](=O)[O-])cc1 ','COC(C#N)C(C)(c1ccccc1)c1ccc(C#N)cc1 ','COC(Cc1ccc(C#N)cc1)OC ','COCOC(c1ccccc1)C(C#N)c1ccc(-c2ccccc2)cc1 ','COCOC(c1ccccc1)C(C#N)c1ccccc1 ','N#CC(Cc1ccc([N+](=O)[O-])cc1)Oc1ccccc1 ','COCOCC(C#N)(C#N)c1ccc(C(=O)OC)cc1 ','COC(c1ccccc1)C(C)c1ccc(C#N)cc1 ','COC(OC)C(c1ccccc1)c1ccc([N+](=O)[O-])cc1 ','COCOC(OC)C(C#N)(C#N)c1ccc(C#N)cc1 ','COC(=O)c1ccc(C(c2ccccc2)(c2ccccc2)C(C#N)Oc2ccccc2)cc1 ','CCc1ccc(C(C)(C)C(OC)OCOC)cc1 ','COc1ccc(C(C#N)(c2ccccc2)C(OC)c2ccccc2)cc1 ','COC(C)C(C)(C#N)c1ccc(C#N)cc1 ','N#CC(c1ccc(-c2ccccc2)cc1)C(Oc1ccccc1)c1ccccc1 ','COCOC(Br)C(C#N)c1ccc([N+](=O)[O-])cc1 ','COCC(C#N)(c1ccccc1)c1ccc([N+](=O)[O-])cc1 ','COCC(C#N)(C#N)c1ccccc1 ','COCOC(C)C(c1ccccc1)(c1ccccc1)c1ccccc1 ','COCOC(Br)C(C)(c1ccccc1)c1ccccc1 ','COCOCC(C)(C#N)c1ccc(-c2ccccc2)cc1 ','COC(OC)C(c1ccccc1)(c1ccccc1)c1ccccc1 ','COc1ccc(C(C)(C#N)C(C)OC)cc1 ','CCc1ccc(C(C)(c2ccccc2)C(Oc2ccccc2)c2ccccc2)cc1 ','COCOC(c1ccccc1)C(C#N)c1ccc(C#N)cc1 ','CCc1ccc(C(c2ccccc2)(c2ccccc2)C(OC)c2ccccc2)cc1 ','CC(C)(c1ccccc1)C(Br)Oc1ccccc1 ','CCc1ccc(C(C)(c2ccccc2)C(C#N)Oc2ccccc2)cc1 ','COCOCC(C#N)(C#N)c1ccc(-c2ccccc2)cc1 ','COC(C#N)C(C)c1ccccc1 ','COCOC(OC)C(C#N)(c1ccccc1)c1ccccc1 ','COC(=O)c1ccc(C(C)(c2ccccc2)C(C#N)Oc2ccccc2)cc1 ','CCc1ccc(C(C)C(C#N)Oc2ccccc2)cc1 ','COC(C)C(C#N)c1ccc([N+](=O)[O-])cc1 ','COC(C)C(C)(C)c1ccc(C#N)cc1 ','COC(=O)c1ccc(C(c2ccccc2)C(OC)OC)cc1 ','CCc1ccc(C(c2ccccc2)C(C#N)Oc2ccccc2)cc1 ','COC(OC)C(C)(C#N)c1ccc([N+](=O)[O-])cc1 ','COCOC(C)C(C#N)(c1ccccc1)c1ccc(OC)cc1 ','COCOC(Br)Cc1ccc(-c2ccccc2)cc1 ','COCOC(c1ccccc1)C(c1ccccc1)(c1ccccc1)c1ccc([N+](=O)[O-])cc1 ','COC(C)C(C)c1ccccc1 ','COCC(C)c1ccc(C#N)cc1 ','COC(=O)c1ccc(C(C#N)(c2ccccc2)C(Br)OC)cc1 ','COCOC(OC)C(C#N)(c1ccccc1)c1ccc(C#N)cc1 ','COCOC(Br)C(C#N)(C#N)c1ccccc1 ','COCC(C#N)c1ccc(C#N)cc1 ','COCOC(C#N)Cc1ccc(-c2ccccc2)cc1 ','CCc1ccc(C(C#N)COC)cc1 ','COC(=O)c1ccc(C(C#N)(C#N)C(Oc2ccccc2)c2ccccc2)cc1 ','N#CC(COc1ccccc1)c1ccc(-c2ccccc2)cc1 ','CCc1ccc(C(c2ccccc2)(c2ccccc2)C(C#N)OCOC)cc1 ','CCc1ccc(C(c2ccccc2)(c2ccccc2)C(C#N)Oc2ccccc2)cc1 ','COCOCC(C#N)c1ccccc1 ','CCc1ccc(C(C)C(C#N)OC)cc1 ','COC(Br)C(C)c1ccc(C#N)cc1 ','COCOC(Br)C(C)(C#N)c1ccc(-c2ccccc2)cc1 ','COC(OC)C(C#N)(C#N)c1ccc([N+](=O)[O-])cc1 ','COC(C)C(C#N)(c1ccccc1)c1ccccc1 ','CCc1ccc(C(C)COCOC)cc1 ','COCOC(C#N)C(C#N)(c1ccccc1)c1ccc(C#N)cc1 ','COC(C)C(C#N)(c1ccccc1)c1ccc(-c2ccccc2)cc1 ','COc1ccc(C(C#N)C(Br)OC)cc1 ','COCOCC(C)(C#N)c1ccc(C(=O)OC)cc1 ','CCc1ccc(C(C)C(OC)OC)cc1 ','CC(Oc1ccccc1)C(C#N)(c1ccccc1)c1ccccc1 ','COc1ccc(C(C#N)C(Oc2ccccc2)c2ccccc2)cc1 ','COCOC(C#N)C(C#N)c1ccc(OC)cc1 ','COCOC(OC)C(C#N)(C#N)c1ccc(OC)cc1 ','COC(=O)c1ccc(C(C)C(OC)OC)cc1 ','CCc1ccc(C(C#N)(C#N)C(C)OCOC)cc1 ','COCOCC(C)(c1ccccc1)c1ccc(C(=O)OC)cc1 ','CCc1ccc(C(C#N)(c2ccccc2)C(C#N)OCOC)cc1 ','COCOC(C#N)C(C)(C#N)c1ccccc1 ','COC(OC)C(C#N)(c1ccccc1)c1ccccc1 ','CCc1ccc(C(c2ccccc2)(c2ccccc2)C(OC)OC)cc1 ','COC(C#N)C(C#N)c1ccccc1 ','COC(C)C(C#N)(c1ccccc1)c1ccc(C#N)cc1 ','COCOC(OC)C(C)c1ccc(C#N)cc1 ','COCOC(C#N)C(c1ccccc1)c1ccc(-c2ccccc2)cc1 ','CC(Oc1ccccc1)C(C#N)(c1ccccc1)c1ccc(-c2ccccc2)cc1 ','COC(Oc1ccccc1)C(C)(c1ccccc1)c1ccc([N+](=O)[O-])cc1 ','COCOC(OC)C(C#N)c1ccc(-c2ccccc2)cc1 ','COCOC(C)C(C#N)(C#N)c1ccccc1 ','CCc1ccc(C(c2ccccc2)(c2ccccc2)C(C#N)OC)cc1 ','CC(Oc1ccccc1)C(c1ccccc1)c1ccc([N+](=O)[O-])cc1 ','COCOC(OC)C(C#N)(C#N)c1ccc(-c2ccccc2)cc1 ','COCOC(C)C(C)(c1ccccc1)c1ccc([N+](=O)[O-])cc1 ','CC(Oc1ccccc1)C(C#N)(C#N)c1ccc([N+](=O)[O-])cc1 ','CCc1ccc(C(C)C(OCOC)c2ccccc2)cc1 ','COc1ccc(C(C)(C#N)C(Br)Oc2ccccc2)cc1 ','COC(Br)Cc1ccccc1 ','N#CC(Cc1ccc(-c2ccccc2)cc1)Oc1ccccc1 ','N#CC(Oc1ccccc1)C(C#N)(c1ccccc1)c1ccccc1 ','CCc1ccc(C(c2ccccc2)C(C#N)OCOC)cc1 ','COC(=O)c1ccc(CC(OC)Oc2ccccc2)cc1 ','COC(Cc1ccc([N+](=O)[O-])cc1)c1ccccc1 ','CCc1ccc(C(c2ccccc2)C(OC)c2ccccc2)cc1 ','COC(C#N)C(C)c1ccc(-c2ccccc2)cc1 ','N#Cc1ccc(C(c2ccccc2)C(Oc2ccccc2)c2ccccc2)cc1 ','c1ccc(OCC(c2ccccc2)(c2ccccc2)c2ccccc2)cc1 ','COC(=O)c1ccc(CC(Br)Oc2ccccc2)cc1 ','CCc1ccc(C(C)(C#N)C(C)Oc2ccccc2)cc1 ','COC(C)C(C#N)(C#N)c1ccccc1 ','COc1ccc(C(C#N)(c2ccccc2)C(C#N)OC)cc1 ','COC(=O)c1ccc(C(c2ccccc2)(c2ccccc2)C(C#N)OC)cc1 ','COCOC(C)C(C#N)(c1ccccc1)c1ccccc1 ','COCOCC(c1ccccc1)c1ccc(C#N)cc1 ','COCOC(c1ccccc1)C(C)(C#N)c1ccccc1 ','COCC(c1ccccc1)(c1ccccc1)c1ccc(C(=O)OC)cc1 ','COCOC(C)C(C#N)(C#N)c1ccc(C(=O)OC)cc1 ','COC(C)C(C)(C#N)c1ccccc1 ',\n",
    "\t\t  'COCOC(Cc1ccc(C(=O)OC)cc1)OC ','N#CC(Oc1ccccc1)C(c1ccccc1)c1ccc(-c2ccccc2)cc1 ','N#Cc1ccc(C(COc2ccccc2)c2ccccc2)cc1 ','COCOC(OC)C(c1ccccc1)c1ccccc1 ','COCOC(OC)C(C#N)(c1ccccc1)c1ccc(OC)cc1 ','COCOCC(C)c1ccc(C(=O)OC)cc1 ','COCOC(Br)C(C)(C#N)c1ccc(OC)cc1 ','COCOCC(C#N)(c1ccccc1)c1ccc([N+](=O)[O-])cc1')]\n",
    "for m in ms:\n",
    "    _ = Chem.AllChem.GenerateDepictionMatching2DStructure(m,template)\n",
    "    #Draw.MolToFile(ms[0],'./SMILES_mol1.o.png')\n",
    "    #Draw.MolToFile(ms[1],'./SMILES_mol2.o.png') \n",
    "#print(ms)\n",
    "img=Draw.MolsToGridImage(ms[:8],molsPerRow=4,subImgSize=(200,200),legends=[x.GetProp(\"_Name\") for x in ms[:8]])\n",
    "img.save('images/SMILES_mol2.o.png') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generating Similarity Maps Using Fingerprints\n",
    "from rdkit import Chem\n",
    "mol = Chem.MolFromSmiles('N#CC(c1ccccc1)C(Br)Oc1ccccc1')\n",
    "refmol = Chem.MolFromSmiles('N#CC(Oc1ccccc1)C(c1ccccc1)c1ccc(-c2ccccc2)cc1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit.Chem import Draw\n",
    "from rdkit.Chem.Draw import SimilarityMaps\n",
    "fp = SimilarityMaps.GetAPFingerprint(mol, fpType='normal')\n",
    "fp = SimilarityMaps.GetTTFingerprint(mol, fpType='normal')\n",
    "fp = SimilarityMaps.GetMorganFingerprint(mol, fpType='bv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, maxweight = SimilarityMaps.GetSimilarityMapForFingerprint(refmol, mol, SimilarityMaps.GetMorganFingerprint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit import DataStructs\n",
    "fig, maxweight = SimilarityMaps.GetSimilarityMapForFingerprint(refmol, mol, lambda m,idx: SimilarityMaps.GetMorganFingerprint(m, atomId=idx, radius=1, fpType='count'), metric=DataStructs.TanimotoSimilarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit.Chem import Descriptors\n",
    "m = Chem.MolFromSmiles('N#CC(c1ccccc1)C(Br)Oc1ccccc1')\n",
    "print (Descriptors.TPSA(m))\n",
    "print (Descriptors.MolLogP(m))\n",
    "Chem.AllChem.ComputeGasteigerCharges(m)\n",
    "m.GetAtomWithIdx(0).GetDoubleProp('_GasteigerCharge')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualization of Descriptors\n",
    "from rdkit.Chem.Draw import SimilarityMaps\n",
    "mol = Chem.MolFromSmiles('N#CC(c1ccccc1)C(Br)Oc1ccccc1')\n",
    "Chem.AllChem.ComputeGasteigerCharges(mol)\n",
    "contribs = [mol.GetAtomWithIdx(i).GetDoubleProp('_GasteigerCharge') for i in range(mol.GetNumAtoms())]\n",
    "fig = SimilarityMaps.GetSimilarityMapFromWeights(mol, contribs, colorMap='jet', contourLines=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit.Chem import rdMolDescriptors\n",
    "contribs = rdMolDescriptors._CalcCrippenContribs(mol)\n",
    "fig = SimilarityMaps.GetSimilarityMapFromWeights(mol,[x for x,y in contribs], colorMap='jet', contourLines=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Chemical Features\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import ChemicalFeatures\n",
    "from rdkit import RDConfig\n",
    "import os\n",
    "fdefName = os.path.join(RDConfig.RDDataDir,'BaseFeatures.fdef')\n",
    "factory = ChemicalFeatures.BuildFeatureFactory(fdefName)\n",
    "m = Chem.MolFromSmiles('N#CC(c1ccccc1)C(Br)Oc1ccccc1')\n",
    "feats = factory.GetFeaturesForMol(m)\n",
    "len(feats)\n",
    "print(feats[0].GetFamily())\n",
    "print(feats[0].GetType())\n",
    "print(feats[0].GetAtomIds())\n",
    "print(feats[4].GetFamily())\n",
    "print(feats[4].GetAtomIds())\n",
    "Chem.AllChem.Compute2DCoords(m)\n",
    "print(feats[0].GetPos())\n",
    "print(list(feats[0].GetPos()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Molecular Fragments\n",
    "fName=os.path.join(RDConfig.RDDataDir,'FunctionalGroups.txt')\n",
    "from rdkit.Chem import FragmentCatalog\n",
    "fparams = FragmentCatalog.FragCatParams(1,6,fName)\n",
    "print(fparams.GetNumFuncGroups())\n",
    "fcat=FragmentCatalog.FragCatalog(fparams)\n",
    "fcgen=FragmentCatalog.FragCatGenerator()\n",
    "m = Chem.MolFromSmiles('N#CC(c1ccccc1)C(Br)Oc1ccccc1')\n",
    "print(fcgen.AddFragsFromMol(m,fcat))\n",
    "print(fcat.GetEntryDescription(0))\n",
    "print(fcat.GetEntryDescription(1))\n",
    "print(fcat.GetEntryDescription(2))\n",
    "list(fcat.GetEntryFuncGroupIds(2))\n",
    "fparams.GetFuncGroup(1)\n",
    "print(Chem.MolToSmarts(fparams.GetFuncGroup(1)))\n",
    "print(Chem.MolToSmarts(fparams.GetFuncGroup(34)))\n",
    "print(fparams.GetFuncGroup(1).GetProp('_Name'))\n",
    "print(fparams.GetFuncGroup(34).GetProp('_Name'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = Chem.MolFromSmiles('N#CC(c1ccccc1)C(Br)Oc1ccccc1')\n",
    "m.GetNumAtoms()\n",
    "help(m.GetNumAtoms)\n",
    "m.GetNumAtoms(onlyExplicit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Advanced Topics/Warnings Editing Molecules\n",
    "m = Chem.MolFromSmiles('N#CC(c1ccccc1)C(Br)Oc1ccccc1')\n",
    "m.GetAtomWithIdx(0).SetAtomicNum(7)\n",
    "Chem.SanitizeMol(m)\n",
    "rdkit.Chem.rdmolops.SanitizeFlags.SANITIZE_NONE\n",
    "print(Chem.MolToSmiles(m))\n",
    "#Do not forget the sanitization step, without it one can end up with results that look ok (so long as you don’t think):\n",
    "m = Chem.MolFromSmiles('N#CC(c1ccccc1)C(Br)Oc1ccccc1')\n",
    "m.GetAtomWithIdx(0).SetAtomicNum(8)\n",
    "print(Chem.MolToSmiles(m))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can explore the range of solubilities found in the dataset by plotting a histogram of solubility values from the dataset. Our machine learning models will aim to predict these solubilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sns.distplot(dataset[\"measured log solubility in mols per litre\"])\n",
    "df = pd.DataFrame(dataset)\n",
    "display(df)\n",
    "df_condition = df[(df['sssr'] < 10) & (df[\"clogp\"] > 0.25)]\n",
    "# https://buildmedia.readthedocs.org/media/pdf/rdkit/latest/rdkit.pdf\n",
    "\n",
    "# 'sssr', -- smallest set of smallest rings\n",
    "# 'clogp', --\n",
    "# 'mr', --\n",
    "# 'mw', --\n",
    "# 'tpsa', -- topological polar surface area (TPSA) descriptor\n",
    "# 'chi0n', 'chi1n', 'chi2n', 'chi3n', 'chi4n', --  Connectivity Descriptors returns the ChiXn value for a molecule for X=0-4 Rev. Comput. Chem. 2:367-422 (1991)\n",
    "# 'chi0v', 'chi1v', 'chi2v', 'chi3v', 'chi4v', -- returns the ChiXv value for a molecule for X=0-4 Rev. Comput. Chem. 2:367-422 (1991)\n",
    "# 'fracsp3', -- \n",
    "# 'hall_kier_alpha', -- Rev. Comput. Chem. 2:367-422 (1991)\n",
    "# 'kappa1', 'kappa2', 'kappa3', -- Rev. Comput. Chem. 2:367-422 (1991)\n",
    "# 'labuteasa', -- J. Mol. Graph. Mod. 18:464-77 (2000)\n",
    "# 'number_aliphatic_rings', --\n",
    "# 'number_aromatic_rings', --\n",
    "# 'number_amide_bonds', --\n",
    "# 'number_atom_stereocenters', -- \n",
    "# 'number_bridgehead_atoms', --\n",
    "# 'number_HBA', --\n",
    "# 'number_HBD', --\n",
    "# 'number_hetero_atoms', -- \n",
    "# 'number_hetero_cycles', --\n",
    "# 'number_rings', --\n",
    "# 'number_rotatable_bonds', --\n",
    "# 'number_spiro', -- Number of spiro atoms (atoms shared between rings thatshare exactly one atom)\n",
    "# 'number_saturated_rings', --\n",
    "# 'number_heavy_atoms', --\n",
    "# 'number_nh_oh', --\n",
    "# 'number_n_o', --\n",
    "# 'number_valence_electrons', --\n",
    "# 'max_partial_charge', --\n",
    "# 'min_partial_charge',-- \n",
    "# 'fr_C_O', --\n",
    "# 'fr_C_O_noCOO', --\n",
    "# 'fr_Al_OH', --\n",
    "# 'fr_Ar_OH', --\n",
    "# 'fr_methoxy', --\n",
    "# 'fr_oxime', --\n",
    "# 'fr_ester', --\n",
    "# 'fr_Al_COO', --\n",
    "# 'fr_Ar_COO',-- \n",
    "# 'fr_COO', --\n",
    "# 'fr_COO2', --\n",
    "# 'fr_ketone', --\n",
    "# 'fr_ether', --\n",
    "# 'fr_phenol', --\n",
    "# 'fr_aldehyde',-- \n",
    "# 'fr_quatN', --\n",
    "# 'fr_NH2', --\n",
    "# 'fr_NH1', --\n",
    "# 'fr_NH0', --\n",
    "# 'fr_Ar_N', --\n",
    "# 'fr_Ar_NH', --\n",
    "# 'fr_aniline', --\n",
    "# 'fr_Imine', --\n",
    "# 'fr_nitrile', --\n",
    "# 'fr_hdrzine', --\n",
    "# 'fr_hdrzone', --\n",
    "# 'fr_nitroso', --\n",
    "# 'fr_N_O', --\n",
    "# 'fr_nitro', --\n",
    "# 'fr_azo', --\n",
    "# 'fr_diazo', --\n",
    "# 'fr_azide', --\n",
    "# 'fr_amide', --\n",
    "# 'fr_priamide',-- \n",
    "# 'fr_amidine', --\n",
    "# 'fr_guanido', --\n",
    "# 'fr_Nhpyrrole', --\n",
    "# 'fr_imide', --\n",
    "# 'fr_isocyan', --\n",
    "# 'fr_isothiocyan',-- \n",
    "# 'fr_thiocyan',-- \n",
    "# 'fr_halogen', --\n",
    "# 'fr_alkyl_halide',-- \n",
    "# 'fr_sulfide',-- \n",
    "# 'fr_SH', --\n",
    "# 'fr_C_S', --\n",
    "# 'fr_sulfone', --\n",
    "# 'fr_sulfonamd', --\n",
    "# 'fr_prisulfonamd',-- \n",
    "# 'fr_barbitur', --\n",
    "# 'fr_urea', --\n",
    "# 'fr_term_acetylene', -- \n",
    "# 'fr_imidazole',-- \n",
    "# 'fr_furan', --\n",
    "# 'fr_thiophene', --\n",
    "# 'fr_thiazole', --\n",
    "# 'fr_oxazole', --\n",
    "# 'fr_pyridine', --\n",
    "# 'fr_piperdine', --\n",
    "# 'fr_piperzine', --\n",
    "# 'fr_morpholine', --\n",
    "# 'fr_lactam', --\n",
    "# 'fr_lactone', --\n",
    "# 'fr_tetrazole', --\n",
    "# 'fr_epoxide', --\n",
    "# 'fr_unbrch_alkane',-- \n",
    "# 'fr_bicyclic', --\n",
    "# 'fr_benzene', --\n",
    "# 'fr_phos_acid', --\n",
    "# 'fr_phos_ester', --\n",
    "# 'fr_nitro_arom', --\n",
    "# 'fr_nitro_arom_nonortho', --\n",
    "# 'fr_dihydropyridine', --\n",
    "# 'fr_phenol_noOrthoHbond', --\n",
    "# 'fr_Al_OH_noTert', --\n",
    "# 'fr_benzodiazepine', --\n",
    "# 'fr_para_hydroxylation',-- \n",
    "# 'fr_allylic_oxid', --\n",
    "# 'fr_aryl_methyl', --\n",
    "# 'fr_Ndealkylation1',-- \n",
    "# 'fr_Ndealkylation2', --\n",
    "# 'fr_alkyl_carbamate', --\n",
    "# 'fr_ketone_Topliss', --\n",
    "# 'fr_ArN', --\n",
    "# 'fr_HOCCN',--\n",
    "\n",
    "display(df_condition)\n",
    "df_clogp = df[df.clogp.eq(0.26)]\n",
    "display(df_clogp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit import Chem\n",
    "from rdkit.Chem import rdMolDescriptors\n",
    "from rdkit.Chem.Draw import IPythonConsole\n",
    "from rdkit.Chem import Draw\n",
    "smiles_fig = Chem.MolFromSmiles('N#CC(Oc1ccccc1)C(c1ccccc1)c1ccc(-c2ccccc2)cc1')\n",
    "smiles_fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate a Morgan fingerprint and save information about the bits that are set using the bitInfo argument:\n",
    "bi = {}\n",
    "fp = rdMolDescriptors.GetMorganFingerprintAsBitVect(smiles_fig, radius=2, bitInfo=bi)\n",
    "# show 10 of the set bits:\n",
    "list(fp.GetOnBits())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#In its simplest form, the new code lets you display the atomic environment that sets a particular bit. Here we will look at bit 674:\n",
    "Draw.DrawMorganBit(smiles_fig,674,bi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DrawMorganBits(), for drawing multiple bits at once (thanks to Pat Walters for suggesting this one):\n",
    "tpls = [(smiles_fig,x,bi) for x in fp.GetOnBits()]\n",
    "Draw.DrawMorganBits(tpls[:12],molsPerRow=4,legends=[str(x) for x in fp.GetOnBits()][:12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import interact,fixed,IntSlider\n",
    "def renderFpBit(mol,bitIdx,bitInfo,fn):\n",
    "    bid = bitIdx\n",
    "    return(display(fn(mol,bid,bitInfo)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interact(renderFpBit, bitIdx=list(bi.keys()),mol=fixed(smiles_fig),\n",
    "         bitInfo=fixed(bi),fn=fixed(Draw.DrawMorganBit));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cell we will plot a histogram of SMILES string lengths from dataset. These lengths will be used to determine the length of the inputs for our CNN and VAE models. Below are examples of the SMILES representation: \n",
    "1. Methane: 'C'\n",
    "2. Pentane: 'CCCCC'\n",
    "3. Methanol and Ethanol: 'CO' and 'CCO'\n",
    "4. Pyridine: 'C1:C:C:N:C:C:1'\n",
    "\n",
    "To learn more about the SMILES representation, click [here](https://chem.libretexts.org/Courses/University_of_Arkansas_Little_Rock/ChemInformatics_(2017)%3A_Chem_4399%2F%2F5399/2.3%3A_Chemical_Representations_on_Computer%3A_Part_III)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smiles_lengths = map(len, dataset.smiles.values)\n",
    "#sns.distplot(list(smiles_lengths), bins=20, kde=False)\n",
    "plt.rcParams.update({'font.size': 20})\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.title('SMILES string lengths Histogram')\n",
    "plt.ylabel('Density')\n",
    "plt.xlabel('SMILES string lengths')\n",
    "ax = sns.distplot(list(smiles_lengths), color=\"b\", bins=20, rug=True, rug_kws={\"color\": \"k\"}, kde=True, kde_kws={\"color\": \"r\", \"label\": \"Gaussian Kernel Density Estimate (KDE)\"}, hist_kws={\"histtype\": \"bar\", \"linewidth\": 3, \"alpha\": 1, \"color\": \"b\"} )\n",
    "ax=plt.savefig('./gap_smiles_lengths.png', dpi=600, facecolor='w', edgecolor='w',orientation='portrait', papertype=None, format=None,transparent=False, bbox_inches=None, pad_inches=0.1,frameon=None, metadata=None)\n",
    "#ax=plt.savefig('gdrive/MyDrive/Colab Notebooks/data/fig_smiles_lengths.png', dpi=600, facecolor='w', edgecolor='w',orientation='portrait', papertype=None, format=None,transparent=False, bbox_inches=None, pad_inches=0.1,frameon=None, metadata=None)\n",
    "# ax=plt.savefig('../data/fig_smiles_lengths.png', dpi=600, facecolor='w', edgecolor='w',orientation='portrait', papertype=None, format=None,transparent=False, bbox_inches=None, pad_inches=0.1,frameon=None, metadata=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.reset_index()\n",
    "dataset = dataset.drop(['index'], axis = 1)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_df = dataset.drop(columns = 'smiles')\n",
    "print(x_df.shape)\n",
    "x_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from https://proxy.nanohub.org/weber/2004336/GBdSjVSdDDS3NYpl/4/notebooks/LLZO_MachineLearning.ipynb\n",
    "# This code is to drop columns with std = 0. \n",
    "#x_df = pd.DataFrame(X)\n",
    "#All columns that have a standard deviation of zero are dropped, as they don't contribute new information to the models.\n",
    "x_df = x_df.loc[:, x_df.std() != 0]\n",
    "print(x_df.shape) # This shape is (#Entries, #Descriptors per entry)\n",
    "x_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_df.to_csv('./x_df_SMILES_RDKit_2D.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " plt.figure(figsize=(10,10))\n",
    " plt.rcParams.update({'font.size': 20})\n",
    " smiles_lengths = map(len, dataset.smiles.values)\n",
    " #sns.distplot(list(smiles_lengths), bins=20, kde=False\n",
    "plt.title('Topological polar surface area (TPSA) Distribution Histogram')\n",
    "plt.ylabel('Density') \n",
    "plt.xlabel('Topological polar surface area (TPSA)')              \n",
    "\n",
    "# sns.displot(list(smiles_lengths), bins=20, kde=False)\n",
    "#ax = sns.distplot(dataset[\"lumo\"], rug=True, rug_kws={\"color\": \"g\"}, kde_kws={\"color\": \"k\", \"lw\": 3, \"label\": \"KDE\"}, hist_kws={\"histtype\": \"step\", \"linewidth\": 3,\"alpha\": 1, \"color\": \"g\"})\n",
    "ax = sns.distplot(dataset[\"tpsa\"], rug=True, rug_kws={\"color\": \"g\"},kde_kws={\"color\": \"k\", \"lw\": 3, \"label\": \"KDE\"},hist_kws={\"histtype\":\"step\", \"linewidth\": 3,\"alpha\": 1, \"color\": \"r\"})\n",
    "ax=plt.savefig('./tpsa.png', dpi=600, facecolor='w', edgecolor='w',orientation='landscape', papertype='a4', format=None, transparent=False, bbox_inches=None, pad_inches=None, frameon=None, metadata=None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preparation\n",
    "\n",
    "Now we will pre-process the dataset for the CNN and VAE models. First, we'll get the unique character set from all SMILES strings in the dataset. Then we will use the unique character set to convert our SMILES strings to a one-hot representation, which is a representation that converts raw strings of text to numerical inputs for our models.\n",
    "\n",
    "In a one-hot representation, each character of our SMILES string is encoded as a vector of zeros, except for one non-zero value. For instance, the character 'C' in the SMILES string is converted to a vector of length 31, consisting of 30 zeros and one non-zero entry of one. The length of this vector (31 in our case) is the total number of unique characters in the dataset.\n",
    "\n",
    "Given a string of 5 characters (say Pentane, which is represented as 'CCCCC'), we would thus get 5 vectors each of length 31. Since different molecules have different SMILES string lengths, we can pre-define the length of each string to be the maximum length from the database, with smaller molecules represented with additional characters. In our case, this maximum length is 40 and we represent the extra characters for smaller molecules with pre-defined one-hot vectors. This means that each molecule is now represented as a set of 40 vectors, each of length 31. We can represent this as a 40x31 matrix.\n",
    "\n",
    "One-hot encoding is commonly used in natural language processing, and you can learn more about one-hot encoding [here](https://en.wikipedia.org/wiki/One-hot). \n",
    "\n",
    "Finally, we will define our input and output and create test/train splits in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "charset = generate_charset(\n",
    "    dataset[\"smiles\"].values.ravel()\n",
    ")\n",
    "# get the number of unique characters\n",
    "charset_length = len(charset)\n",
    "# define max number of SMILES for model input vector\n",
    "max_smiles_chars = 70\n",
    "# dimension of input vector\n",
    "input_dim = charset_length * max_smiles_chars\n",
    "# get one-hot representation of the SMILES strings \n",
    "one_hots = smiles_to_onehots(dataset[\"smiles\"].values, charset, max_smiles_chars)\n",
    "# split input into train and test sets\n",
    "X_train = one_hots[:-100] #This takes the first 133885-13385=120500  entries to be the Training Set\n",
    "X_test = one_hots[-100:] # This takes the last 13385 entries to be the Testing Set\n",
    "\n",
    "# split output to train and test sets\n",
    "output = dataset[\"tpsa\"].values\n",
    "#output = dataset[\"homo\"].values\n",
    "#output = dataset[\"cv\"].values\n",
    "#output = dataset[\"r2\"].values\n",
    "\n",
    "# \"alpha\" - Isotropic polarizability (unit: Bohr^3)\n",
    "# \"gap\" - Gap between HOMO and LUMO (unit: Hartree)\n",
    "#\"mol_id\" - Molecule ID (gdb9 index) mapping to the .sdf file\n",
    "#\"A\" - Rotational constant (unit: GHz)\n",
    "#\"B\" - Rotational constant (unit: GHz)\n",
    "#\"C\" - Rotational constant (unit: GHz)\n",
    "#\"mu\" - Dipole moment (unit: D)\n",
    "#\"alpha\" - Isotropic polarizability (unit: Bohr^3)\n",
    "#\"homo\" - Highest occupied molecular orbital energy (unit: Hartree)\n",
    "#\"lumo\" - Lowest unoccupied molecular orbital energy (unit: Hartree)\n",
    "#\"gap\" - Gap between HOMO and LUMO (unit: Hartree)\n",
    "#\"r2\" - Electronic spatial extent (unit: Bohr^2)\n",
    "#\"zpve\" - Zero point vibrational energy (unit: Hartree)\n",
    "#\"u0\" - Internal energy at 0K (unit: Hartree)\n",
    "#\"u298\" - Internal energy at 298.15K (unit: Hartree)\n",
    "#\"h298\" - Enthalpy at 298.15K (unit: Hartree)\n",
    "#\"g298\" - Free energy at 298.15K (unit: Hartree)\n",
    "#\"cv\" - Heat capavity at 298.15K (unit: cal/(mol*K))\n",
    "#\"u0_atom\" - Atomization energy at 0K (unit: kcal/mol)\n",
    "#\"u298_atom\" - Atomization energy at 298.15K (unit: kcal/mol)\n",
    "#\"h298_atom\" - Atomization enthalpy at 298.15K (unit: kcal/mol)\n",
    "Y_train = output[:-100] #This takes the first 133885-100=133785 entries to be the Training Set\n",
    "Y_test = output[-100:] # This takes the last 100 entries to be the Testing Set\n",
    "\n",
    "# This Reshape function in the next two lines, turns each of the horizontal lists [ x, y, z] into a\n",
    "# vertical NumPy array [[x]\n",
    "#                       [y]\n",
    "#                       [z]]\n",
    "# This Step is required to work with the Sklearn Linear Model\n",
    "#Y_train = np.array(melt_train).reshape(-1,1) \n",
    "#Y_test  = np.array(melt_test).reshape(-1,1)\n",
    "print(len(X_train),len(X_test),len(Y_train),len(Y_test))\n",
    "# print(X_train[0]) # print a sample entry from the training set\n",
    "# print(X_test[0]) # print a sample entry from the training set\n",
    "# print(order)\n",
    "\n",
    "\n",
    "##  Train-Test Split  ##\n",
    "# https://proxy.nanohub.org/weber/1914019/IVqSH6gE0f3W6g9X/5/notebooks/mldefect.ipynb?\n",
    "# XX = copy.deepcopy(X)\n",
    "# n = dopant.size\n",
    "# m = np.int(X.size/n)\n",
    "\n",
    "# print(n)\n",
    "# print(m)\n",
    "\n",
    "# t = 0.20\n",
    "\n",
    "# X_train, X_test, Prop_train, Prop_test, dop_train, dop_test, sc_train, sc_test, ds_train, ds_test = train_test_split(XX, prop, dopant, CdX, doping_site, test_size=t)\n",
    "\n",
    "# n_tr = Prop_train.size\n",
    "# n_te = Prop_test.size\n",
    "\n",
    "# print(n_tr)\n",
    "# print(n_te)\n",
    "\n",
    "# Prop_train_fl = np.zeros(n_tr)\n",
    "# for i in range(0,n_tr):\n",
    "#     Prop_train_fl[i] = copy.deepcopy(float(Prop_train[i]))\n",
    "    \n",
    "# print(Prop_train_fl)\n",
    "\n",
    "# Prop_test_fl = np.zeros(n_te)\n",
    "# for i in range(0,n_te):\n",
    "#     Prop_test_fl[i] = copy.deepcopy(float(Prop_test[i]))\n",
    "    \n",
    "# print(Prop_test_fl)\n",
    "    \n",
    "# X_train_fl = [[0.0 for a in range(m)] for b in range(n_tr)]\n",
    "# for i in range(0,n_tr):\n",
    "#     for j in range(0,m):\n",
    "#         X_train_fl[i][j] = np.float(X_train[i][j])\n",
    "\n",
    "# print(X_train_fl)\n",
    "\n",
    "# X_test_fl = [[0.0 for a in range(m)] for b in range(n_te)]\n",
    "# for i in range(0,n_te):\n",
    "#     for j in range(0,m):\n",
    "#         X_test_fl[i][j] = np.float(X_test[i][j])\n",
    "\n",
    "# print(X_test_fl)\n",
    "\n",
    "# X_out_fl = [[0.0 for a in range(m)] for b in range(n_out)]\n",
    "# for i in range(0,n_out):\n",
    "#     for j in range(0,m):\n",
    "#         X_out_fl[i][j] = np.float(X_out[i][j])\n",
    "\n",
    "# print(X_out_fl)\n",
    "\n",
    "# X_all_fl = [[0.0 for a in range(m)] for b in range(n_all)]\n",
    "# for i in range(0,n_all):\n",
    "#     for j in range(0,m):\n",
    "#         X_all_fl[i][j] = np.float(X_all[i][j])\n",
    "\n",
    "# print(X_all_fl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's briefly visualize what our input data looks like using a heatmap that shows the position of each character in the SMILES string, you can change the index to see various molecules. Each molecule is represented by a 40x31 sparse matrix, the bright spots in the heatmap indicate the position at which a one is found in the matrix. For instance, the first row has a bright spot at index 18, indicating that the first character is 'C'. The second row has a bright spot at index 23, which indicates that the second character is 'O'. For the compound Dimethoxymethane with a SMILES string 'COCOC', we expect the matrix to have alternating bright spots at index 18 and index 23 for the first five rows. Beyond that, the rows all have a bright spot at index 1, which stands for the extra characters padded on to our string to make all SMILES strings the same length. The heatmap below is plotted using the [Seaborn](https://seaborn.pydata.org/) library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_rows = 4\n",
    "num_cols = 4\n",
    "num_images = num_rows*num_cols\n",
    "plt.figure(figsize=(6*num_cols, 6*num_rows))\n",
    "import matplotlib\n",
    "matplotlib.rcParams.update(matplotlib.rcParamsDefault)\n",
    "for i in range(num_images):\n",
    "    plt.subplot(num_rows, num_cols, i+1)\n",
    "    #plot_image(i, predictions, testLabels, testImages)\n",
    "    #plt.figure(figsize=(30,30))\n",
    "    #for i in range(25): #133785 \n",
    "    #plt.subplot(5,5,i+1)\n",
    "    plt.xticks([],fontsize=8)\n",
    "    plt.yticks([],fontsize=8)\n",
    "    plt.grid(True)\n",
    "    #plt.xlabel(X_test(dataset.iloc[i])\n",
    "    plt.xlabel('Character', fontsize=16)\n",
    "    #plt.ylabel(X_test(dataset.iloc[i])\n",
    "    plt.ylabel('Position in SMILES String', fontsize=16)\n",
    "    #X_test[i] = X_test[i](\"Position in SMILES String\", \"Character\")\n",
    "    plt.title(f\"SMILES: {dataset.iloc[i]['smiles']}\", fontsize=16)\n",
    "    #plt.plot(range(num_images), label=f\"SMILES: {dataset.iloc[i]['smiles']}\")\n",
    "    #plt.legend()\n",
    "    #sns.heatmap(X_test[i])\n",
    "    sns.heatmap(X_train[i])\n",
    "\n",
    "    #plt.imshow(X_train[i], cmap=plt.cm.binary)\n",
    "    #plt.xlabel(class_names[int(trainLabels[i])])\n",
    "    #print(dataset.iloc[i]['smiles'])\n",
    "\n",
    "    \n",
    "#plt.imshow(X_train[index]) # By altering 'index' you will see another of the pictures imported\n",
    "#plt.colorbar()\n",
    "#plt.grid(False)\n",
    "#print(\"Train Images Array shape:\", trainImages.shape)\n",
    "#print(\"Train Labels Array shape:\", trainLabels.shape)\n",
    "#print(\"Test Images Array shape:\", testImages.shape)\n",
    "#print(\"Test Labels Array shape:\", testLabels.shape)\n",
    "\n",
    "#index = 6986 #index runs from 0 to 138388\n",
    "#sns.heatmap(X_train[index]) # This is a single training example -- note that it is a matrix, not a single vector!\n",
    "#plt.xlabel('Character')\n",
    "#plt.ylabel('Position in SMILES String')\n",
    "#print(dataset.iloc[index]['smiles'])\n",
    "#ax=plt.savefig('gdrive/MyDrive/Colab Notebooks/data/fig_smiles_character.png', dpi=600, facecolor='w', edgecolor='w',orientation='portrait', papertype=None, format=None,transparent=False, bbox_inches=None, pad_inches=0.1,frameon=None, metadata=None)\n",
    "#ax=plt.savefig('./homo_fig_smiles_character.png', dpi=600, facecolor='w', edgecolor='w',orientation='portrait', papertype=None, format=None,transparent=False, bbox_inches=None, pad_inches=0.1,frameon=None, metadata=None)\n",
    "\n",
    "#ax = sns.distplot(dataset[\"r2\"], rug=True, rug_kws={\"color\": \"g\"},kde_kws={\"color\": \"k\", \"lw\": 3, \"label\": \"KDE\"},hist_kws={\"histtype\": \"step\", \"linewidth\": 3,\"alpha\": 1, \"color\": \"r\"})\n",
    "#ax=plt.savefig('./homo_X_test.png', dpi=600, facecolor='w', edgecolor='w',orientation='landscape', papertype='a4', format=None, transparent=False, bbox_inches=None, pad_inches=None, frameon=None, metadata=None, annot=True, fmt=\"d\")\n",
    "ax=plt.savefig('./tpsa_X_train.png', dpi=600, facecolor='w', edgecolor='w',orientation='landscape', papertype='a4', format=None, transparent=False, bbox_inches=None, pad_inches=None, frameon=None, metadata=None, annot=True, fmt=\"d\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <ins>Supervised CNN model for predicting solubility</ins>\n",
    "\n",
    "In this section, we will set up a convolutional neural network to predict solubility using one-hot SMILES as input. A convolutional neural network is a machine learning model that is commonly used to classify images, and you can learn more about them [here](https://en.wikipedia.org/wiki/Convolutional_neural_network)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define model structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will create the model structure, starting with the input layer. As described above, each training example is a 40x31 matrix, which is the shape we pass to the Input layer in Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the input layer\n",
    "# NOTE: We feed in a sequence here! We're inputting up to max_smiles_chars characters, \n",
    "# and each character is an array of length charset_length\n",
    "\n",
    "\n",
    "smiles_input = Input(shape=(max_smiles_chars, charset_length), name=\"SMILES-Input\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will define the convolution layers where each layer attempts to learn certain features of the images, such as edges and corners. The input to each layer (a matrix) is transformed via convolution operations, which are element by element multiplications of the input matrix and a filter matrix. The convolutional layer learns the filter matrix that will best identify unique features of the image. You can learn more about convolution operations and the math behind convolutional neural networks [here](https://towardsdatascience.com/gentle-dive-into-math-behind-convolutional-neural-networks-79a07dd44cf9)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set parameters for convolutional layers \n",
    "num_conv_filters = 16\n",
    "kernel_size = 3\n",
    "#kernel_init = initializers.RandomNormal(seed=0)\n",
    "#bias_init = initializers.Zeros()\n",
    "init_weights = initializers.glorot_normal(seed=0)\n",
    "\n",
    "# Define the convolutional layers\n",
    "# Multiple convolutions in a row is a common architecture (but there are many \"right\" choices here)\n",
    "conv_1_func = Conv1D(\n",
    "    filters=num_conv_filters, # What is the \"depth\" of the convolution? How many times do you look at the same spot?\n",
    "    kernel_size=kernel_size, # How \"wide\" of a spot does each filter look at?\n",
    "    name=\"Convolution-1\",\n",
    "    activation=\"relu\", # This is a common activation function: Rectified Linear Unit (ReLU)\n",
    "    kernel_initializer=init_weights #This defines the initial values for the weights\n",
    ")\n",
    "conv_2_func = Conv1D(\n",
    "    filters=num_conv_filters, \n",
    "    kernel_size=kernel_size, \n",
    "    name=\"Convolution-2\",\n",
    "    activation=\"relu\",\n",
    "    kernel_initializer=init_weights\n",
    ")\n",
    "conv_3_func = Conv1D(\n",
    "    filters=num_conv_filters, \n",
    "    kernel_size=kernel_size, \n",
    "    name=\"Convolution-3\",\n",
    "    activation=\"relu\",\n",
    "    kernel_initializer=init_weights\n",
    ")\n",
    "conv_4_func = Conv1D(\n",
    "    filters=num_conv_filters, \n",
    "    kernel_size=kernel_size,\n",
    "    name=\"Convolution-4\",\n",
    "    activation=\"relu\",\n",
    "    kernel_initializer=init_weights\n",
    ")\n",
    "\n",
    "# strides and paddind can be added in the convolution netowrk\n",
    "# strides=2, padding=\"same\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The four convolution layers defined above will attempt to learn features of the SMILES string (represented as a 40x31 matrix) that are relevant to predicting the solubility. To get a numerical prediction, we now flatten the output of the convolution and pass it to a set of regular `Dense` layers, the last layer predicting one value for the solubility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define layer to flatten convolutions\n",
    "flatten_func = Flatten(name=\"Flattened-Convolutions\")\n",
    "\n",
    "# Define the activation function layer\n",
    "hidden_size = 32\n",
    "dense_1_func = Dense(hidden_size, activation=\"relu\", name=\"Fully-Connected\", kernel_initializer=init_weights)\n",
    "\n",
    "# Add a Dense layer with a L1 activity regularizer\n",
    "#dense_1_func = Dense(hidden_size, activation=\"relu\", name=\"Fully-Connected\", activity_regularizer=regularizers.l1(10e-5), kernel_initializer=init_weights)\n",
    "\n",
    "# Define output layer -- it's only one dimension since it is regression\n",
    "output_size = 1\n",
    "output_mobility_func = Dense(output_size, activation=\"linear\", name=\"Log-lumo\", kernel_initializer=init_weights)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have defined all the layers, we will connect them together to make a graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# connect the CNN graph together\n",
    "conv_1_fwd = conv_1_func(smiles_input)\n",
    "conv_2_fwd = conv_2_func(conv_1_fwd)\n",
    "conv_3_fwd = conv_3_func(conv_2_fwd)\n",
    "conv_4_fwd = conv_4_func(conv_3_fwd)\n",
    "flattened_convs = flatten_func(conv_4_fwd)\n",
    "dense_1_fwd = dense_1_func(flattened_convs)\n",
    "output_mobility_fwd = output_mobility_func(flattened_convs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View model structure and metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the model is ready to train! But first we will define the model as `solubility_model` and compile it, then view some information on the model using the [keras2ascii](https://github.com/stared/keras-sequential-ascii) tool, which visually represents the layers in our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model\n",
    "mobility_model = Model(\n",
    "            inputs=[smiles_input],\n",
    "            outputs=[output_mobility_fwd]\n",
    ")\n",
    "mae_st = []\n",
    "# compile model\n",
    "#optimizer = optimizers.RMSprop(0.002) # Root Mean Squared Propagation\n",
    "# This line matches the optimizer to the model and states which metrics will evaluate the model's accuracy\n",
    "\n",
    "# loss= mse, mae\n",
    "# loss= categorical_crossentropy\n",
    "#loss='sparse_categorical_crossentropy'\n",
    "#loss='binary_crossentropy'\n",
    "#metrics=['accuracy', 'binary_crossentropy']\n",
    "#metrics=['accuracy']\n",
    "mobility_model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=\"mse\",\n",
    "    metrics=[\"mae\"]\n",
    ")\n",
    "mobility_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#!pip install keras_sequential_ascii\n",
    "from keras_sequential_ascii import keras2ascii\n",
    "# view model as a graph\n",
    "keras2ascii(mobility_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train CNN\n",
    "\n",
    "Now we will train our CNN solubility model to the training data! During training, we will see metrics printed after each epoch such as test/train loss (both as Mean Squared Error (MSE) and Mean Absolute Error (MAE))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#logdir=\"mobility_logs/fit/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "#tensorboard_callback = keras.callbacks.TensorBoard(log_dir=logdir)\n",
    "mae_st = []\n",
    "history = mobility_model.fit(\n",
    "    X_train, # Inputs\n",
    "    Y_train, # Outputs\n",
    "    epochs=20, # How many times to pass over the data\n",
    "    batch_size=64, # How many data rows to compute at once\n",
    "    verbose=1,\n",
    "    validation_data=(X_test, Y_test),\n",
    "    #callbacks=[tensorboard_callback] # You would usually use more splits of the data if you plan to tune hyperparams\n",
    ")\n",
    "#print('mse')\n",
    "#print('mae')\n",
    "mobility_model.save(os.path.expanduser('./tpsa_cnn_model.h5'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's view the learning curve for the trained model.\n",
    "\n",
    "This code will generate a plot where we show the test and train errors (MSE) as a function of epoch (one pass of all training examples through the NN).\n",
    "\n",
    "The learning curve will tell us if the model is overfitting or underfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the learning curve \n",
    "plt.rcParams.update({'font.size': 18})\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.subplots_adjust(left=0.16, bottom=0.16, right=0.95, top=0.90)\n",
    "plt.rc('font', family='Arial narrow')\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('CNN Model loss tpsa', fontname='Arial Narrow', size=18) #pad=12\n",
    "plt.ylabel('Error',fontname='Arial Narrow', size=18)\n",
    "plt.xlabel('Epoch',fontname='Arial Narrow', size=18)\n",
    "#plt.xlim(0,20)\n",
    "#plt.ylim(0,20)\n",
    "plt.legend(['Train', 'Validation',], loc='upper right')\n",
    "# te = '%.2f' % mean_absolute_error\n",
    "# tr = '%.2f' % mse_X_test\n",
    "# plt.text(4.3, 0.8, 'Test_rmse = ', c='r', fontsize=16)\n",
    "# plt.text(7.4, 0.8, te, c='r', fontsize=16)\n",
    "# plt.text(8.5, 0.8, 'eV', c='r', fontsize=16)\n",
    "# plt.text(4.2, 0.1, 'Train_rmse = ', c='r', fontsize=16)\n",
    "# plt.text(7.4, 0.1, tr, c='r', fontsize=16)\n",
    "# plt.text(8.5, 0.1, 'eV', c='r', fontsize=16)\n",
    "plt.savefig('./tpsa_cnn_X_training_loss.png', dpi=600, facecolor='w', edgecolor='w', scale=1, width=600, height=350)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# plot the learning curve \n",
    "plt.rcParams.update({'font.size': 18})\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.subplots_adjust(left=0.16, bottom=0.16, right=0.95, top=0.90)\n",
    "plt.rc('font', family='Arial narrow')\n",
    "plt.plot(history.history['mean_absolute_error'])\n",
    "plt.plot(history.history['val_mean_absolute_error'])\n",
    "plt.title('CNN Model MAE tpsa', fontname='Arial Narrow', size=18) #pad=12\n",
    "plt.ylabel('Mean Absolute Error',fontname='Arial Narrow', size=18)\n",
    "plt.xlabel('Epoch',fontname='Arial Narrow', size=18)\n",
    "#plt.xlim(0,20)\n",
    "#plt.ylim(0,2)\n",
    "plt.legend(['Train', 'Validation',], loc='upper right')\n",
    "# te = '%.2f' % mean_absolute_error\n",
    "# tr = '%.2f' % mse_X_test\n",
    "# plt.text(4.3, 0.8, 'Test_rmse = ', c='r', fontsize=16)\n",
    "# plt.text(7.4, 0.8, te, c='r', fontsize=16)\n",
    "# plt.text(8.5, 0.8, 'eV', c='r', fontsize=16)\n",
    "# plt.text(4.2, 0.1, 'Train_rmse = ', c='r', fontsize=16)\n",
    "# plt.text(7.4, 0.1, tr, c='r', fontsize=16)\n",
    "# plt.text(8.5, 0.1, 'eV', c='r', fontsize=16)\n",
    "plt.savefig('./tpsa_cnn_X_training_mae.png', dpi=600, facecolor='w', edgecolor='w', scale=1, width=600, height=350)\n",
    "plt.show()\n",
    "# plot the learning curve "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use CNN to make solubility predictions\n",
    "Now that we've trained our model, we can use it to make solubility predictions for any SMILES string! We just have to convert the SMILES string to 1-hot representation, then feed it to the `solubility_model` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_smiles = ['CC(C)CCCCO(C)N','CCC(C)CCC(C)OC','CC=CC1CCC1=O','CCOC()CCC','CC1(CC1OC)C#C'  ]\n",
    "#'CC(C)CCCCO(C)N','CCC(C)CCC(C)OC','CC=CC1CCC1=O','CCOC()CCC','CC1(CC1OC)C#C'\n",
    "#'Cc1cc(c1CCO)C#N','CCCCCCCCCC#C', 'CCC(C)CCC(C)C#C' ,'OCCCCC', 'CCC(C)(=O)C#C#N' , 'CCCCCCC#CCC' 'CC(=O)C=C(N)F', 'CCC'                  \n",
    "for smiles in example_smiles:\n",
    "    predict_test_input = smiles_to_onehots([smiles], charset, max_smiles_chars)\n",
    "    mobility_prediction = mobility_model.predict(predict_test_input)[0][0]\n",
    "    print(f'The predicted tpsa for SMILES {smiles} is {mobility_prediction}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now make a parity plot comparing the CNN model predictions to the ground truth data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = mobility_model.predict(X_train)\n",
    "x_y_line = np.linspace(min(Y_train.flatten()), max(Y_train.flatten()), 500)\n",
    "plt.figure(figsize=(8,8))\n",
    "#plt.subplots_adjust(left=0.16, bottom=0.16, right=0.95, top=0.90)\n",
    "#plt.rc('font', family='Arial narrow')\n",
    "\n",
    "plt.plot(Y_train.flatten(), preds.flatten(), 'o', label='predictions')\n",
    "plt.plot(x_y_line, x_y_line, label='y=x')\n",
    "plt.xlabel(\"tpsa (ground truth)\", fontname='Arial Narrow', size=16)\n",
    "plt.ylabel(\"tpsa (predicted)\", fontname='Arial Narrow', size=16)\n",
    "plt.title('Parity plot: predictions vs ground truth data', fontsize=16, pad=12)\n",
    "plt.rc('xtick', labelsize=14)\n",
    "plt.rc('ytick', labelsize=14)\n",
    "#a  = [-175,0,125]\n",
    "#b = [-175,0,125]\n",
    "#plt.plot(b, a, c='k', ls='-')\n",
    "#plt.legend(loc='upper left',ncol=1, frameon=True, prop={'family':'Arial narrow','size':16})\n",
    "plt.savefig('./tpsa_cnn_X_predict.png', dpi=600, facecolor='w', edgecolor='w', scale=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save model\n",
    "We can save/load this model for future use, using the `save()` and `load_model()` functions from Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "mobility_model.save(\"tpsa_model.hdf5\")\n",
    "\n",
    "# Load it back\n",
    "loaded_model = load_model(\"tpsa_model.hdf5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <ins>VAE model for generating SMILES strings</ins>\n",
    "In this section, we will set up a variational autoencoder to encode and decode SMILES strings. An autoencoder is a model that encodes the input to the model into a set of variables (known as encoded or 'latent variables'), which are then decoded to recover the original input. A variational autoencoder is an advanced version of an autoencoder where the encoded/latent variables are learnt as probability distributions rather than discrete values. You can learn more about autoencoders and variational autoencoders [here](https://www.jeremyjordan.me/variational-autoencoders/) and [here](https://www.jeremyjordan.me/autoencoders/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define model structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll need to define some new layers for this model, but we can also reuse old ones! (You will see this when we connect the model together.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hidden activation layer\n",
    "hidden_size = 16\n",
    "dense_1_func = Dense(hidden_size, activation=\"relu\", name=\"Fully-Connected-Latent\", kernel_initializer=init_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll define the layers to map to the latent space. We then define a sampling function that samples from a gaussian distribution to return the sampled latent variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VAE sampling \n",
    "# K.shape= Keras.shape\n",
    "def sampling(args):\n",
    "    z_mean, z_log_var = args\n",
    "    batch = K.shape(z_mean)[0]\n",
    "    dim = K.int_shape(z_mean)[1]\n",
    "    epsilon = K.random_normal((batch, dim), mean=0.0, stddev=1.0)\n",
    "    return z_mean + K.exp(0.5 * z_log_var) * epsilon # mu + sigma*epsilon yields a shifted, rescaled gaussian, \n",
    "                                                     # if epsilon is the standard gaussian\n",
    "#latent space.last hidden_size = 16 to latent_dim = 32 \n",
    "# encode to latent space\n",
    "latent_dim = 32 \n",
    "z_mean_func = Dense(latent_dim, name='z_mean')\n",
    "log_z_func = Dense(latent_dim, name='z_log_var')\n",
    "z_func = Lambda(sampling, name='z_sample')\n",
    "#print(z_mean_func)\n",
    "#print(log_z_func)\n",
    "#print(z_func)\n",
    "#z = Lambda(sampling)([z_mean, z_log_var])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll define the RNN (Recurrent Neural Network) layers for decoding SMILES from latent space values. Recurrent neural networks are known to perform well for learning a time series of data, where each cell of the recurrent network can learn from the previous cells, thus learning time dependencies in the data. This RNN uses Gated Recurrent Units as cells and you can learn more about recurrent neural networks and Gated Recurrent Units [here](https://towardsdatascience.com/understanding-gru-networks-2ef37df6c9be)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this repeat vector just repeats the input `max_smiles_chars` times \n",
    "# so that we get a value for each character of the SMILES string\n",
    "repeat_1_func = RepeatVector(max_smiles_chars, name=\"Repeat-Latent-1\")\n",
    "\n",
    "# RNN decoder\n",
    "rnn_size = 32\n",
    "gru_1_func = GRU(rnn_size, name=\"RNN-decoder-1\", return_sequences=True, kernel_initializer=init_weights)\n",
    "gru_2_func = GRU(rnn_size, name=\"RNN-decoder-2\", return_sequences=True, kernel_initializer=init_weights)\n",
    "gru_3_func = GRU(rnn_size, name=\"RNN-decoder-3\", return_sequences=True, kernel_initializer=init_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we'll define the output, which should map to the original SMILES input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_func = TimeDistributed(\n",
    "    Dense(charset_length, activation=\"softmax\", name=\"SMILES-Output\", kernel_initializer=init_weights), \n",
    "    name=\"Time-Distributed\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have defined all the layers, we will connect them together to make a graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# connecting the VAE model as a graph\n",
    "\n",
    "# cnn encoder layers\n",
    "conv_1_fwd = conv_1_func(smiles_input)\n",
    "conv_2_fwd = conv_2_func(conv_1_fwd)\n",
    "conv_3_fwd = conv_3_func(conv_2_fwd)\n",
    "conv_4_fwd = conv_4_func(conv_3_fwd)\n",
    "\n",
    "# flattening\n",
    "flattened_convs = flatten_func(conv_4_fwd)\n",
    "dense_1_fwd = dense_1_func(flattened_convs)\n",
    "\n",
    "# latent space\n",
    "z_mean = z_mean_func(dense_1_fwd)\n",
    "z_log_var = log_z_func(dense_1_fwd)\n",
    "z = z_func([z_mean, z_log_var])\n",
    "\n",
    "# rnn decoder layers\n",
    "repeat_1_fwd = repeat_1_func(z)\n",
    "gru_1_fwd = gru_1_func(repeat_1_fwd)\n",
    "gru_2_fwd = gru_2_func(gru_1_fwd)\n",
    "gru_3_fwd = gru_3_func(gru_2_fwd)\n",
    "smiles_output = output_func(gru_3_fwd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View model structure and metadata\n",
    "Now the model is ready to train! But first we will compile the VAE model, then view model metadata, again using the [keras2ascii](https://github.com/stared/keras-sequential-ascii) tool. To compile the model, we will need to define our own VAE loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vae loss function -- reconstruction loss (cross entropy) plus KL divergence loss against a Gaussian prior\n",
    "# Intuitive meaning for this loss function: \"Reconstruct the data but stay close to a Gaussian\"\n",
    "def vae_loss(x_input, x_predicted):\n",
    "    reconstruction_loss = K.sum(binary_crossentropy(x_input, x_predicted), axis=-1)\n",
    "    reconstruction_loss *= input_dim\n",
    "    kl_loss = 1 + z_log_var - K.square(z_mean) - K.exp(z_log_var)\n",
    "    kl_loss = K.sum(kl_loss, axis=-1)\n",
    "    kl_loss *= -0.5\n",
    "    return K.mean(reconstruction_loss + kl_loss)\n",
    "\n",
    "# create model\n",
    "vae_model = Model(\n",
    "            inputs=[smiles_input],\n",
    "            outputs=[smiles_output]\n",
    ")\n",
    "\n",
    "# compile model\n",
    "vae_model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=vae_loss,\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "vae_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# view model as a graph\n",
    "keras2ascii(vae_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train VAE\n",
    "\n",
    "When training our VAE, we will see metrics printed after each epoch such as test/train loss and accuracy values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Reset model and set all layers are trainable\n",
    "vae_model.reset_states()\n",
    "for layer in vae_model.layers:\n",
    "    layer.trainable = True\n",
    "\n",
    "# fit model to training data\n",
    "history = vae_model.fit(\n",
    "    x=X_train,\n",
    "    y=X_train,\n",
    "    epochs=20,\n",
    "    validation_data=(X_test, X_test),\n",
    "    batch_size=64,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's view the learning curve for the trained model. \n",
    "\n",
    "This code will generate a plot where we show the test and train errors as a function of epoch (one forward pass and one backward pass of all training examples through the NN).\n",
    "\n",
    "The learning curve will tell us if the model is overfitting or underfitting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the learning curve \n",
    "plt.rcParams.update({'font.size': 18})\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.subplots_adjust(left=0.16, bottom=0.16, right=0.95, top=0.90)\n",
    "plt.rc('font', family='Arial narrow')\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('VAE Model accuracy Gap between HOMO and LUMO (Hartree)', fontname='Arial Narrow', size=18) #pad=12\n",
    "plt.ylabel('Error',fontname='Arial Narrow', size=18)\n",
    "plt.xlabel('Epoch',fontname='Arial Narrow', size=18)\n",
    "#plt.xlim(0,20)\n",
    "#plt.ylim(800,2000)\n",
    "plt.legend(['Train', 'Validation',], loc='upper right')\n",
    "# te = '%.2f' % mean_absolute_error\n",
    "# tr = '%.2f' % mse_X_test\n",
    "# plt.text(4.3, 0.8, 'Test_rmse = ', c='r', fontsize=16)\n",
    "# plt.text(7.4, 0.8, te, c='r', fontsize=16)\n",
    "# plt.text(8.5, 0.8, 'eV', c='r', fontsize=16)\n",
    "# plt.text(4.2, 0.1, 'Train_rmse = ', c='r', fontsize=16)\n",
    "# plt.text(7.4, 0.1, tr, c='r', fontsize=16)\n",
    "# plt.text(8.5, 0.1, 'eV', c='r', fontsize=16)\n",
    "plt.savefig('./tpsa_vae_X_training_loss.png', dpi=600, facecolor='w', edgecolor='w', scale=1, width=600, height=350)\n",
    "plt.show()\n",
    "# plot the learning curve \n",
    "\n",
    "\n",
    "# plot the learning curve \n",
    "plt.rcParams.update({'font.size': 18})\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.subplots_adjust(left=0.16, bottom=0.16, right=0.95, top=0.90)\n",
    "plt.rc('font', family='Arial narrow')\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('VAE Model accuracy Gap between HOMO and LUMO (Hartree)', fontname='Arial Narrow', size=18) #pad=12\n",
    "plt.ylabel('Accuracy',fontname='Arial Narrow', size=18)\n",
    "plt.xlabel('Epoch',fontname='Arial Narrow', size=18)\n",
    "#plt.xlim(0,20)\n",
    "#plt.ylim(800,2000)\n",
    "plt.legend(['Train', 'Validation',], loc='lower right')\n",
    "# te = '%.2f' % mean_absolute_error\n",
    "# tr = '%.2f' % mse_X_test\n",
    "# plt.text(4.3, 0.8, 'Test_rmse = ', c='r', fontsize=16)\n",
    "# plt.text(7.4, 0.8, te, c='r', fontsize=16)\n",
    "# plt.text(8.5, 0.8, 'eV', c='r', fontsize=16)\n",
    "# plt.text(4.2, 0.1, 'Train_rmse = ', c='r', fontsize=16)\n",
    "# plt.text(7.4, 0.1, tr, c='r', fontsize=16)\n",
    "# plt.text(8.5, 0.1, 'eV', c='r', fontsize=16)\n",
    "plt.savefig('./tpsa_vae_X_training_acc.png', dpi=600, facecolor='w', edgecolor='w', scale=1, width=600, height=350)\n",
    "plt.show()\n",
    "# plot the learning curve \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a decoder model and use to generate SMILES from noise\n",
    "\n",
    "Now that we have trained our VAE, we can use the decoding part of the VAE to generate SMILES strings! Let's start by defining our decoder model. Note that this model doesn't need to be compiled since we are not training this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# connect the decoder graph\n",
    "decoder_input = Input(shape=(latent_dim,), name=\"decoder_input\")\n",
    "decoder_repeat_1_fwd = repeat_1_func(decoder_input)\n",
    "decoder_gru_1_fwd = gru_1_func(decoder_repeat_1_fwd)\n",
    "decoder_gru_2_fwd = gru_2_func(decoder_gru_1_fwd)\n",
    "decoder_gru_3_fwd = gru_3_func(decoder_gru_2_fwd)\n",
    "decoder_smiles_output = output_func(decoder_gru_3_fwd)\n",
    "\n",
    "# define decoder model\n",
    "decoder_model = Model(\n",
    "    inputs=[decoder_input],\n",
    "    outputs=[decoder_smiles_output]\n",
    ")\n",
    "decoder_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# view decoder graph. this should look like a subset of the VAE graph.\n",
    "keras2ascii(decoder_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's generate SMILES strings! First we will randomly sample from a unit gaussian distribution, feed the random samples into the decoder model, and take the output of the decoder model and convert it back into SMILES characters. Don't be surprised to see strange SMILES strings! We used a very small dataset, and did not train for very long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for x in range(20):\n",
    "    \n",
    "    # draw from a unit gaussian \n",
    "    decoder_test_input = np.random.normal(0, 1, latent_dim).reshape(1, latent_dim)\n",
    "    decoder_test_output = decoder_model.predict(decoder_test_input)\n",
    "    \n",
    "    decoded_one_hots = np.argmax(decoder_test_output, axis = 2)\n",
    "\n",
    "    SMILES = ''\n",
    "    for char_idx in decoded_one_hots[0]:\n",
    "        if charset[char_idx] in [\"PAD\", \"NULL\"]: \n",
    "            break # Stop decoding if you hit padding or an out-of-vocab character (NULL)\n",
    "        \n",
    "        SMILES = SMILES + charset[char_idx]\n",
    "\n",
    "    print(SMILES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save VAE and decoder models\n",
    "We can save/load these models for future use, again using the `save()` and `load_model()` functions from Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# save and load the decoder model \n",
    "decoder_model.save(\"tpsa_decoder_model.hdf5\")\n",
    "loaded_decoder_model = load_model(\"tpsa_decoder_model.hdf5\")\n",
    "\n",
    "# for VAEs, we must instantiate model w/ same architecture then load weights onto this model\n",
    "vae_model.save_weights(\"tpsa_vae.hdf5\")\n",
    "loaded_vae_model = vae_model.load_weights(\"tpsa_vae.hdf5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
